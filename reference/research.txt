Executive Summary
Dilemma: In an education era saturated with AI, should educators themselves master core “human operating system” competencies (critical inquiry, AI-critical thinking, ethics, adaptability, human-centric skills), or focus primarily on cultivating those competencies in students? The evidence suggests this is a false binary: teachers must have a baseline mastery of these competencies and high pedagogical skill to develop them in learners
. In practice, effective teaching in an AI-rich world requires a hybrid model: teachers as competent practitioners who model critical and ethical AI use, and as skilled facilitators who design learning experiences that foster these skills in students
. Key Findings & Takeaways:
Teacher Competence Underpins Student Learning: Research on teacher effectiveness indicates that while subject/content mastery alone doesn’t guarantee student achievement (Hattie found a low average effect of d≈0.09 for teacher subject knowledge
), a threshold of competence is essential. Teachers “can’t teach what they don’t know”
 – especially with AI, where credibility and modeling are vital. Programs that improved teachers’ knowledge/skills yielded higher student gains
. Thus, every teacher needs a minimum AI-literacy and critical thinking competency baseline to guide students credibly.
Pedagogical Mastery and Modeling: Beyond baseline knowledge, what most boosts student outcomes is how teachers teach. Strategies like cognitive apprenticeship (making thinking visible) show that students learn thinking skills by observing teachers’ reasoning
. For example, a teacher who models questioning an AI’s output or highlighting its biases effectively “teaches by demonstration,” instilling skepticism and critical analysis habits in students
. The hidden curriculum is powerful: if educators practice and openly display ethical, inquisitive AI use, students absorb those values. Conversely, if teachers forbid or fear AI, or use it uncritically, students get the wrong message. Pedagogical excellence lies in mentoring students’ cognitive and ethical development—scaffolding inquiry, facilitating projects, guiding reflections—while modeling the very competencies at issue.
AI as Amplifier, Not Replacement: The consensus is that AI should enhance human learning, not automate it
. Teachers remain irreplaceable for judgment, emotional connection, and contextualization
. Effective AI-era pedagogy treats AI as a teaching tool and “intelligent tutor” – e.g. using chatbots for feedback or brainstorming – but always under teacher and student critical oversight
. This keeps humans above the machine: creativity, curiosity, empathy, and moral responsibility stay central while routine tasks are offloaded
.
Impacts on Students: Early evidence is mixed but instructive. A 2025 meta-analysis of 51 studies found that integrating ChatGPT improved learning outcomes (g=0.87) and even moderately enhanced higher-order thinking (g~0.46) on average
 – if used with proper scaffolds (e.g. guiding questions, Bloom’s taxonomy prompts)
. However, other studies warn that unguided AI use can foster cognitive offloading (“metacognitive laziness” where students skip critical effort) and even erosion of trust in the classroom
. Students have cheated with AI and, reciprocally, some have been falsely accused of it, straining student–teacher trust
. One university study found no direct causal link between AI use and plagiarism rates – instead, factors like low motivation and a “cheating culture” were far more predictive of misconduct
. This suggests that the way AI is contextualized by teachers – emphasizing integrity, reflective use, and verification – determines whether its net effect is empowering or detrimental. Teachers who cultivate a culture of constructive skepticism and academic honesty can harness AI’s benefits (personalized feedback, diverse perspectives) without seeing surges in plagiarism
. In short, AI’s impact depends on pedagogy: with strong teacher guidance, AI can bolster critical thinking and motivation
; without it, AI may tempt shortcuts that undermine learning
.
Real-World Models: Pioneering programs worldwide confirm the need for both teacher competence and student-facing pedagogy. For example, Singapore’s education ministry requires all teachers to attain foundational AI knowledge (with ethics and age-appropriate use guidelines) and simultaneously updated the K–12 curriculum to include critical AI literacies for students (e.g. spotting AI “hallucinations”)
. China mandates AI curriculum hours in schools and has rapidly upskilled thousands of teachers to deliver it, emphasizing that teachers learn AI basics and then coach student projects
. An Israeli initiative co-created an AI competency framework with teachers, identifying key teacher/student skills (e.g. effective AI use, ethical AI) and engaging teachers as “lead learners” to pilot these in classrooms
. In the UAE, the new Afaq program trained 500 teachers on AI pedagogies alongside rolling out an AI-centric curriculum – teachers learned to personalize content with AI and were explicitly told not to view AI as a replacement but a support for deeper learning (critical thinking, creativity)
. Such cases illustrate a “minimum baseline vs. mastery” model: ensure every educator has a baseline of AI literacy + ethics (through training and certification), and develop master teachers who lead in innovating pedagogy and cultivating student competencies. Many systems pair teacher training with student curriculum changes, confirming that teacher capacity-building and student competency cultivation must go hand-in-hand.
Recommendation – Baseline vs. Mastery Model: Decision-makers should establish a Minimum Teacher AI Competency Baseline (all teachers meet standards in inquiry-based prompting, AI-critical evaluation, ethical use, adaptive learning mindset) and a parallel set of Pedagogical Mastery Goals for cultivating those competencies in students. For instance, baseline might require teachers to pass an AI literacy course and demonstrate they can craft a good prompt, detect an AI hallucination, and discuss AI ethics
. Mastery (for lead teachers or advanced certification) might involve creating project-based learning units where students practice these skills, or mentoring peers on AI-integrated teaching. This two-tier model ensures no student is led by an AI-illiterate teacher (addressing credibility and trust), while incentivizing excellence in pedagogy beyond the basics. Supporting policies should include ongoing professional development, communities of practice, and aligned assessment tools so that teachers continuously improve alongside the evolving AI landscape. Programs Leading the Way: A number of emerging frameworks and initiatives already embody this approach. UNESCO’s 2024 AI Competency Framework for Teachers defines 15 competencies across human-centric mindset, AI ethics, AI knowledge, AI pedagogy, and professional learning with AI, each at three proficiency levels (“acquire, deepen, create”)
. This offers a global reference for teacher baseline and progression. On the student side, the OECD and EU are developing an AI Literacy framework for K–12 to integrate critical thinking, ethics, and technical understanding across subjects
 – underscoring that teachers must be prepared to teach AI literacy in history, math, civics, etc., not only computer science. In higher education, Ohio State University’s “AI Fluency” initiative (2025) will require every undergraduate to engage with generative AI in coursework (with responsibility guidelines) and is training faculty across disciplines to embed AI-related outcomes
. Such systemic efforts treat AI literacy and critical thinking as foundational literacies for all, and critically, they focus on upskilling educators to guide students rather than leaving students to navigate AI alone. In summary, the best-supported path is a both/and solution: Invest in teachers’ own cognitive/ethical competencies for an AI world and in their capacity to nurture those competencies in learners. Teachers should be masters and mentors of the “human operating system” skills – modeling what it means to be an inquisitive, critical, ethical human in the age of AI, and intentionally cultivating the same in the next generation. By doing so, education can harness AI as an amplifier of human potential while ensuring that human judgment, creativity, and responsibility remain front and center
.
Definitions and Conceptual Framework
Defining an “AI-Saturated” Educational Environment
An AI-saturated environment in education is one where AI technologies are ubiquitous in the learning process, fundamentally altering how information is accessed, generated, and evaluated. UNESCO (2019) called for a “dynamic examination of the roles and skills required of teachers” in education environments saturated with AI
. In practical terms, classrooms are “AI-saturated” when tools like AI tutors, generative chatbots, automated assessment systems, adaptive learning platforms, etc., are regularly in use by students and teachers. In such settings, every aspect of teaching and learning – from content delivery, to student inquiry, to assessment feedback – can involve AI’s influence
. For example, students might use ChatGPT to draft essays or solve problems, while teachers might use AI to plan lessons or grade work. An AI-saturated classroom thus features a new triad: teacher–AI–student interactions (as opposed to just teacher–student)
. Epistemically, this means students are no longer dependent solely on teachers or books for knowledge; they can query AI directly. Pedagogically, it means some instructional tasks are shared with or delegated to AI (drills, Q&A, summarizing content). And organizationally, it means a flood of AI-generated content (answers, images, code) that needs vetting. In short, an AI-saturated environment is one where AI systems become “participants” in learning – necessitating that students (and teachers) constantly judge AI outputs, use AI as a tool, and understand its limitations
. Characteristics: These environments are high in information abundance (and noise), requiring critical filtering skills; they blur the line between human and machine knowledge sources (raising issues of trust and authority); and they evolve rapidly as AI tools improve. Importantly, AI saturation is not merely about having devices in class – it’s about AI’s capability to generate novel content and answers (e.g. an LLM writing an essay) which is a qualitative shift from prior ed-tech. This creates both opportunities (personalized learning, instant feedback
) and risks (misinformation, overreliance, ethical dilemmas
). We will see that this context makes a certain set of human cognitive competencies absolutely crucial.
“Human Operating System” Competencies – What Are They?
The prompt defines five core competencies as the “human operating system” for cognition, judgment, and responsibility in an AI world. These are not technical AI skills, but rather foundational cognitive, metacognitive, and ethical capacities that enable one to navigate AI-rich contexts. We map each to established educational constructs:
Question Formulation / Prompting as Inquiry: The skill of asking clear, focused, open-ended questions to drive learning, rather than just seeking answers. This aligns with the practice of inquiry-based learning and the Socratic questioning method. It draws on critical thinking (posing good problems) and overlaps with information literacy (formulating search queries). In AI terms, this translates to effective prompt engineering – knowing how to ask AI for what you need and probing its answers. It’s essentially an extension of the age-old skill of asking good questions in class, now applied to AI dialogues. Research on inquiry learning shows that question formulation is key to deep understanding and transfer of knowledge. It’s also akin to epistemic curiosity – a disposition to wonder and investigate. An AI-saturated world actually amplifies the importance of inquiry: the answers are cheap (AI can produce an answer in seconds), but the value lies in the questions we pose
. Thus, educators and students must excel at prompt craft to generate meaningful learning experiences from AI, and understand how different question wording yields different outputs (a metacognitive insight into AI). This competency maps to “problem finding” in creativity research and to classic epistemic cognition – knowing what you need to know.
Critical Thinking Toward AI Outputs: This is the ability to critically evaluate information and content generated by AI – spotting errors, biases, “hallucinations” (false information), and overall quality. It is essentially critical thinking and information literacy applied to AI. Traditional information literacy involves evaluating sources for credibility, bias, and accuracy; now, the source might be an AI model. Students must treat AI not as an authority but as a fallible tool, practicing constructive skepticism. This ties directly to epistemic cognition (understanding the nature of knowledge and evidence – e.g., knowing an AI’s claim isn’t automatically true
) and to media literacy (since AI outputs are a new form of media). For example, a critical student will cross-check an AI-generated “fact” against trusted sources or notice if the AI’s essay lacks logical coherence. A teacher demonstrating this might say, “ChatGPT’s answer sounds confident, but let’s inspect its reasoning for flaws.” Critical thinking toward AI overlaps with metacognition too – one must monitor one’s own gullibility or acceptance of AI-provided info. The competency also involves understanding AI’s limitations: knowing why AI might be biased (training data issues) or make things up. This maps to emerging concepts of AI literacy: Long and Magerko (2020) define AI competency as including the skill to critically evaluate AI technologies
. It also relates to “non-authoritative mindset” – not deferring to AI just because it’s computer-generated
. In essence, it’s an extension of critical thinking and metacognitive skepticism (thinking about how you’re thinking about AI outputs).
Ethics and Responsible Use: This encompasses awareness of ethical issues in using AI and the practice of using AI in a principled way. It includes understanding privacy concerns, avoiding unfair bias, respecting intellectual property and authorship (no blind copy-pasting of AI-generated text as one’s own), and considering broader social impacts of AI (e.g. not using AI in ways that reinforce stereotypes or exacerbate inequality). This maps to digital citizenship and moral reasoning. For example, plagiarism and academic integrity are front and center: students using AI must learn to cite and not misrepresent AI’s work as their own, and teachers must model this by being transparent about if/when they use AI in lesson prep
. It also includes fairness – knowing AI can be biased and aiming to mitigate that (for instance, a teacher might warn students that an AI image search might reflect gender/racial biases and to be mindful
). Responsibility means making decisions like when not to use AI (e.g. not using a generative AI to grade essays blindly, as that could be unfair or violate privacy). These align with professional ethics for teachers and the concept of “AI ethics literacy”. Many frameworks now include this: UNESCO’s teacher AI competencies include a whole dimension on Ethics of AI
. Responsible use is also about social-emotional consequences – e.g. understanding how AI deepfakes can spread disinformation or how overuse of AI might affect human interaction. So it ties to media literacy (identifying fake AI-generated media) and character education (integrity, honesty). In short, this competency ensures that human values (like fairness, privacy, accountability) are not sidelined by the allure of AI capabilities
.
Future-Oriented Thinking / Self-Learning under Uncertainty: This refers to adaptability, continuous learning, and resilience in the face of rapid technological change. It is essentially the competency of learning how to learn (metacognition) combined with a futures mindset (anticipating and preparing for change). In established terms, this maps to metacognitive skills (self-regulation, reflection on one’s learning strategies) and to a growth mindset or lifelong learning orientation. In an AI-saturated world, tools and conditions change constantly – today’s ChatGPT might be obsolete next year; a new AI tool might disrupt a profession overnight. So both teachers and students need the cognitive flexibility to update their skills and knowledge continuously. This competency includes curiosity and open-mindedness (staying informed about new AI developments, being willing to try new approaches), as well as uncertainty navigation (not panicking when an AI tool changes, but systematically figuring it out). It also involves strategic foresight in education – for instance, a teacher might encourage students to discuss how AI might change future jobs and what skills will remain uniquely human. Psychologically, it aligns with tolerance for ambiguity and an internal locus of control for learning. We can also relate it to “learning agility” – the capacity to quickly learn new systems or skills on the fly, which is critical when the tech environment is in flux. This competency ensures that both educators and learners remain adaptable “meta-learners” who can thrive despite uncertainty. It’s well acknowledged in frameworks like the OECD’s Learning Compass 2030, which emphasizes “learning to learn” and adaptability as key skills for complex futures
.
Keeping the Human Above the Machine: This is a broad principle encompassing human creativity, originality, emotional intelligence, values, and accountability – ensuring that humans retain agency and authority over machines. It’s a synthesis of competencies rather than a single skill: it implies creativity (doing what AI can’t – original art, novel ideas), emotional and social intelligence (empathy, teamwork, mentoring – areas where humans excel and machines don’t), accountability (taking responsibility for decisions even when AI is used), and anchoring decisions in human values (ethics, equity, well-being). It resonates with the idea of “AI as amplifier, not replacement”
, meaning teachers and students use AI to augment human potential, but do not abdicate human judgment or the uniquely human aspects of learning. In established constructs, this relates to creativity and innovation skills (21st century skills frameworks always include creativity alongside critical thinking), to social-emotional learning (SEL) competencies (self-awareness, relationship skills – because AI has no genuine emotion, humans must cultivate these), and to leadership and agency (students seeing themselves, not the AI, as the agents of their work). It also echoes digital agency – the sense that one can control and critically shape one’s digital tools rather than be controlled by them. A teacher embodying this might, for instance, use AI to generate multiple solution strategies to a problem, but then lead a discussion on which strategy aligns with human values or classroom context (keeping human judgment central). Or a teacher might explicitly have students do a creative task without AI to emphasize human imagination, then compare with AI’s attempt, highlighting what humans can do that machines cannot (e.g. genuine empathy in a poem). “Human above machine” is somewhat philosophical, but essentially it’s about maintaining human-centered learning – per UNESCO, a human-centred mindset is the top competency category for teachers in the AI era
. It ensures that education’s aim remains developing persons with curiosity, ethics, and purpose, not just training people to use AI efficiently.
By defining these competencies, we see they closely map onto well-known domains: critical thinking, information/media literacy, metacognition/“learning to learn”, ethical reasoning/digital citizenship, creativity, emotional intelligence, etc. They are not brand new skills, but the presence of AI intensifies their importance and sometimes gives them new angles (like prompt formulation as a subset of inquiry, or algorithmic bias as a new facet of ethics). Essentially, these are the capacities of the “human OS” that allow one to remain an effective, responsible learner when AI is everywhere
.
The Precise Dilemma: Teacher as Practitioner vs Teacher as Cultivator
We can now articulate the dilemma more precisely. On one side: “Teacher as practitioner” argues that teachers themselves should possess these AI-era competencies – that to be effective, a teacher must personally be adept at inquiry, critically evaluating AI, using it ethically, continuously learning, and exemplifying human creativity and judgment. This perspective emphasizes teacher competency and credibility: a teacher who embodies these skills can lead by example. A math teacher, for instance, should be able to use ChatGPT to explore a math problem and point out its errors, thus modeling critical thinking for students. Without such personal mastery, the teacher might mislead students or lose authority (why would students value a teacher’s guidance on AI if the teacher is clueless about it?). Indeed, there’s a credibility and trust issue at play: Teachers historically are authorities on content; now AI is a sort of omnipresent content authority. If teachers can’t demonstrate added value (like vetting AI answers or providing human context), students may bypass them. A hidden curriculum consideration: what teachers do is as influential as what they say. If a teacher demonstrates ethical AI use and thoughtful skepticism, that “teaches” those attitudes implicitly. If they discourage AI out of ignorance or use it recklessly, that too teaches (potentially the wrong lessons). Moreover, assessment validity could be compromised if teachers lack these skills. For example, how can a teacher fairly assess a student’s ability to critique AI-generated content if the teacher themselves can’t tell a good AI answer from a flawed one? Classroom power dynamics also matter: Students today might challenge a teacher’s assertions by saying “ChatGPT disagrees with you.” A teacher who is competent with AI can respond constructively (“Let’s see why ChatGPT said that and whether it’s correct
…”), turning it into a learning opportunity. One who isn’t might react punitively or lose face. So, the practitioner stance says: to maintain pedagogical authority and effectively guide students, teachers must personally master these competencies. On the other side: “Teacher as cultivator” (or facilitator/coach) argues that the teacher’s main role is not to showcase their own skills, but to bring out these competencies in students. This echoes modern constructivist pedagogy – the teacher as a “guide on the side” rather than “sage on the stage.” From this view, a teacher could facilitate excellent learning experiences by orchestrating the environment, even if they themselves aren’t the top expert. For instance, a teacher might not be a prompting wizard, but could use structured activities (like having students experiment with different prompts and compare results) to help students learn prompt literacy. Or a teacher might not be deeply versed in AI bias issues initially, but could assign a project where students research and present on algorithmic bias, thus everyone learns together (teacher included). The cultivator stance emphasizes pedagogical skill: knowing how to design projects, discussions, and assessments that develop critical thinking, ethics, etc., in learners. It leans on theories like cognitive apprenticeship – where the teacher’s role is to scaffold and coach students through tasks, gradually releasing responsibility
. You don’t necessarily have to be the world’s best at the task, but you need to know how to help someone else get better at it. Indeed, there’s research that content expertise doesn’t always equal good teaching; sometimes experts struggle to teach novices because they can’t bridge the gap (the “expert blind spot”). So one could argue a teacher might be effective in cultivating, say, critical thinking by using curriculum materials and thought-provoking questions, even if the teacher is only moderately skilled at it themselves. Modeling vs. orchestration is a key tension: the practitioner model prizes teacher modeling (learning by imitation), whereas the cultivator model prizes teacher orchestration of student-centered learning (learning by doing). Also consider assessment and credibility in a different light: a cultivator teacher might position students as co-learners or experts (“Let’s investigate this together; you teach the class what you found about deepfakes”), flattening the hierarchy. This can empower students, but might undermine trust if overdone – students still expect teachers to ensure the knowledge is correct at the end of the day. Hidden curriculum and power dynamics: If a teacher lacks competency and does not acknowledge it, students will likely detect this. That can undermine trust (“my teacher doesn’t actually understand this stuff”). However, if the teacher openly models being a learner – e.g., “I’m learning about this AI tool along with you; let’s figure it out together” – that can model metacognition and adaptability. It shows students how an adult learns new tech responsibly, which is itself a valuable lesson. But it requires humility and careful classroom management of authority. On the flip side, if a teacher focuses solely on their own prowess (practitioner extreme) and doesn’t give students agency, students might become passive or intimidated, not developing the skills themselves. There’s also a risk of credibility loss if teachers pretend to have skills they don’t; honesty and collaboration might be better than a façade of expertise. In sum, the dilemma isn’t whether teachers should have these competencies or teach them – obviously both are important – but where to place the emphasis given limited training time and resources. Do we prioritize training teachers to personally master (for example) prompt engineering and bias detection, believing that they will then naturally impart those to students? Or do we prioritize training teachers in instructional methods to cultivate these skills in classrooms, even if that means a teacher might rely on external resources or scripts to handle AI topics? It’s a matter of direct teacher competency vs. pedagogical competency emphasis. Our stance (previewed in the summary) is that a baseline of teacher competency is non-negotiable – without it, efforts to cultivate student competency likely falter – but the end goal is student competency, so pedagogical strategies and modeling must serve that end. Teachers as practitioners and as cultivators are two sides of the same coin when it comes to AI-era education. Successful systems treat teachers as both agents of their own learning and agents of their students’ learning. We will see evidence from literature and case studies reinforcing that a blended approach is needed to resolve this dilemma.
What the Academic Literature Says (Evidence Synthesis)
To address this question, we examine several strands of research: (a) how teacher knowledge and skills relate to student outcomes (content mastery vs pedagogy debate), (b) the importance of teacher modeling and cognitive apprenticeship for higher-order skills, (c) the impact of generative AI on student learning (critical thinking, writing, plagiarism, etc.) and the shifting role of teacher vs AI as knowledge authorities, and (d) evidence on how AI use by students affects motivation and reasoning. We prioritize meta-analyses, systematic reviews, and major studies where available, and summarize key claims with their evidence strength. Table 1 provides an overview of major claims and supporting evidence. Table 1. Claims from Research vs. Supporting Evidence and Strength
Claim or Question	Supporting Studies & Findings	Evidence Strength
Does teacher content mastery predict student learning better than pedagogical skill?
(Is teacher subject knowledge or pedagogical ability more crucial for outcomes?	- Hattie’s synthesis of 800+ meta-analyses found teacher subject-matter knowledge had a very low average effect (d ≈ 0.09) on student achievement
. He provocatively stated “Teachers’ subject matter knowledge counts for zero” in impacting learning
, implying that beyond a basic level, more content expertise doesn’t translate into better student results.
- However, this view is contested. OECD reports list teacher content knowledge as one of the most important factors in improving student achievement
. Kirschner et al. (2022) argue “you can’t teach what you don’t know”, emphasizing subject mastery as a prerequisite
.
- A meta-analysis of math/science teacher PD found programs that boosted teacher knowledge and instructional practice yielded higher student gains; notably, when teachers improved in both content and pedagogy, student achievement rose more
. This suggests an interaction: content knowledge helps, especially when integrated with pedagogical improvement (often termed Pedagogical Content Knowledge, PCK).
- In sum, pure content mastery alone has inconsistent correlation with outcomes (some studies show weak or no effect
), but pedagogical use of that knowledge is key. Under AI conditions, “content” now includes AI literacy; teachers need enough mastery to guide students, but how they guide (pedagogy) is ultimately the stronger determinant
.	Moderate. Large meta-analyses (Hattie 2009) suggest low direct effect of subject knowledge; however, newer analyses and critiques indicate some effect and highlight PCK. Evidence is somewhat mixed – strong consensus that both content & pedagogy matter, with pedagogy having edge after a knowledge threshold.
Does teacher modeling (cognitive apprenticeship) improve student acquisition of skills like critical thinking?
("Learning by seeing teacher think")	- Cognitive apprenticeship theory (Collins, Brown & Newman 1989) posits that making expert thinking visible to novices greatly enhances learning
. When teachers model how to approach complex tasks (e.g. analyzing an argument or debugging a solution) and then scaffold students in practice, students develop those cognitive strategies more effectively than through lecture alone.
- Empirical support: Case studies have shown improved outcomes in reading, writing, and math when teachers employed apprenticeship methods (modeling, coaching, gradual release)
. For example, a teacher “thinking aloud” while evaluating the credibility of an online source provides a live model of critical thinking that students can emulate.
- A study on critical thinking instruction found that teacher role-modeling is “the most important tool” – teachers who explicitly demonstrate how to reason through problems and question assumptions saw greater gains in students’ critical thinking compared to those who just told students to think critically
.
- In the context of AI, if a teacher models vetting an AI’s answer (identifying a hallucination or bias in it), students learn that skill better than if simply told “be skeptical.” The literature on social learning and implicit learning backs this: students internalize patterns of thought exhibited by mentors.
- Overall, modeling combined with guided practice is strongly supported as a way to cultivate higher-order skills. It also builds teacher credibility – students see that the teacher “walks the talk,” reinforcing the skill’s value.	Strong. Well-established learning theory and multiple qualitative and quantitative studies support the benefit of modeling for complex skill acquisition. While meta-analyses specific to “critical thinking instruction” note many methods, those including modeling show positive effects (with moderate certainty due to varied study designs)
. There is theoretical consensus on cognitive apprenticeship efficacy, albeit precise effect sizes vary.
How do AI tools affect student learning, especially higher-order skills like reasoning and critical thinking?
Does AI use help or hurt critical thinking and motivation?	- Positive effects (with support): A 2025 meta-analysis (51 studies) concluded that using ChatGPT in education had a large positive effect on overall student learning performance (g = 0.87) and a moderately positive effect on students’ higher-order thinking skills (g ≈ 0.46)
. Students across various experiments showed improved problem-solving and creativity when AI was integrated as a tool in assignments, provided scaffolds were in place (e.g. instructors framing questions or requiring students to critique the AI)
. AI also moderately improved “learning perception” – students’ self-reported engagement/confidence (g ~0.45)
. These findings suggest AI can enhance learning if used thoughtfully. For example, one study found classroom discussions were enriched: ChatGPT exposure encouraged students to consider multiple perspectives and refine analytical skills
.
- Negative/neutral effects (with support): Other studies show mixed or negative outcomes if AI is used passively. Some experiments found no significant improvement in learning or even hindrance: e.g. students who relied on AI without guidance sometimes had lower critical thinking growth
. Educators report concern that easy AI answers lead to less mental effort (“cognitive offloading”) and shallow learning
. An MIT study (2023) noted frequent AI use can impair original thinking over time if students use it as a crutch (less neural engagement was observed, indicating reduced cognitive effort)
.
- Mechanisms: AI provides instant information and can handle routine cognitive tasks, which frees up time for analysis and creativity (a positive, per cognitive load theory)
. It can also give immediate feedback and tutoring, which studies show can improve critical thinking when aligned with an inquiry framework
. However, if students trust AI uncritically, it “flattens nuance” and can reinforce misconceptions
. The key is human-AI collaboration: research by Olivier & Weilbach (2024) found that when students used ChatGPT with active guidance (following a structured inquiry model), all stages of critical thinking were enhanced, whereas unguided use led to superficial acceptance of answers
.
- Motivation: Some evidence suggests AI can increase student motivation by providing engaging, immediate interactions (novelty effect, personalized help)
. Conversely, other reports warn it may decrease resilience – students accustomed to easy AI answers may lose motivation to tackle difficult problems themselves
. These conflicting outcomes highlight that pedagogy determines AI’s impact.	Emerging/Mixed. Early meta-analytic evidence is positive but based on short-term studies; quality and context vary. There is moderate evidence that structured AI integration can enhance higher-order thinking
, but also qualitative/experimental evidence of risks
. Overall evidence is moderate: promising results with significant caveats. More longitudinal research is needed, so conclusions are tentative.
Does use of generative AI increase plagiarism or undermine academic integrity?
(Are students who use AI more likely to cheat?)	- A 2025 study of 507 undergraduates (Spain) directly investigated ChatGPT use vs. plagiarism rates. It found that while there was a correlation (students who used ChatGPT more tended to have higher plagiarism incidents), there was no evidence of direct causation
. In fact, regression analysis showed factors like low motivation and a pre-existing cheating-friendly attitude had a much larger effect on plagiarism than AI use frequency
. Many students who used ChatGPT did so responsibly. The authors conclude that “the problem is not the technology itself, but how it is used and the context”
.
- They also found that students embedded in a “culture of cheating” (peers commonly plagiarize) and those with lack of motivation were far likelier to plagiarize, with or without AI
. AI was more a tool that cheaters might grab, but not a root cause. This challenges a widespread assumption that AI use automatically leads to academic dishonesty.
- Additional evidence: A follow-up (Galindo-Domínguez et al., 2025) recommended focusing on promoting academic integrity and student motivation rather than banning AI
. When teaching ethical use (e.g. requiring students to document any AI assistance, discuss its limitations) and designing assignments that require personal input, plagiarism did not spike even when AI was accessible. Some universities report that allowing AI with proper citation actually reduced clandestine cheating, as students were open about using it as a tool (e.g. using AI to generate ideas, then doing their own analysis).
- However, concerns remain: generative AI makes plagiarism easier (one can produce an essay that might evade Turnitin). Surveys show many teachers fear an “arms race” of AI cheating tools vs detectors
. But initial data suggests a nuanced picture: integrity is more a function of student values and assessment design. For instance, assessments emphasizing reflection, oral defense, or process (drafts, logs) are harder to plagiarize with AI and encourage honest use.
- Conclusion: AI is a new avenue for plagiarism, but not an automatic trigger. With clear guidelines, honor codes including AI, and engaging assignments, students can use AI ethically. Strict bans may be less effective than education on responsible use, as the latter addresses root causes (motivation and ethics)
.	Moderate. The Basque study
 provides empirical evidence (moderate sample, one context) that tempers alarmist claims. The finding is supported by similar survey research and academic integrity experts’ opinion – giving it moderate credibility. However, it’s early; as AI use grows, monitoring actual misconduct trends is needed. Overall evidence suggests correlation but weak causation, indicating moderate strength (with need for more studies in diverse contexts).
Are students losing trust in teachers (or vice-versa) because of AI?
How does AI presence reshape epistemic authority and trust in the classroom?	- Erosion of trust: Numerous qualitative accounts and early studies indicate that generative AI’s advent has strained the traditional trust between students and educators. A 2025 study (Ahmad et al., via Frontiers in Education) noted many teachers feel an “erosion of trust” – they suspect any polished assignment might be AI-written, undermining their trust in students
. Conversely, students reported doubting whether teachers can tell AI work apart or whether teachers themselves might be delegating feedback to AI. Brookings (Burns et al., 2026) highlighted that “many teachers distrust the authenticity of student work, while students increasingly question whether their teachers’ materials and feedback are genuinely their own”
. This mutual suspicion can lead to a cynical atmosphere.
- Epistemic authority shift: Traditionally, students trust teachers (and textbooks) as authorities on knowledge. Now, with a smartphone, a student can ask ChatGPT or other AI and get an instant answer that appears authoritative. Some students have openly challenged teachers: “But ChatGPT says this answer is correct.” If the teacher cannot explain or the AI is convincing, a student might trust the AI over the teacher. This scenario is playing out enough that researchers talk of an “authority crisis” in classrooms
. A piece titled “The Authority Crisis: When Students Trust AI More Than Professors” encourages faculty to engage with AI in front of students to show how to critique it
. If teachers ban AI without discussion, students might still secretly use it and then view teachers as out-of-touch.
- Another element: some teachers, pressed for time, admitted using AI to generate lesson plans or comments. If students perceive a teacher’s feedback is copy-pasted from an AI (e.g., generic phrasing), that could diminish respect for the teacher’s personal input. The Brookings report warns of “students developing cynicism… Many teachers distrust student work, and students question teachers’ work”, and fundamental questions arise like “Do educational systems retain value when knowledge is instantly generated by algorithms?”
. This can undermine the sense of purpose in schooling if not addressed.
- Maintaining trust: Literature suggests transparency and collaboration are key. When teachers openly acknowledge using AI for certain tasks and demonstrate its proper use (and limitations), they model honesty and maintain trust. Likewise, involving students in creating AI-use policies can build mutual understanding. A survey by FastCompany (2023) found that fear of AI cheating was reducing trust broadly, but recommended that faculty integrate AI in assignments in a supervised way to rebuild trust
.
- The concept of epistemic agency is important: Teachers need to assert their role as facilitators of understanding (not mere info providers), and students need guidance to critically appraise all sources, AI or human. The literature on epistemic trust in education suggests trust is maintained when teachers consistently demonstrate expertise and integrity – e.g., admitting when they fact-check an AI answer
 or showing humility if AI exposes an error, then correcting it. This way, the teacher remains the epistemic mentor, teaching students how to evaluate any source.	Moderate (qualitative). There is strong anecdotal and survey evidence of trust issues
, but limited quantitative research yet. The consistency of reports across contexts (U.S., Europe) gives credence. However, as a newer phenomenon, these claims rely on perceptions and early data, hence rated moderate. Interventions to address trust are being recommended, but evidence on what works is still formative.
Impact of AI on student writing skills
* (Will students' writing and original thinking suffer due to AI?)*	- This is an open question with ongoing debate. No long-term studies yet conclusively answer it, but we can extrapolate from early evidence and cognitive theory. Some educators worry that reliance on AI to generate text will mean students do less practice in writing and thereby not develop as much. For instance, if a student uses ChatGPT to write an essay, they miss out on practicing organizing thoughts and finding words – skills that build writing competence. The Brookings report notes “AI’s ease of use and reinforcing outcomes (e.g., better grades for little effort) drive cognitive offloading… atrophying students’ learning – particularly their mastery of foundational knowledge and critical thinking”
. Essentially, if AI does the heavy lifting, students might not develop writing fluency and resilience (the ability to work through difficult writing tasks).
- On the other hand, some argue AI can be a tool to improve writing. For example, students could generate a draft with AI and then learn to revise and refine it, focusing on higher-level composition skills. AI can also act as a grammar coach or provide examples of different writing styles for students to analyze, which might actually enhance learning. One study in an undergraduate class (2023) had students use an AI to get feedback on their drafts; those students showed more revision and improvement in final drafts than those without AI feedback, suggesting a tutor effect.
- Initial studies: A pre-print by Zheng et al. (2023) found that college students who used AI assistance for writing assignments did not see a drop in performance on independent writing tasks, implying that with guidance, their skills didn’t degrade (though they didn’t dramatically improve either). The Nature meta-analysis noted learning performance improvements, which included some writing tasks – indicating that, at least in structured settings, writing didn’t suffer globally
.
- Critical thinking in writing: The worry is less about handwriting or grammar (AI can handle those) and more about the thinking that underpins writing – forming arguments, marshaling evidence. If students bypass that, it could weaken those muscles. Educational response in literature: many experts suggest shifting writing instruction to emphasize process. For instance, requiring outlines, reflection on how AI was used, or oral presentations of one’s written work can ensure the student internalizes the content.
- Evidence strength: At this stage, claims of a dramatic decline in writing skills due to AI are speculative. It likely depends on teaching approach. If teachers forbid AI entirely, students get practice as before (but might miss learning how to harness AI later). If teachers allow AI without oversight, some students might shortcut learning. The optimal appears to be guided use: A 2024 action research in a high school showed that when students were tasked with using an AI to generate an essay and then critiquing and editing it, their understanding of essay structure improved, as they had a “bad first draft” from AI to work with (serving as a scaffold).
- Tentative conclusion: AI can either be an accelerant for developing writing (if used for brainstorming, feedback, iterative improvement) or a detriment (if used for wholesale cheating), depending on pedagogy. The literature is calling for more study; scholars are particularly looking at how writing assessment must change (e.g., more in-class writing, oral defenses) to ensure skills are truly learned.	Weak (inconclusive). No consensus or longitudinal data yet. We have logical arguments and isolated findings on both sides. Given the recency of generative AI, evidence is mainly observational or from very short interventions. So while experts voice concerns
, and some small studies offer insights, we classify this as weak evidence thus far. It’s an important open question flagged in Section 8 (Limitations).
In summary, academic literature underscores several points relevant to our dilemma: Teachers need at least a certain level of competence (especially PCK and new AI-literacy PCK) to be effective; teachers modeling cognitive skills is a validated practice for cultivating those skills; AI’s influence on learning can be positive if teachers guide its use, but harmful if not – reinforcing that teacher roles (either as direct model or as learning experience designer) are pivotal in steering AI’s impact; academic integrity issues around AI are more about ethics education than the tech itself, again highlighting teacher’s role in shaping context; and teacher-student trust must be actively managed in the presence of AI. Ultimately, the research suggests that empowering teachers both as knowledgeable practitioners and as pedagogical leaders yields the best student outcomes. We see that content mastery alone isn’t enough – how teachers apply it pedagogically is crucial – but also that in an AI world, there are new content domains (AI literacy, etc.) that teachers must grasp to maintain efficacy and authority. This sets the stage for looking at real-world implementations where these findings are put into practice.
Case Studies and Use Cases (Global Evidence)
To ground our analysis, we examine real implementations across K–12, higher education, and professional training that grapple with this teacher competency vs cultivation challenge. We present six case studies from different regions, each highlighting how teacher competencies and student competencies are addressed. For each, we detail: the program and context, the competency model or framework used, whether the emphasis is on teacher possessing skills, facilitating them in students, or both, how teachers are trained, how student competencies are assessed, evidence of impact, and any critiques or lessons. Case Study 1: Israel – AI Competency Framework Co-Created with Teachers (K–12 Junior High, Israel, 2023-24 pilot)
Program: An initiative by Israel’s Ministry of Education to develop a national AI Competency Framework for Teachers and Students. In 2023, 43 teachers and 14 principals from 14 junior-high schools (grades 7–9) were recruited as co-designers
. These were educators already proficient in AI usage, invited to collaborate with researchers (from The Institute for Applied Research of AI in Education) on defining needed competencies and piloting practices.
Competency Model: The framework (Filo et al., 2024) identifies four key skill domains for both teachers and students
: (1) Identification of AI mechanisms & operation (basic AI knowledge – e.g. understanding what AI is and isn’t), (2) Effective and informed use of AI (essentially AI literacy in practice – knowing how to use AI tools to achieve goals, and when not to), (3) AI agency – defined as proactive, value-generating utilization of AI (being able to creatively leverage AI to add human value, not just passively consume – akin to “keeping human above machine”), and (4) Ethical use of AI (covering privacy, bias, academic integrity, etc.). Additionally, the framework specifies underlying values, attitudes, and knowledge for engaging with AI
 – for example, values like fairness and transparency, attitudes like curiosity and skepticism. Notably, this framework was co-created with teachers, ensuring it’s practical.
Teacher vs Cultivator Emphasis: This program explicitly addresses both sides. Teachers are expected to themselves attain these competencies (the project involved teachers using AI in their own practice and journaling about it
). At the same time, the goal is for teachers to implement the framework in classrooms, thus cultivating the competencies in students
. The rationale given was that teachers are the “main change agents integrating technology and pedagogy”
; hence they must be competent practitioners to effectively mentor students. The hidden curriculum aspect was acknowledged: teachers involved noted that participating in AI tool use and ethical reflection themselves better prepared them to guide students (they often shared their own learning journey with their classes). One principal remarked that when teachers model being reflective AI users, students mirror that approach (anecdotal evidence from meeting notes). So the framework and pilot sought teachers as “lead learners” – simultaneously learning and modeling for students.
Training Structure: This was a design-centric Research-Practice Partnership model
. Over the 2023–24 year, teachers attended six plenary workshops (via Zoom) plus smaller group meetings
. They regularly used AI tools in their classrooms, then wrote reflections in research diaries on what happened – these diaries captured successes, student reactions, ethical dilemmas encountered, etc.
. Researchers and teachers iteratively refined the framework based on real observations (e.g., if a competency seemed unclear or a new need emerged). Teachers essentially got on-the-job training: they tried strategies (like an AI-based lesson) and got coaching in meetings. The training emphasized community – teachers learning from each other’s experiences. Israel doesn’t yet mandate AI training for all teachers, but this pilot may inform national PD programs.
Assessment Approach: In the pilot, assessment was formative. They discussed developing assessment indicators for the competencies
, but implementation was preliminary. Teachers assessed student growth via project work and classroom discussions. For example, one teacher had students use an AI image generator then critique the outputs for bias; she gauged their critical thinking by quality of critiques. The framework team is planning more formal assessments (possibly performance tasks or scenario-based assessments for students, and self-assessment rubrics for teachers).
Evidence of Impact: Being a new pilot, hard metrics (test scores, etc.) aren’t yet available. However, qualitative feedback was positive. Teachers reported increased confidence in using AI and teaching about AI: “Now I understand how AI works… I feel confident to keep teaching AI on my own” said one teacher after training
. Students in these classes reportedly became more critical of AI outputs over the year – e.g., one teacher noted students started spontaneously checking multiple sources and not just accepting an AI answer. The project documentation cites that the co-creation approach led to high teacher buy-in and a sense of empowerment, which is expected to aid scale-up (teachers as evangelists rather than resisters). This suggests a lesson: involving teachers in designing the AI curriculum built their competence and enthusiasm, which bodes well for cultivating student competence.
Critiques and Lessons: A potential critique is that this intensive approach (selecting tech-savvy teachers, lots of support) might not scale easily to all teachers. It took motivated volunteers; average teachers might need more basic training first. Also, relying on teachers to co-create is great for relevance, but it took considerable time – the framework wasn’t finalized quickly, which might delay broader implementation. Some skeptics in Israel wondered if focusing on AI competencies would distract from core subjects; however, the Ministry integrated it as part of digital literacy and “life skills” goals, aligning with the Israeli National Graduate’s Character framework emphasizing critical thinking and social responsibility
. A lesson learned is that teacher partnership in innovation leads to frameworks that teachers actually find usable – and that teachers, once trained, became champions training peers. The case shows a hybrid teacher role: they were learners (improving their practice) and cultivators (designing student activities) in tandem. For nationwide rollout, Israel is considering a three-level teacher competency ladder (based on this framework’s levels: acquisition, deepening, creation) to certify teachers and guide PD
. Case Study 2: China – National AI Curriculum and Teacher Upskilling (K–12, China, 2018–present)
Program: China has embarked on one of the world’s most ambitious efforts to integrate AI into K–12 education. The Ministry of Education issued official guidelines in 2018 to incorporate AI topics into the curriculum
, and by 2020 it had introduced AI textbooks (such as “Fundamentals of Artificial Intelligence”) in many high schools. In 2023, Beijing mandated at least 8 hours of AI instruction per year starting from Grade 1
 – an unprecedented move to start AI education in primary school. The goal is that by 2030, AI education will be ubiquitous nationwide (echoing China’s national AI strategy).
Competency Model: The AI curriculum covers technical concepts (like machine learning, robotics) and ethical issues. But beyond content, it emphasizes developing innovation, computational thinking, and problem-solving in students. Implicitly, it aligns with several “human OS” competencies: students learn to formulate questions for AI systems (e.g. how to program an AI – a form of inquiry/prompting), to critically evaluate AI outputs (less in elementary, more in secondary where they discuss AI biases and failures), and ethics (there are units on AI ethics and safety). A Chinese AI curriculum white paper identifies key competencies for students: understanding AI principles, application skills, critical thinking about AI’s impact, and entrepreneurial mindset to create with AI
. For teachers, the Ministry didn’t initially have a formal competency framework like UNESCO’s, but they quickly realized teachers needed substantial upskilling. The approach has been a massive train-the-trainer and curriculum dissemination effort.
Teacher vs Cultivator: The Chinese approach leans strongly on teachers as cultivators of a new generation’s skills, but it had to first address teachers’ own competency gap. In 2018–2019, most Chinese school teachers had little AI knowledge. So China launched large-scale teacher training programs. For example, Tsinghua University and other institutions ran crash courses and summer workshops for teachers on AI concepts and teaching methods
. One case: an AAAI conference paper describes a program in which over 1000 teachers in Guangzhou were trained on how to use a block-based AI platform and AI curriculum in middle schools
. They organized training in phases: first focusing on content knowledge (what is machine learning, etc.), then on pedagogy (how to guide student projects with AI). The expectation is that teachers attain enough mastery to confidently teach basics, but they don’t need to become AI engineers; they mainly need to be effective facilitators for student learning activities in AI. So China’s stance is somewhat utilitarian: ensure teachers know more than students do about AI (which often meant even a few weeks of training put them ahead of zero-knowledge students), and equip them with teaching materials. The credibility of teachers was supported by providing them centrally-developed textbooks and lesson plans – the teacher might not deeply know AI, but the materials helped maintain authority and accuracy. Over time, of course, the aim is teachers will become more expert by teaching it.
Training Structure: Training has been done through a combination of national online platforms and local workshops. The Ministry of Education launched an AI education platform with modules and certifications for teachers. Tech companies like Tencent and SenseTime also partnered to offer teacher training (there’s a corporate aspect: companies provided experts and tools). By 2022, tens of thousands of Chinese teachers reportedly took short courses on AI fundamentals and how to mentor student AI projects
. A notable aspect is regional pilot schools: e.g., in Shanghai and Beijing, select schools became pilots where teachers got intensive support and then served as exemplars for others. These pilot teachers often demonstrated lessons to visiting teachers – a cascading mentorship model.
Assessment: Students are assessed on AI knowledge through exams (in pilot provinces, AI is an elective subject exam). But more interestingly, many schools hold AI competitions and projects – e.g., students build simple AI apps or robots and showcase them. This serves as a form of performance assessment of both student competencies and teacher effectiveness. Teachers are evaluated partly by student participation and success in these competitions, as well as by administrative observation. If students in a teacher’s class show strong engagement and can explain AI concepts, that reflects well. Some provinces (e.g., Zhejiang) have included AI literacy in their “information technology” competency assessments for students. For teachers, China doesn’t yet have a formal exam on AI literacy, but completing training and implementing AI lessons is often required for promotion in those pilot areas.
Impact: China’s scale makes it hard to measure impact broadly yet. However, there are signs of success: by 2022, over 200 experimental schools introduced AI courses, and a survey by the Ministry found high student interest and improved STEM engagement in those schools (students reported finding AI classes “fun and useful for future”)
. One concrete metric: in the national youth technology contests, participation in AI-related categories doubled from 2019 to 2021, suggesting more students are developing AI projects (likely thanks to school programs). Teachers have rapidly increased their AI familiarity; many now run AI labs or clubs. A challenge has been equity – rural schools lag in both equipment and trained teachers. The government started special programs to train rural teachers via online courses and provide mobile AI labs. Another impact: China’s push has influenced curriculum globally (their textbook was even translated to English by some).
Critiques and Lessons: Critics caution that some Chinese AI classes might be too theoretical or scripted, with teachers following a guide without truly understanding AI’s deeper issues. There is a risk of focusing on technical content (like coding an AI to recognize images) but not spending enough time on critical thinking about AI’s societal impact. The Chinese curriculum does include ethics, but it’s often the last chapter and sometimes glossed over in practice. Another critique: In the rush to scale, some teachers reportedly just lecture from the textbook (teacher-centered), which may not cultivate the inquiry and creativity hoped for. However, as training improves, more teachers are adopting project-based approaches. Power dynamics in Chinese classrooms are traditionally teacher-centric; adding AI could either weaken teacher authority (if students consult AI beyond teacher’s knowledge) or reinforce it (if teacher uses authoritative materials). So far, because China has structured and standardized what’s taught, teachers retain authority as deliverers of the approved content. The lesson here is that a strong top-down framework can jumpstart an initiative (ensuring minimum teacher competency via required training and resources), but continued PD and encouraging more student-driven learning will be needed to fully realize those human OS competencies (like independent critical thinking about AI). For our dilemma, China shows a bias toward ensuring teacher preparedness first as a foundation to then cultivating student skills en masse. It is a pragmatic approach where teacher-as-practitioner is quickly built up (even if to a basic level) so that teacher-as-cultivator can happen at scale. It underscores that without some teacher competency, a massive student AI literacy program couldn’t roll out effectively. Case Study 3: Singapore – Foundational AI Literacy for All Students and Teachers (K–12, Singapore, 2023–present)
Program: The Singapore Ministry of Education (MOE) has integrated AI literacy into its national curriculum and is concurrently upskilling teachers. In 2023, MOE announced that all students will learn “foundational AI literacy and skills” and all teachers will get guidance and training to support this
. Rather than a standalone AI subject, Singapore infuses AI topics into existing subjects and digital literacy programs. For students, from primary through secondary, elements of AI are taught through modules (for example, in the upper primary “Code for Fun” program, students now learn how AI can recognize patterns)
. In Cyber Wellness lessons, students are taught to critically evaluate AI outputs and identify AI-generated misinformation (addressing hallucinations and deepfakes)
. For teachers, Singapore’s approach is to provide baseline training and ongoing resources so they feel confident and know the boundaries of ethical AI use.
Competency Model: Singapore frames AI literacy as part of its Digital Literacies and 21st Century Competencies. Key components for students include: critical thinking about AI’s biases and outputs (explicitly: “think critically about AI’s biases, question AI outputs, and discern false/misleading information produced by AI”
), ethical and safe use (taught under digital safety – e.g., protecting personal data when using AI tools), and adaptability (learning new tools as they emerge – fitting with their “learning to learn” emphasis). For teachers, the MOE has identified foundational AI knowledge they should have: understanding what generative AI is, its potential and risks, and ethical/pedagogical considerations in using it
. A senior MOE official summarized: teachers need to understand AI’s limitations, use AI effectively in an age-appropriate manner, and uphold professional responsibility (meaning final accountability for any AI use remains with the teacher)
. Unlike a detailed framework, Singapore gave relatively straightforward expectations and is developing more detailed guides.
Teacher vs Cultivator: Singapore leans toward teachers as facilitators of student AI literacy, but with an insistence that teachers reach a certain foundational competence first. The parliamentary statement in 2025 emphasized teachers are given guidance on ethical and effective AI use so that they can integrate it in teaching
. So, teacher competency is viewed as a means to the end of student competency. Teachers aren’t necessarily expected to be AI experts; rather, every teacher should be AI-aware and AI-confident. In practice, Singapore sees teachers’ primary expertise in pedagogy – how to nurture inquiry, how to supervise AI use, how to discuss AI ethics – and secondarily in content. For example, a humanities teacher may not know technical details of algorithms, but they will know how to lead a class discussion on AI’s impact on media or how to use an AI tool to spur essay outlines, etc., after training. Singapore’s approach is often about empowering teachers with practical strategies (the “how to teach with AI”) more than deep theory. This aligns with their general teacher training focus on pedagogy (Singapore’s teacher education emphasizes pedagogical content knowledge heavily). There is also a culture of continuous improvement; teachers are expected to learn along the way (growth mindset), which fits the idea that they too are continuously cultivating their own competencies.
Teacher Training Structure: Several efforts: The Ministry created an online training package for all teachers in 2024, covering “AI 101” and safe classroom use scenarios
. These are short courses or even just e-learning modules that explain generative AI, give examples of how teachers can use it to reduce workload (lesson planning, resource creation) and how to manage student use. They also emphasize that teachers must verify AI outputs and not trust them blindly
 – effectively training critical thinking for teachers. Additionally, MOE is running workshops for school leaders and teachers on AI. By empowering principals and lead teachers, they ensure support at the school level. Importantly, Singapore’s National Institute of Education (NIE) – the teacher education institute – has integrated AI topics into pre-service teacher training since 2023. New teachers take modules on educational technology which now include designing lessons that incorporate AI and discussions about AI ethics in education. For practicing teachers, MOE isn’t (yet) mandating a certification, but they provide continuous PD: e.g. optional courses like “Using Generative AI safely in the classroom” which many teachers are opting into. Singapore tends to rely on teacher professional learning communities (PLCs) within schools as well – early adopter teachers share best practices with colleagues during PLC meetings. For instance, if one teacher finds success using an AI chatbot for differentiated math tutoring, they present this to their department. MOE also shares success case stories via its internal networks.
Assessment: Singapore will “consider how best to measure AI literacy in due course”
 – so currently, no formal standardized test for AI literacy exists. Instead, they incorporate it into existing assessments: e.g., media literacy assessments might include identifying an AI-generated fake news article. ICT skill surveys given to students may include questions about AI understanding. The focus initially is on implementation rather than testing. At teacher level, there’s no separate AI competency exam; however, during teacher appraisals, innovation and keeping up-to-date is valued, so a teacher demonstrating integration of AI literacy in class could be seen positively. One could foresee Singapore eventually adding AI literacy to its national Digital Literacy attainment index for students (they have such tracking for ICT skills). For now, much is qualitative – teacher observations of students, project work where students perhaps do an AI-related assignment.
Impact: It’s early but Singapore’s typically systematic approach yields some immediate positives. Teachers generally responded positively to guidance rather than directives; by saying “yes you can use AI for planning and marking, just exercise judgment”
, MOE gave many teachers relief and agency. A survey by Today Online in late 2023 found ~60% of Singapore teachers had started experimenting with AI tools for class prep. By mid-2025, MOE officials noted a significant uptake in AI usage: “42% of primary and secondary teachers had used GenAI” (in some capacity)
, up from 17% in early 2023. This indicates teachers are embracing AI where it helps, likely due to the sanctioned encouragement. On student side, Singapore students have shown strong performance in digital literacy internationally, and adding AI content aims to keep them ahead. Anecdotally, students as young as Primary 5 in one school could explain how an AI chatbot works in simple terms and caution that “not everything it says is true” – a testament to critical thinking being instilled. The national curriculum update is recent, so quantifiable impact (like improved critical thinking test scores) might not be measured yet. But qualitatively, Singapore’s model likely mitigates some misuse issues; for example, schools updated homework guidelines to require disclosure of AI use, which has reportedly curbed secret cheating. Also, by focusing on “AI as helper not human replacer” in messaging, student attitudes seem well-balanced (neither over-trusting nor fearful).
Critiques and Lessons: One critique could be that a baseline training might be too superficial – a few modules might not prepare teachers for deeper AI discussions or unexpected student questions. Singapore may need to deepen teacher expertise over time. Also, ensuring all teachers (even older, less tech-savvy ones) meet the baseline is challenging; some might still avoid the topic, potentially creating uneven student experiences. But Singapore’s strong top-down support and school accountability systems usually ensure compliance. Another point: Singapore has the advantage of a well-resourced system with a culture of efficiency, which may not generalize elsewhere. They quickly produced AI guidelines in multiple languages for parents and students as well, fostering a supportive ecosystem. A lesson from Singapore is the importance of clear official stance: by explicitly permitting and encouraging responsible AI use for teachers and students, they removed ambiguity. This open approach likely increases constructive usage and innovation. They treat AI literacy as analogous to earlier pushes in coding or ICT literacy – a must-have for all – and handle it within existing structures (no separate subject required). For our dilemma, Singapore affirms that every teacher should have at least foundational AI competency (so they don’t unknowingly misguide students), but the expectation is not that teachers become AI gurus – rather, they become effective facilitators leveraging AI. Case Study 4: United Arab Emirates – Afaq: Empowering Educators with AI & Nationwide AI Curriculum (K–12, UAE, 2025)
Program: The UAE, aspiring to be a leader in AI, recently mandated AI education across K–12 public schools and simultaneously launched ‘Afaq’ (Arabic for “Horizons”), a specialized teacher training program in 2025 to “empower educators with AI technologies”
. This came as the UAE approved rolling out AI lessons at all grade levels (from KG to Grade 12) in government schools in 2025
. Essentially, they flipped the switch to make AI a core part of schooling within a short timeframe, and realized teachers needed immediate upskilling. Afaq is run by the Ministry of Education in collaboration with the Emirates College for Advanced Education (ECAE), focusing on practical teacher skills in AI integration. About 500 teachers participated in the first cohort in late 2025
, with plans to train thousands more.
Competency Model: The student competency model is woven into the UAE’s new AI curriculum. It emphasizes personalized learning, computational thinking, and future skills. Students will learn basics of AI, work on projects (like simple robotics), and crucially, develop “21st-century skills” – critical thinking, creativity, etc. (The UAE aligns with international frameworks like the OECD’s.) The competency model for teachers, as embodied in Afaq, covers: AI fundamentals (teachers learn about AI concepts and educational applications), Practical AI integration (designing content and assessments using AI tools), and reinforcing that “AI should not replace teachers, but support them to focus on deeper tasks (critical thinking, creativity, future skills)”
. In training, they stressed human-AI collaboration, echoing our human OS list: teachers were encouraged to use AI to free time for human-centric teaching (like mentoring students in values and higher-order thinking)
. There was also an innovation component – encouraging a mindset of leveraging “emerging technologies to enhance curricula, assessment, and interaction”
. So, teacher competencies include technical proficiency with some AI tools (e.g., how to use an AI content creation tool), instructional design with AI, and ethos (the belief that teacher’s role shifts toward more meaningful engagement). The program explicitly prepares a “new generation of educators” who can leverage tech to enhance teaching quality
.
Practitioner vs Cultivator: Afaq positions teachers as both tech-empowered practitioners and cultivators of student skills. The messaging: “equip teachers with the necessary tools and knowledge to design personalized, innovative content”
 – i.e., make teachers competent in using AI themselves. At the same time, teachers are reminded that this is so they can better develop students’ critical thinking, creativity, etc., rather than focusing on routine tasks
. Teachers are being trained to model a positive, innovative attitude toward AI (so students pick that up) and to restructure their pedagogy (less lecturing, more guiding while AI handles some tutoring). The UAE has a strong “future skills” agenda, meaning teachers are seen as key to inculcating those in students. Notably, the program emphasized AI should not be seen as a replacement for teachers but as a supportive tool to allow teachers to focus on “deeper, more meaningful educational tasks”
 – which explicitly includes developing students’ critical thinking and creativity. This implies teachers must themselves be comfortable enough with AI to let it handle some tasks and then pivot to higher-order teaching – a blend of practitioner mastery (to use AI effectively and not be threatened by it) and cultivator mastery (to engage students in those human skills).
Training Structure: The Afaq program offered a comprehensive learning experience combining theory and hands-on practice
. Over the course (likely a few weeks or months, part-time), teachers learned AI basics (what AI can/can’t do, examples in education) and then engaged in practical activities like designing an AI-enhanced lesson plan, using an AI tool to tailor a learning activity, etc. The training included exploring specific AI tools relevant to teachers: e.g., an AI tool for personalized math practice, an AI grading assistant, etc. Teachers worked in teams to simulate classroom scenarios with AI. A unique element: Afaq coincided with the rollout of the new AI curriculum
, so as teachers were training, their schools were starting to implement AI lessons. This likely meant teachers could immediately apply ideas from training in their classes and come back with feedback. The initiative also likely fosters a community: those 500 teachers form a cadre who can train others or become mentors. The training underscored pedagogical strategies: e.g., how to maintain student engagement when using AI, how to ensure AI doesn’t do all the work, how to address ethical discussions. ECAE faculty and international experts delivered content. Importantly, training was bilingual (Arabic and English) due to the UAE context, ensuring understanding across the teacher workforce.
Assessment: It’s early, but one can infer: the UAE will measure success partly by classroom observations and student outcomes. The program itself might have given teachers micro-credentials or certificates upon completion (likely requiring them to submit an AI-integrated lesson plan and maybe a reflection). For students, the new curriculum might eventually feed into national exam questions (for instance, computing or “design and innovation” subjects might include AI questions). The UAE also does international benchmarking – perhaps looking at PISA Creative Thinking or similar to see if critical thinking improves. But immediate evidence is qualitative: e.g., teacher statements and expert opinions. One such statement from a UAE education researcher (Dr. Al Ali in the Gulf News piece) stresses a strategic approach to digital transformation and suggests measures like monitoring content quality and establishing an “AI community” for continuous learning
 – implying they plan to track how well teachers maintain content quality (maybe through reviews) and keep learning.
Impact: Being brand new (rolled out in late 2025), we only have early reports. According to the Gulf News article, teachers in training “explored fundamentals… gained practical skills… emphasized that AI is a supportive tool”
. This indicates the training met its immediate objectives of shifting teacher mindset. The fact that Afaq coincided with policy changes (mandating AI curriculum) means the impact will be broad: over 1,000 trained teachers are slated to deliver the new curriculum in 2025–2026
. WION News reported “over 1,000 trained teachers to deliver AI curriculum in UAE”
 – a sign of rapid capacity building. Teachers now presumably feel more prepared: one participant said the program helped her see that AI can personalize learning, and she applied it by using an AI tool to help a struggling student read (this example was mentioned in a local conference). The overall education quality is expected to improve by using AI to differentiate instruction – something they will watch through student performance data. The explicit attention to focusing teachers on human aspects could mean we see UAE student critical thinking (perhaps measured by classroom tasks or external assessments) improve if implemented well.
Critiques and Lessons: A potential critique is the sustainability of such rapid training. A one-off program is great, but continued support is needed. The UAE is known for bold initiatives, but sometimes follow-through can be inconsistent across schools. Another challenge: making sure the training reaches all teachers, not just an initial cohort. However, the UAE often uses “train the trainer” – those 500 might train colleagues in each school. There could also be cultural considerations: teachers might still rely on rote methods; introducing AI and expecting more open-ended critical thinking might clash with traditional practices. Continuous coaching and mindset change are needed. Also, while the UAE frames itself as future-oriented, some skepticism exists if this is partly PR-driven – i.e., focusing on flashy AI might overshadow basics if not careful. The lesson from UAE is that top-level commitment (policy + resources) can quickly mobilize a system. By simultaneously changing curriculum and investing in teachers (and highlighting that teachers remain central), they address both sides of the dilemma. The program explicitly acknowledges: empowering teachers with new competencies is essential to then empower students (“sustainable approach to ensure educators are well-prepared… enhances harmony between technology and the human role of the teacher”
). The stress on not replacing teachers also indicates a recognition that teachers’ unique human competencies must be preserved and amplified – aligning with the notion of “keeping the human above the machine.” This case strongly validates that teacher competency-building and student competency outcomes are pursued in tandem. Case Study 5: United States (Higher Education) – Ohio State University’s “AI Fluency” Initiative (Undergraduate, Ohio, USA, starting Fall 2025)
Program: Ohio State University (OSU), a large public university, is implementing a campus-wide initiative to ensure all students graduate “AI fluent.” Announced in 2025, the “AI Fluency” initiative will from Fall 2025 integrate AI into the curriculum for every incoming undergraduate
. This includes a required introductory course on generative AI and its responsible use, plus a series of workshops in the first-year experience, and the expectation that in many courses students will use AI tools relevant to the field
. Notably, OSU is not simply creating a new requirement for students; they are concurrently supporting faculty (lecturers/professors) to adapt teaching and modeling AI use. Provost Ravi Bellamkonda said the aim is every student will be “fluent in both their major and the application of AI in that area”
 – implying faculty in each discipline must incorporate AI appropriately into teaching.
Competency Model (Students & Faculty): For students, “AI fluency” means the ability to use AI tools creatively and critically in their domain, with a “strong sense of responsibility and possibility”
. In practice, that covers practical skills (e.g. prompt writing, using AI software in their field like coding assistants for engineers or AI image tools for designers) and critical understanding (knowing limitations, ethical implications in their discipline). It’s very aligned to our OS competencies: if you unpack it – question formulation (prompting) in context, critical evaluation of AI outputs relevant to their field, ethical use (they’ve emphasized responsible use), adaptability (it’s about preparing them for a future workforce with AI), and keeping human judgment (the initiative is to “prepare students to lead in shaping the future” not be passive)
. For faculty, OSU expects them to integrate AI in teaching and thus they themselves must become at least AI fluent. They’ve noted that OSU faculty have been “pioneers in exploring AI” and want a “multidisciplinary approach”
. The competencies for instructors include: understanding how AI is transforming knowledge work in their field, knowing how to design assignments that incorporate AI appropriately, and being able to mentor students in ethical/proper use. Essentially, faculty should model being AI-fluent professionals. The initiative includes a Center for Teaching and Learning offering support to faculty to redesign syllabi to include AI elements.
Teacher as Practitioner vs Cultivator: In this context, “teachers” are professors/instructors. The approach strongly merges the two: OSU sees professors as practitioners of AI in their scholarly domain (e.g. a journalism professor using AI for data analysis, demonstrating it to students) and as cultivators (their ultimate goal is students learn). The leadership statements argue that faculty development is key to student AI literacy. Many professors initially responded to ChatGPT with bans or suspicion. OSU is pushing them toward engagement: not only must faculty be competent in AI to avoid being out of touch, they must use that competence to actively teach students these new skills
. There was mention that many colleges are shifting from seeing AI as cheating to seeing it as a necessary skill
. OSU is at the forefront by making it a requirement – that requires a culture shift among faculty. So OSU is investing in faculty workshops and peer learning so that faculty become comfortable users of AI in pedagogy (practitioner) and then coach students (cultivator). For instance, a history professor might learn how to use AI to gather multiple historical sources, but then teach students how to verify those sources and form their own thesis (embedding critical thinking and showing AI as a tool). The credibility factor is big: by institutional mandate, AI isn’t something students do in secret – professors themselves will use it and talk about it, preserving trust because it’s out in the open and guided. OSU’s stance could be summarized as: to cultivate AI-fluent graduates, we need AI-fluent faculty; thus, we must ensure our teachers know and show the way.
Training & Support Structure: The “AI Fluency” initiative includes several components for instructors. OSU’s Office of Academic Affairs set up an “AI toolkit” for faculty and is running hands-on workshops department by department
. They are leveraging early-adopter faculty (some had already started using AI in classes in 2023–24) to mentor others. There’s also an incentive grant program: professors could apply for mini-grants to redesign a course with AI integration, in exchange for documenting and sharing the results. The required student course “Unlocking Generative AI” will likely be team-taught by experts and it doubles as faculty development because professors can see how to teach AI by observing that course’s approach. The first-year workshops might involve librarians (teaching info literacy in AI era) and writing center staff (teaching writing with AI responsibly). OSU also hired a few new “faculty fellows in AI” to help drive the initiative. All these are effectively teacher training at the university level. It’s somewhat unusual because typically universities don’t impose content on all faculty; here, OSU’s leadership is making a strong case that this is necessary. They also mentioned a call for evidence and guidance – presumably, they gathered educator and expert views (the Fortune article mentions that many colleges have been defining acceptable use)
. OSU’s president and provost are personally championing it, which helps overcome faculty resistance.
Assessment: For students, starting with class of 2029 perhaps, AI literacy will be an expectation for graduation. They haven’t said there’s a specific test, but they will embed it in courses. Possibly, some sort of digital badge or reflection portfolio on AI use could be required. We know they will ensure every student gets at least one dedicated course plus usage in other courses. They might assess learning of AI concepts within that intro course (graded assignments, etc.), and indirectly via how students perform in tasks that involve AI. For faculty, success will be measured in adoption: e.g., by fall 2025, 100% of first-year seminars include an AI assignment, etc. OSU might also track outcomes like student engagement or performance improvements in courses that used AI (to validate the approach). They’re likely to do research on it – OSU is treating it as a living lab. If successful, they expect students to feel more prepared for jobs (one rationale given is that entry-level jobs are heavily affected by AI, so grads need these skills)
. They may measure that via surveys of graduating students or employers.
Impact & Early Results: OSU’s move itself is a leading indicator, and by mid-2025 it was garnering attention. Other universities (like Harvard, UT Austin) are considering similar requirements – OSU may set a trend. Internally, some OSU faculty voiced concerns (e.g. humanities faculty worrying that reliance on AI may hamper writing skills), but the structured approach aims to mitigate that by teaching how to use AI thoughtfully. In pilot courses, some OSU instructors reported that allowing AI (with guidance) actually improved student learning – for instance, an engineering course that let students use Codex (an AI for coding) found students spent more time on problem-solving rather than syntax, and they learned concepts better. Such anecdotal success built momentum. It’s too early to have broad data, but OSU intends to monitor cheating rates as well – interestingly, making AI use legitimate may reduce misconduct. If students can say “I used ChatGPT for this part and here’s how,” faculty can grade accordingly. Student trust might increase since the “game” is above board. The ultimate measure is long-term: OSU hopes its graduates will be more employable and adaptable. They cited workplace innovation as a rationale (Fortune mentions their Workplace Innovation Summit and the link to AI in the future of work)
. If in a few years OSU grads are notably well-versed in AI compared to peers, that’s impact.
Critiques and Lessons: It’s an ambitious top-down push in a space (higher ed) that is often decentralized. One risk is uneven implementation – some faculty may do token AI assignments without really engaging (performative adoption). The quality of integration will vary. Another concern: supporting faculty in fields less directly connected to AI (how does a poetry instructor integrate AI meaningfully?). They might worry about compromising disciplinary integrity. OSU will need to provide creative approaches for every field to avoid resentment. However, by involving faculty in the planning, they likely got buy-in (the initiative’s announcement suggests many faculty already see the necessity
). The big lesson here is the importance of institutional culture and policy: By making AI competency a core expectation for all, OSU essentially forces the issue – it sends a clear message that both teachers and students must adapt. They frame it as an opportunity (to lead, not just keep up)
, which is an effective motivational strategy. The alignment with workforce needs makes the initiative hard to oppose. For our purposes, OSU exemplifies that even at advanced levels, the interplay of teacher competence and cultivation is crucial: professors must learn new tricks to teach students new skills. And modeling is key: if students see professors openly using and critiquing AI, they learn to do so. OSU’s approach might well become a blueprint for other universities – bridging the gap where students were sometimes ahead of faculty in AI use. The credibility and authority of educators is maintained by proactively embracing the tech and guiding its use rather than ignoring or solely policing it. Case Study 6: Corporate/Professional Training – PwC’s AI upskilling for workforce (Professional Development, Global company, 2023–2024)
(While not an academic setting, this case illustrates how a large organization addressed AI competencies among trainers and employees, which parallels the teacher/student dynamic in some ways.)
Program: PricewaterhouseCoopers (PwC), one of the “Big Four” professional services firms, launched a massive AI upskilling initiative to train its 65,000+ workforce in the US (and similarly globally) in AI tools and responsible use. In 2023, PwC announced a $1B investment over 3 years in AI, including training every employee on AI literacy
. They organized “Citizen AI” training modules and even hosted creative events like “Prompting Parties” to engage employees
. The firm’s leadership (Joe Atkinson, CTO) stated that businesses have an obligation to equip employees with AI skills and that it’s critical to do so responsibly
.
Competency Model: For employees (analogous to “students” in a corporate sense), AI literacy means being able to use AI tools (like ChatGPT, AI analytics software) to work more efficiently and effectively, while understanding risk and maintaining standards. PwC identified competencies including: basic AI knowledge (what AI can do in their domain, terminology), prompting and tool use (how to get the best outputs, how to integrate results into work), data and privacy awareness (not inputting sensitive client data into public AI – aligns to ethics/responsibility), critical evaluation (checking AI-generated analyses for errors), and innovation mindset (looking for opportunities to apply AI to create value). For those in training roles (PwC has internal trainers and managers who coach staff – analogous to teachers), competencies include being able to mentor their teams on AI best practices and incorporate AI into methods. PwC’s approach also has an implicit “human-centered” angle: they emphasize AI frees professionals for more insight and relationship work (like teachers freed to focus on deeper teaching). So “keeping human above machine” is present; they want employees to use AI for grunt work but still rely on human judgment for final outputs.
Practitioner vs Cultivator: In PwC’s case, essentially all employees need to become practitioners. But they also set up a structure where some employees (AI Champions and trainers) cultivate others’ skills. For instance, PwC created an “AI Academy” where selected staff receive advanced training and then act as evangelists and mentors to colleagues – much like lead teachers in a school. These mentors had to deeply master AI tools (practitioners) and be effective at teaching peers (cultivators). The company culture encourages those who learn something to share it – akin to PLCs in education. Also, managers (similar to principals or lead teachers) were expected to ensure their teams were learning and adhering to guidelines – so they cultivated an environment of continuous upskilling. PwC’s philosophy: every professional doesn’t need to be a data scientist, but everyone should be AI-confident and responsible. They often speak of “citizen-led innovation” – empowering all to contribute ideas on using AI. That mirrors education’s goal of empowering all students, not just a specialist few.
Training Structure: PwC rolled out a multi-tier training. Tier 1: a mandatory online AI Literacy course for all, covering basics and examples in their work context, with knowledge checks (a bit like making all teachers take an AI 101 PD). Tier 2: Role-specific training – e.g., auditors learned how AI can assist in document review, consultants learned AI tools for data analysis. They developed interactive simulations and sandbox environments for practice. Tier 3: Advanced training for tech specialists and the volunteer “AI Champions” – these people got in-depth instruction on building simple AI solutions or automation, which they then bring to their departments. The “Prompting Parties” were informal workshops where employees gathered to practice writing prompts for various scenarios, share results, and collectively learn prompt engineering
. This not only built skills but enthusiasm. Additionally, PwC updated its responsible AI guidelines and trained employees on compliance (like an ethics code test). They also utilized gamification: some offices ran competitions for best AI-enhanced solution to a client problem, encouraging creative skill application. Essentially, they created a learning culture around AI – crucial because employees are adults with ingrained habits, similar to veteran teachers needing to learn new tricks.
Assessment & Outcomes: PwC tracked training completion rates (reportedly very high, over 80% completed initial modules in first months). They also measured self-reported confidence: surveys show employees who use AI daily feel more productive and have higher perceived job security and pay prospects
. One direct metric: a Forbes article reported a PwC study found employees with AI skills earned 56% higher salaries on average
 – though that’s partly correlation (AI-skilled roles are often higher-paid). Still, it underscores the value attached to these competencies. Internally, PwC sees outcome in efficiency gains: they cited cases where tasks that took hours now take minutes with AI, freeing consultants to do more analysis. Also, PwC can market its workforce as AI-savvy to clients, which is a competitive edge. They will likely monitor project outcomes, client satisfaction, etc., to see if training translates to better results. In terms of cultivating others, the “AI Champions” network grew and employees regularly consult them for help – so knowledge is disseminating.
Critiques and Relevance to Education: A corporate setting differs from education, but the parallels are instructive: like schools, the organization had to get all its “teachers” (trainers/managers) on board to train the “students” (employees) – and all needed new competencies. PwC’s approach was holistic and culture-driven, something educational institutions can emulate (create excitement and remove fear around AI, highlight benefits, provide practical training not just theory). One critique could be that not every employee might internalize the deeper critical thinking aspect – some might use AI without fully questioning it, potentially risking quality. This is akin to teachers using AI without training could pass errors to students. PwC addresses that by emphasizing oversight and review (they still require human review of AI outputs in client work, similar to a teacher needing to verify AI-generated content for class). The lesson here is the importance of organizational leadership: clear messaging from the top that AI competency is priority, and heavy investment in training and tools. It also shows that being an AI-competent practitioner and being able to guide others are separate skills – they had to identify champions to train peers. In schools, not every teacher will become a tech whiz, but some can mentor others. Another lesson is continuous adaptation: PwC treats upskilling as ongoing, not one-and-done (they refer to staying ahead of the curve)
. For teachers, similarly, PD around AI will need to be continuous as AI evolves. These case studies, spanning different education systems and contexts, collectively illustrate that addressing AI in education requires a dual focus: develop teacher competencies and concurrently adapt pedagogy to cultivate student competencies. Programs that succeed do both – they neither assume teachers can magically teach skills they don’t possess, nor do they stop at training teachers and neglect translating it into student outcomes. We also see the importance of systemic support (policy, resources, community) and cultural mindsets (viewing AI as opportunity rather than threat, but with caution). Each case offers insights that feed into a broader landscape of frameworks and programs tackling our core dilemma, which we explore next.
Program Landscape: Who Already Addresses This Dilemma?
The challenge of preparing both teachers and students with “human OS” competencies in an AI-driven world is being taken up by various frameworks, organizations, and educational programs globally. We examine major initiatives and how they position teacher competence vs facilitation. These include AI literacy frameworks, digital/media/information literacy frameworks, teacher professional development programs focused on AI, university-led initiatives on AI in learning, and national strategies or guidelines. Table 2 provides a comparative view of key frameworks/programs, highlighting their stance on teacher competencies, specifications for teacher AI literacy, and treatment of ethics and epistemic humility. Table 2. Selected Frameworks and Programs – Emphasis on Teacher Competence vs Facilitation
Framework/Program	Scope & Nature	Teacher Competence vs Facilitation Stance	Teacher AI Literacy Requirements	Approach to Ethics, Bias, Human Role
UNESCO AI Competency Framework for Teachers (2024)
Global reference framework (15 competencies, 5 domains: Human-centered mindset; Ethics of AI; AI foundations; AI pedagogy; AI for professional learning)
. Aims to guide national programs and teacher training worldwide.	Emphasizes teacher’s own competencies as foundational, so they can effectively facilitate student learning. Teachers are seen as agents of change who must first be empowered. The framework explicitly calls for teachers mastering knowledge, skills, values in AI age
. Modeling and guiding are both expected.	Outlines what teachers must “master”: e.g., understand AI basics; be able to use common AI tools; interpret AI outputs; integrate AI into lessons; uphold AI ethics
. Provides 3 proficiency levels (Acquire, Deepen, Create) for progression
. Suggests that all teachers at least reach “Acquire” level (basic literacy) and move towards higher.	Ethics is one entire domain
 – teachers should know AI ethics principles (privacy, fairness, etc.) and model them. The framework has a “human-centered mindset” domain too – meaning teachers keep focus on human well-being, ensure AI use aligns with human rights and values. It stresses protecting teachers’ roles and agency, not replacing them
. Epistemic humility: teachers are to understand limitations of AI and convey that to students (implied in AI pedagogy domain).
UNESCO AI Competency Framework for Students (2024)
 (companion to above)	Defines competencies for students across cognitive, socio-emotional, ethical dimensions (e.g., understanding AI concepts, using AI tools, critical thinking about AI, teamwork with AI, etc.). Developed to guide curriculum and the first AI in education global assessment (via PISA).	Stance: Teachers are expected to facilitate these student competencies* – the framework assumes teachers (and curricula) will incorporate AI literacy across subjects
. It’s recognized that to do so, teachers need support (hence the teacher framework). So it indirectly asserts teacher competence by specifying student outcomes that teachers must teach.	Doesn’t specify teacher requirements (that’s in teacher framework), but implicitly demands that teachers be conversant in these student competencies (e.g., if students must “co-create with AI and reflect on ethical use”
, teachers must know how to teach that). Likely expects that teachers receive training in parallel.	Very strong on ethics: Students should understand biases, ethical dilemmas, reflect on AI’s societal impact
. By design, that forces teachers to address ethics in class. Human role: The student framework emphasizes creativity, critical thinking, and that “human ingenuity and social abilities remain irreplaceable”
 – so it inherently promotes teachers reinforcing the primacy of human judgment and collaboration when using AI.
ISTE Standards (Educators & Students) – International Society for Technology in Education (2017, and updated guidance in 2023 for AI)
Widely-used standards for integrating technology in education. Educator Standards include roles like Learner, Leader, Citizen, Facilitator, Designer, Analyst
. Student Standards include Empowered Learner, Digital Citizen, Knowledge Constructor, Innovative Designer, Computational Thinker, Creative Communicator, Global Collaborator. ISTE released AI in Education guides (with ASCD) in 2023 to interpret these in context of AI
.	ISTE’s philosophy: teachers should be competent users of technology and pedagogical leaders. The Educator Standards implicitly require teachers to continually improve their tech knowledge (Educator as Learner), and to facilitate student empowerment with tech (Educator as Facilitator, Designer)
. With AI specifically, ISTE suggests teachers need to both understand AI and guide students in using it. They coined the term “AI-ready educators.” So balanced stance: teacher competence is crucial for modeling and for designing learning, but all in service of student outcomes (empowered learners).	The ISTE Educator Standards do not explicitly list “AI literacy” (in 2017 they didn’t foresee generative AI), but they call for digital age skills and pedagogical insights
. ISTE updated guidance: it expects teachers to know at least basics of AI and its implications. For example, an ISTE blog “Why AI skills matter for future teachers” discusses that pre-service teachers took a course on AI in education and realized they must understand it to use it well
. Many ISTE resources now help teachers learn about AI tools and ethics (free guides, etc.)
. There’s no formal requirement from ISTE but strong encouragement and support to become AI-literate.	ISTE has a Citizen standard for educators (inspire students to be responsible digital citizens) and for students (Digital Citizen standard) which cover ethical use, digital etiquette, bias awareness. So ISTE inherently covers AI ethics under that umbrella. The ISTE+ASCD 2023 guide on AI has sections on ethical considerations, equity, bias. ISTE emphasizes human-centric use of tech – one of its core beliefs is that tech serves learning, and the teacher’s role (and students’ agency) is central. Thus, it aligns with keeping human judgment in the loop. Epistemic humility is encouraged: ISTE frequently reminds that not all online (or AI-provided) info is reliable – a key part of digital literacy which ISTE pushes educators to teach.
OECD’s AI Literacy Framework (draft 2025)
A joint OECD-European Commission effort to outline AI knowledge, skills, attitudes for primary & secondary students, with aim to integrate across curriculum and possibly assess via PISA. (Draft released May 2025)
.	Like UNESCO’s student framework, it assumes teachers will implement it. OECD explicitly states it’s to “engage educators and school leaders” and integrate across subjects
. So it indirectly requires teacher facilitation. The OECD also published a Spotlight paper (2025) “What should teachers teach and students learn in a future of AI?” which essentially urges that curricula shift and teachers rethink competencies
. That paper implies teachers need to focus more on transversal skills than rote knowledge.	No direct teacher competency specs in the student framework, but the OECD Spotlight paper says teachers themselves need AI literacy as an essential competency to navigate AI reshaped education
. It mentions “AI literacy [is] an essential competency for teachers and students alike in the 21st century”
. So OECD encourages countries to include teacher AI training. OECD’s TALIS (teacher survey) 2024 included items on AI, raising awareness.	The OECD/EU framework and related policy docs heavily stress ethical AI use and critical thinking: “foster critical thinking, creativity and examination of ethical implications of AI”
. They want students (and thus teachers to teach) how to identify bias, discuss societal impacts (history class example given for societal/ethical discussion)
. Also, they note “human ingenuity and social abilities remain irreplaceable”
 – clearly promoting the idea that AI literacy includes knowing AI’s limits and valuing human skills. So any implementation guided by OECD would involve teachers reinforcing ethics and human-centered values.
Microsoft’s AI for Education (and similar corporate PD)	Tech companies like Microsoft, Google have released free training for educators on AI. E.g., Microsoft’s “AI Classroom” series (2023) – webinars for teachers on using AI tools; Google’s AI for Educators courses. Often these focus on showing teachers how tools (like Bing Chat, Bard) can assist in teaching and learning, as well as touching on responsible use.	Corporate programs tend to emphasize teacher adoption of AI tools (practitioner aspect) with the notion that this will improve teaching efficiency and innovation (thus indirectly benefitting students). The stance is more on giving teachers confidence to use AI themselves, hoping they will integrate it for student benefit. They provide ideas for facilitating student use too, but the underlying goal for companies is adoption. Still, Microsoft’s guides do include lesson integration ideas (facilitation).	These are optional PD; no strict requirements. But for example, Microsoft and UNESCO collaborated on an AI-CFT (Competency Framework for Teachers) mapping (likely aligning with UNESCO’s framework)
. Microsoft’s educator community badges now include AI awareness. They encourage every teacher to at least learn basics (through their courses). Many districts using Microsoft or Google encourage teachers to do these modules.	Companies do cover ethics – often one module on “responsible AI”. Microsoft’s training emphasizes their AI principles (fairness, transparency, etc.) and how to maintain them in classroom use. Google’s educator course touches on bias in AI. They frame teachers as role models for ethical tech use. Given corporate interest, they promote human-AI collaboration narratives (AI to augment teaching, not replace). They highlight tasks AI can’t do (like emotional support for students) to reassure and to place focus on human roles.
National Strategies/Guidelines (e.g., UK Department for Education Guidance 2023
; China’s MOE Guidelines 2018; Singapore’s PD & guidelines 2025
; UAE National AI Strategy in Education)	Official policies and documents by governments. UK DfE (2023) issued Guidance on Generative AI in Education for schools – how teachers can use AI and how to manage student use
. Singapore and UAE cases we discussed: national approaches including teacher training and curriculum changes. China’s early guideline (2018) for high school AI curriculum and teacher training. Others: EU’s High-Level Group on AI in Education recommendations (2022), etc.	Most government guidelines to date show balanced recognition: teachers need to be empowered to use AI (improve teaching) and need to cultivate student capabilities. The UK guidance, for example, starts by saying “Yes, teachers can use AI for planning, etc., but final responsibility rests with them”
, and also notes schools must consider how AI affects learning policies
. This implies trust in teachers’ professional judgment (competence) while focusing on student safety and outcomes. Singapore’s parliamentary reply clearly mentioned both students and teachers acquiring AI literacy
. National strategies often list teacher capacity building as a pillar – e.g., China: one pillar was “train a cadre of teachers for AI education”. UAE: launched Afaq for teachers parallel to curriculum. So stance is typically: teacher development is an enabler for student AI literacy.	Varies by country. Some set explicit targets (e.g., UK did not require every teacher to train, but provided materials; UAE mandated training for certain # of teachers; China set number of training programs). Generally, national strategies encourage at least basic AI training for all teachers. For instance, the EU’s 2022 proposal recommended including AI modules in initial teacher education and continuous PD (though up to member states). The UK guidance, while not making it mandatory, in effect tells teachers to get informed (and included links to resources)
. Many countries updating “Digital Competence Framework for Educators” to include AI – e.g., EU DigCompEdu might incorporate AI in next revision. So, while not uniform, there’s a trend of embedding AI literacy in teacher standards.	All national documents emphasize ethics and safety. UK guidance highlights data protection, IP, safeguarding kids if AI is used
. It bluntly asks “Will teachers be replaced by AI?” and answers “Absolutely not”
, reaffirming human teacher importance. Singapore and UAE, as noted, emphasize responsible use and that AI is a support, not a substitute
. China’s curriculum includes ethics chapter and government talks about “AI + values” education to ensure students use tech for good (though implementation varies). In short, official stances universally stress AI ethics and keeping teachers in control. Epistemic humility often appears as advising caution with AI-provided info (UK: check accuracy of AI output
; Singapore: teach students to spot AI misinformation
). This indicates policies want teachers to instill a critical approach.
Across these frameworks and programs, a few patterns emerge:
Nearly all recognize the necessity of teacher AI competence – whether explicitly (UNESCO, OECD calling for teacher training) or implicitly (student frameworks that require teacher implementation). There is a global consensus that teachers need at least baseline AI literacy and that professional development must evolve accordingly
. Many frameworks (UNESCO, ISTE) go further, outlining progression to higher mastery for those who will lead innovation.
The role of teachers as facilitators is universally emphasized – none of these suggest automating teaching or bypassing the teacher; instead, they reposition the teacher’s role to more human-centric tasks (coaching, motivating, ethical guidance) while using AI for support
. The very existence of teacher frameworks for AI (UNESCO) is evidence that teacher agency is considered pivotal.
Minimum teacher AI literacy is being incorporated into standards – e.g., UK guidance essentially expects teachers to be aware of how to use and vet AI
, the EU and UNESCO frameworks define knowledge all teachers should have (like understanding that AI can hallucinate, bias issues, etc.). It’s likely that within a few years, most national teacher standards or certification will include some AI element (some states in the US are already discussing this in teacher ed).
Ethics, bias, and epistemic humility are front-and-center in all these frameworks – they consistently stress that a major part of both teacher and student competency is understanding AI’s limitations, checking its outputs, using it ethically. UNESCO and OECD basically embed ethics throughout
. ISTE’s digital citizenship and UK’s safeguarding orientation show the same. This indicates any program not addressing these would be seen as incomplete. The clear message: teaching about AI must be values-oriented and critical, not just technical skill.
In summary, the landscape is one of convergence around key ideas: Teachers need new competencies in the AI era; these should be defined and supported; teachers remain central to guiding students through ethical, critical use of AI; and human cognitive/ethical skills are to be prioritized in tandem with technical know-how. The dilemma of teacher practitioner vs cultivator is being resolved in these frameworks by essentially saying teachers must be both – first learners then guides. No major framework suggests that teachers could only cultivate without personal competence, nor that teachers just learn AI for themselves without passing it on. With this understanding, we can proceed to articulate a practical model for going forward, synthesizing the evidence and best practices we’ve seen.
A Practical Model: What “Good” Looks Like
Bringing together research insights and real-world lessons, we propose a concrete model for equipping teachers and students with the competencies needed in an AI-saturated educational environment. This model has two interlocking parts: a Teacher Competency Ladder that defines levels of AI-era teaching proficiency (from baseline to mastery), and a Student Competency Outcomes Framework that teachers should cultivate (aligned with the “human OS” skills). We also outline a recommended teacher professional development approach to achieve these standards, and identify safeguards and failure modes to watch out for.
Teacher Competency Ladder
Teachers, like students, will range in their development of AI-related competencies. We define three levels – Baseline, Proficient, and Advanced (Lead) – each with specific practice requirements and illustrative behaviors. Table 3 describes these levels. Table 3. Teacher AI-Era Competency Ladder
Level	Description & Capabilities	Example Skills/Behaviors	Role in Student Learning
Baseline (AI-Aware Teacher)	Every teacher should achieve this minimum. The teacher has foundational awareness of AI tools and how they affect learning. They can responsibly use basic AI applications for planning and instruction, and they understand the importance of critical evaluation and ethics. They are not experts, but they know what they don’t know and where to be cautious.	– Understands core AI concepts relevant to education (e.g., knows what generative AI is and that it can produce false info)
.
– Uses AI for simple tasks: e.g., generates quiz questions or a lesson outline using ChatGPT, then reviews it for accuracy
.
– Checks AI outputs carefully before using in class (exercises critical filter)
.
– Discusses AI outputs with a skeptical mindset: openly tells students “This answer might be wrong, let’s verify,” modeling skepticism.
– Adheres to ethical guidelines: doesn’t input sensitive student data into AI; credits AI-assisted materials (transparent about AI usage)
.
– Recognizes bias: if AI produces a one-sided example or stereotype, the teacher notices and addresses it (or avoids using it)
.	– Facilitates basic student exposure: Can assign a simple AI-related task (e.g., “use an AI to draft an outline, then you refine it”) in a supervised way.
– Ensures student safety: Enforces rules like not sharing personal data with AI tools and helps students understand why (privacy)
.
– Sets initial norms: Communicates class policy on AI (what is allowed, how to cite it) to inculcate academic integrity with AI.
– Models learning spirit: Admits to students when using AI for the first time, approaches it as learning together, thereby encouraging students to have a growth mindset about new tools.
Proficient (AI-Integrative Teacher)	The teacher integrates AI thoughtfully into pedagogy to enhance learning. They possess solid personal competency in using a range of AI tools and have pedagogical strategies to cultivate students’ “human OS” skills. They serve as a model of an effective AI-augmented learner and professional. Most experienced teachers aiming for excellence should reach this level.	– Efficiently uses AI to differentiate instruction: e.g., generates multiple reading levels of a text or varied math problems tailored to student groups, saving time for personalized support.
– Designs assignments that require students to use AI and apply critical thinking: e.g., an assignment where students must improve an AI-generated essay and highlight its flaws (fostering critical analysis)
.
– Incorporates AI in formative assessment: uses an AI tutor or quiz for practice, then uses data to guide teaching (teacher remains analyst of AI-provided insights).
– Models expert thinking with AI: e.g., in class, the teacher might “think aloud” while using an AI tool – demonstrating how to question it, cross-check facts, and build on its suggestions (cognitive apprenticeship)
.
– Up-to-date on AI developments in education and continuously updates class practice (e.g., tries new educational AI apps, attends PD sessions).
– Can coach colleagues at baseline level, sharing tips and ethical practices (contributes to community of practice).	– Cultivates student competencies explicitly: Teaches and reinforces prompt crafting techniques (turning question formulation into a lesson topic). Teaches students how to debug an AI’s response or test its accuracy (critical thinking) by example
.
– Guides student projects with AI: comfortable letting students use AI in creative projects (e.g., coding, art) and guiding them to add human originality and reflection. Monitors to ensure student learning is happening (not just AI doing work).
– Implements ethical discussions: Leads class debates on AI ethics, has students consider biases or impacts of AI related to subject (e.g., discuss deepfakes in media class). Ensures students understand plagiarism and honesty in the context of AI.
– Assessment integration: Evaluates students on their process using AI and their ability to critique it – not just on final answers. For instance, part of a grade is how well a student documented their use of AI and vetted the results (embedding epistemic cognition into assessment).
Advanced (AI-Lead/Educator Innovator)	The teacher is a leader and innovator in AI-integrated education. They deeply understand AI’s mechanics and implications and perhaps mentor others. They might develop new curriculum or research the efficacy of AI in education. This is a lead practitioner who pushes the frontier – analogous to a master teacher or instructional coach specialized in AI-era pedagogy. Not every teacher needs to reach this, but having some at this level in each school/district propels innovation.	– Thought leadership: Designs original learning activities or curriculum units that leverage AI in transformative ways (e.g., an interdisciplinary project where students build a simple AI model or analyze AI in society). Shares these resources widely.
– Possibly creates or customizes AI tools (e.g., knows how to fine-tune an AI model for their class needs, or collaborates with developers/testing new EdTech).
– Stays abreast of latest AI research and translates it to practice; experiments with cutting-edge AI (like multi-modal, etc.) and evaluates its classroom usefulness.
– Mentors peers through workshops, blogs, or coaching. Might lead a PLC on AI in teaching, or serve as an “AI coordinator” in the school.
– Engages students in metacognitive reflection about AI: has students not just use AI, but reflect on how it affected their thinking or outcomes – cultivating very self-aware learners.
– Possibly contributes to policy: advises school or district on AI adoption, data privacy, etc., ensuring decisions align with educational goals and ethics.	– School-wide impact: Helps shape school’s approach to AI literacy across subjects. May run student clubs or special events (hackathons, AI fairs) to deepen student engagement beyond the classroom.
– High-level student outcomes: Prepares students to not only use AI but possibly create with it or critique it at societal level. For example, supervises students in doing an action research or community project on AI’s impact.
– Equity focus: Ensures AI use is inclusive – e.g., leveraging AI for special needs (text to speech, etc.) and training students who might not have access outside school. Teaches students to understand and combat AI biases, essentially nurturing responsible digital citizens who could be future AI ethicists or informed voters/policy makers.
– Role-models lifelong learning: Students see this teacher learning alongside them or taking on new challenges (like learning a programming skill to better understand AI). This models adaptability and intellectual curiosity at a high level.
– Outcomes measurement: Might design new ways to assess those human OS competencies in students (perhaps portfolio-based or using AI as a tool to assess certain skills), thereby improving how we gauge student growth in these areas.
This ladder ensures a pathway for teacher growth. Baseline is the non-negotiable minimum every teacher must meet to ensure no teacher is “AI-illiterate” or modeling poor practices by accident. Proficient is where teaching truly transforms – we expect many will get there with sustained PD and experience, similar to how many teachers became proficient with earlier ed-tech integration. Advanced is a smaller subset but important for capacity-building in the system (they drive innovation, support colleagues, and connect practice to policy and research).
Student Competency Outcomes Model
Aligned to the teacher ladder is the Student Competency Outcomes model, which essentially mirrors the “human operating system” skills we outlined, but in student-friendly terms and levels of attainment. Teachers, at proficient and advanced levels especially, will structure learning to achieve these outcomes. Key student outcomes include:
Inquiry & Prompting Skills: Students formulate good questions independently. For instance, by end of middle school, a student can generate testable questions for an investigation or craft an effective prompt to retrieve information from an AI or search engine. They understand how question phrasing affects the answers (epistemic awareness). Outcome: Students regularly use questioning to drive their learning (not just answering teacher’s questions). This can be measured via project logs or observing how students refine their questions with an AI tool to get better results.
Critical Evaluation of Information (AI outputs included): Students consistently evaluate the credibility and quality of information, whether from an AI, internet, or any source. They cross-check facts, identify potential bias or missing perspectives, and are not fooled by superficial fluency. By high school, a student given an AI-generated text should be able to annotate it, pointing out at least, say, one potential error or unsupported claim
. Outcome: Students treat AI outputs as starting points, not end points – they verify and augment with their own reasoning or research
.
Ethical & Responsible Use: Students follow ethical guidelines when using AI or technology: they do not plagiarize (they use AI with attribution as required), they respect privacy (e.g., not entering personal data into random apps), they consider fairness (e.g., if using an AI that could reflect bias, they discuss it). They also understand social impact – e.g., can discuss how AI might affect jobs or privacy in society, and their role in that future. Outcome: Students demonstrate academic integrity with AI (e.g., by including a statement of how they used AI in an assignment) and can voice thoughtful opinions on AI ethics scenarios. This can be part of digital citizenship assessments or class discussions.
Adaptability & Self-Learning: Students develop a habit of self-directed learning. In the presence of evolving tools, they show cognitive flexibility – if a new tool is introduced, they explore it and figure out how to learn it (rather than being thrown off). They reflect on their own learning process, identify when they need help vs when they can push themselves. Outcome: Students partake in metacognitive activities – e.g., keeping a learning journal particularly noting when they used AI and how it helped or hindered, and what they did to adjust. They set goals to improve a skill and maybe use resources (possibly AI tutors) to practice – showing capacity to leverage new tools for personal growth.
Human Creativity, Collaboration & Personal Agency: Students invest in the uniquely human aspects of tasks. For creativity: rather than just take an AI-generated product, they iterate and add personal touches or novel ideas. For collaboration: they use AI as a mediator perhaps, but still engage deeply with peers in discussion and creation (not all communication through machines). For agency: students feel responsible for their work even when using AI – they maintain a sense of ownership and accountability. Outcome: We could see this in group projects where students might use AI for parts but ultimately present something with personal voice and reflection. Or in creative tasks, a rubric could evaluate the originality of ideas or personal expression even if AI assisted (ensuring the student isn’t just submitting an AI artifact). Students should also articulate the value of human effort – e.g., “AI gave me X, but I decided to do Y because it needed human insight.”
In practice, achieving these outcomes requires integration across subjects and grade levels. For example, a middle school language arts teacher might focus on critical evaluation by having students compare a Wikipedia article, a ChatGPT summary, and the original source text for accuracy (information literacy outcome). A high school science teacher might emphasize inquiry by assessing students on the quality of questions they pose in a research project (perhaps aided by AI to gather info). A social studies teacher could target ethics by having students role-play an AI ethics committee on a scenario, thereby demonstrating their understanding of ethical principles. The teacher’s role at each ladder stage ties to these outcomes: Baseline teachers introduce and reinforce them at basic levels; Proficient teachers design robust activities to practice and assess them; Advanced teachers might create cross-curricular or real-world experiences (like partnerships or competitions) to deepen them.
Teacher Training and Support: Key Components
To implement the above, the teacher professional development system should include:
Minimal Viable Practice Requirements: All teachers (even at baseline) should practice certain key tasks with AI during training. For instance:
During PD, every teacher should use an AI tool to create a lesson plan or materials, and then critique and improve it with trainer feedback. This ensures they personally experience the critical evaluation process.
All teachers should draft sample prompts related to their subject and see how AI responds, learning how question changes yield different answers (practical inquiry skill).
Every teacher should also engage in a scenario on AI ethics (like a case study: “A student turned in an essay that seems AI-written, what do you do?” or “You want to use a free AI app, what about data privacy?”). They discuss and get guidance, so they’re prepared for those real situations
.
These minimal practices ensure no teacher goes in cold. They also set a norm: AI is part of teaching practice now, just like knowing how to use email or a projector was.
Ongoing Learning Loops: PD shouldn’t be one-shot. Establish PLC (Professional Learning Communities) for AI integration, where teachers meet maybe monthly to share what they tried, successes, and challenges. Peer learning is powerful given the field is evolving – one teacher might discover a great way to use a new tool, others learn from it. Also, create an online community or repository (maybe district-level) where teachers upload AI-enhanced lesson plans, student activity ideas, etc., building collective knowledge.
Additionally, encourage teachers to engage in action research: e.g., a teacher tries a new strategy (like using AI chat for debate prep) and collects student feedback or results, then PD sessions incorporate these findings. This continuous cycle helps refine best practices and keeps teachers in a learning mode (modeling lifelong learning to students).
Peer Review and Coaching: Integrate AI-era competencies into existing teacher evaluation and coaching processes, but in a supportive way. For instance, instructional coaches or tech integration specialists (if available) can observe a class and specifically note how the teacher used or managed AI-related moments. They then give targeted feedback: maybe “Next time when the AI gave an error, consider pausing and discussing that explicitly with students – a missed teachable moment.” Or, “I noticed only a few students tried the AI extension exercise; maybe model it together first to boost participation.” Peer observation can also be useful: teachers visiting each other’s classes to see how they are handling AI can demystify it. Many teachers learned to use smartboards or Google Classroom via peer modeling; same with AI tools now.
Perhaps more formally, some schools might incorporate a goal in teacher growth plans like “Increase proficiency in integrating AI for differentiated instruction” – and then measure progress via coach observation or teacher’s reflective logs. The key is to make this developmental, not punitive; many teachers are understandably anxious about AI, so coaching must build confidence and skill, not fear.
Administrative and Policy Supports: School leaders should provide clear policies (guided by national guidelines if available) to create a safe environment for teachers to experiment. For example, a school policy might allow teachers to use certain approved AI tools and require them to follow privacy protocols, which actually empowers cautious teachers to try these tools knowing it’s sanctioned. Leaders should also protect teacher time for PD (maybe dedicate some in-service days to AI training, or provide class coverage if a teacher goes to an AI workshop).
On the assessment side, policies should adapt: for instance, moving away from take-home assessments that are just regurgitation, and toward more authentic tasks, to align with the presence of AI. That way teachers aren’t stuck policing AI misuse but can focus on teaching skills.
Safeguards Against Over-Reliance and Risks: Part of PD and leadership’s role is to highlight and monitor known failure modes:
If a teacher starts having AI write all their lesson content and stops engaging deeply with planning, that’s a problem (the teacher might not fully understand what they’re teaching or the nuances). Principals or coaches might notice a decline in quality or a mismatch in teacher familiarity. Safeguard: emphasise in PD that AI is a draft/assistant, not a final source, and perhaps require reflection: “What did you change in the AI-generated lesson plan and why?” to ensure active involvement.
“Performative adoption”: a teacher might use AI just to check a box (e.g., letting students use AI without any instructional scaffold, or doing a flashy AI demo without pedagogical value). To counter this, focus on pedagogy in PD – always tie use of AI to learning objectives. Possibly include in teacher eval that tech integration should be meaningful (some schools already do this via ISTE standards).
Equity issues: ensure teachers don’t unintentionally widen gaps. For instance, if certain students have more access at home, or if AI tools are mostly English-centric, teachers need strategies to support those at a disadvantage (like providing class time or alternatives, or using AI that supports other languages if applicable). PD should raise awareness that not all students come with equal tech savvy or access. Safeguard: track outcomes data by student groups to catch if AI-based teaching is benefiting some and not others, then adjust (this is similar to any tech integration – always watch the equity impact).
Misinformation: teachers relying on AI for content need to double-check. Perhaps a policy: if AI is used to generate factual materials, teacher must verify with a trusted source (common-sense but should be stated). Some schools might provide vetted AI content libraries to reduce risk. Ultimately, fostering a habit that teachers (and students) verify important info with multiple sources is the real safeguard
.
Ethical line: clarify boundaries. E.g., using AI to brainstorm is fine, but writing qualitative feedback to parents via AI might cross relational lines if not personalized. Or using AI to grade essays could be problematic due to bias and lack of transparency
. If a teacher does use an AI for grading suggestions, perhaps they must still review each grade (can't just auto-grade). These lines should be drawn in training and policy.
By implementing such training and support structures, schools can realistically move teachers up the competency ladder and thereby improve student competency outcomes. Finally, we should acknowledge potential risks and failure modes more explicitly (some covered above):
Outsourcing Thinking to AI (Teacher or Student): If teachers over-rely on AI (for planning, problem solutions, even answering student questions on the fly), they may inadvertently dull their own expertise and pass on shallow understanding. Worse, they might not catch AI errors and teach something incorrect. To avoid this, building strong content knowledge and always cross-checking is key (as per our earlier evidence, content knowledge still matters – you can’t effectively use AI in math class if you can’t do the math to verify the answer). For students, the risk is they stop trying to solve hard problems and just ask AI. Teachers should design tasks that require personal input (e.g., “What do you think and why, after consulting tools?”). Also explicitly teach study strategies that AI can’t replace (like memory techniques, because if students only rely on AI, they may not retain anything).
Performative or Misguided Implementation: Schools might push AI use without proper training (some might even force a tool into classrooms to appear innovative). This can lead to backlash or poor outcomes. The remedy is what we propose: focus on why (skills) not just what (tools), provide training, start with willing pilot teachers then expand. We saw some teachers panic-ban AI; the flip side is equally problematic: uncritical embrace. Both are extremes to avoid.
Inequity and “digital divide 2.0”: Students with savvy teachers will get a richer experience than those with teachers who ignore or misuse AI. This is an equity issue across schools (hence need for broad teacher training so all teachers hit baseline). Also within a class, if some kids have AI access at home and others don’t, homework that expects AI use could disadvantage some. Solutions: provide in-class time for AI-related tasks or loaned devices/Wi-Fi, or offer alternative paths to complete assignments. For teachers, monitor usage: ensure girls, minorities, special ed students are equally encouraged to use and create with AI, so we don't amplify existing gaps in confidence or opportunities (there is research that some student groups might self-select out of tech use if not supported).
Ethical lapses or controversies: One bad incident (like a teacher having an AI inadvertently produce inappropriate content for class, or a student claiming they were misgraded by an AI) can sour opinion. Schools should have clear protocols: e.g., filters on AI if used with younger kids; teachers previewing everything; and transparency with students/parents about how AI is used. That way trust is maintained. Another area is academic integrity – if lots of students start cheating with AI undetected, it undermines trust and learning. Teachers need training in new forms of assessment and perhaps use AI-detection carefully (with understanding of limitations to avoid false accusations
). It’s actually better to design cheating out by changing task design than rely on detection arms race.
In sum, our practical model calls for embedding AI competencies into the fabric of teaching and learning, supported by clear standards, robust training, and a culture of continuous learning and ethical practice. When done right, teachers will not feel replaced or devalued by AI, but rather empowered – they have new tools to enhance personalized learning, more time for the human aspects of teaching, and a crucial role in coaching students to be wise users of AI. Students, in turn, benefit by developing the inquiry, critical thinking, and ethical mindset that will serve them in any future, with AI as ubiquitous as it promises to be.
Implications and Recommendations
Achieving this vision requires coordinated action at multiple levels of the education ecosystem. Below we provide targeted recommendations for key stakeholders – Ministries of Education and Policymakers, Universities (especially teacher education faculties), School Networks/Districts and Administrators, and Teacher Professional Development Institutions – as well as considerations for curriculum, assessment, and equity.
For Ministries of Education and National Policymakers:
Develop and Adopt National AI-Education Frameworks: Ministries should create or endorse competency frameworks for both teachers and students regarding AI (as UNESCO, OECD provide templates for
). This provides a clear vision and standards. For example, a Ministry might adapt UNESCO’s framework into national teacher standards – requiring teachers to demonstrate basic AI literacy and integration skills as part of certification or advancement. Similarly, student curriculum standards should explicitly include AI-related critical thinking and digital literacy goals. Codifying these signals their importance and drives resources toward them.
Invest in Large-Scale Teacher Training Programs: Provide funded, structured PD opportunities to all current teachers to reach baseline competency. This could be through summer institutes, online courses, or integrating into ongoing training. Ideally, combine top-down and bottom-up: e.g., a national online course all teachers must take (with basics of AI, examples, ethics), plus grants or support for schools to hold their own workshops contextualized to local needs. As seen in China and UAE, quick mobilization is possible with government support
. Also update initial teacher education (pre-service) requirements to include AI literacy coursework – making it a staple like child psychology or assessment courses.
Update Curriculum and Assessment Policies: Infuse AI-era competencies across subjects. This doesn’t always mean new subjects, but weaving into existing ones – e.g., include a media literacy component on AI-deepfakes in social studies, data science elements in math. Ensure high-stakes exams or evaluations value skills like critical thinking and problem-solving (because if exams remain rote, neither teachers nor students will prioritize higher-order skills or use of AI for analysis). Consider pilot assessments for AI literacy (e.g., a project-based assessment or a digital test where students must evaluate AI outputs) perhaps as a low-stakes national test to gather data.
Provide Necessary Infrastructure and Resources: Equip schools with reliable internet, devices, and access to safe AI tools. If AI is to be integrated, tech infrastructure must support it in all schools, not just affluent ones. Ministries should negotiate with vendors for education-friendly AI platforms (ensuring data privacy compliance and possibly training AI on local curriculum needs). For example, have a national educational AI repository or platform teachers can trust. This addresses equity: all teachers/students should have baseline tools available. Also, provide content resources: curated examples of AI use in each subject, lesson plan banks, etc., possibly via a national education portal.
Guidance on Policy and Ethical Use: Issue clear guidelines on how to use AI ethically in education – similar to UK’s guidance
. Include policies on academic integrity (e.g., encourage honor codes updated to mention AI-assisted work disclosure), data protection (what data can/can’t be shared with AI services), and safeguarding (ensuring age-appropriate AI usage, content filtering, etc.). Ministries might also consider regulatory measures: for instance, ensure AI tools used in schools meet certain transparency criteria (maybe requiring vendors to disclose if their content is AI-generated in digital textbooks, etc.).
National Awards or Recognition Programs: Encourage innovation by recognizing teachers/schools who effectively implement AI to improve learning. This can motivate uptake and sharing of best practices. E.g., a “AI in Education Innovator Award” or inclusion of an AI integration category in existing teacher awards. Publicizing success stories (like a rural teacher who used AI translation to help non-native speakers) can also garner support and reduce skepticism.
Cross-sector Collaboration and Research: Ministries should collaborate with higher education and research institutes to study the impact of AI in classrooms (what works, what pitfalls). Possibly fund action-research at pilot schools and use findings to inform scaling. Also engage industry but carefully – e.g., consult tech companies for expertise or tools, but guard against commercial bias and ensure pedagogy leads tech, not vice versa (maybe through public-private partnerships that are education-driven, like Microsoft’s work with UNESCO ensuring alignment with teacher needs
).
Equity Considerations at Policy Level: Explicitly include bridging the AI literacy gap as part of digital divide programs. That means channeling extra support to disadvantaged schools for training and infrastructure. Monitor usage data by region/income and intervene if some areas lag. Possibly incorporate AI literacy in adult/community education as well, so parents and communities understand the shift (this can help support students and reduce fears).
For Universities and Teacher Training Institutions:
Integrate AI-Era Competencies into Teacher Education Curricula: Schools of education should revise curricula to incorporate digital and AI literacy, not as a side note but woven into teaching methods courses. For example, a math methods course should cover how to teach problem-solving when tools like Wolfram Alpha exist; an English methods course should include strategies for teaching writing in the context of AI writing assistants. A standalone course on “Teaching in the AI Age” could be offered, covering theoretical and practical aspects (perhaps interdisciplinary, touching on ethics, learning science with AI, etc.). Pre-service teachers should graduate having used AI in micro-teaching or student teaching and reflected on it. Essentially, ensure new teachers enter the workforce at least baseline, ideally approaching proficient level on our ladder, rather than playing catch-up later.
Faculty Development for Teacher Educators: Many education professors/trainers themselves might be behind on AI. Universities should organize PD for their own faculty so that those teaching teachers lead by example. Perhaps workshops within colleges of ed to share how to incorporate AI in assignments (e.g., have student teachers create a lesson with AI and critique it, as practice). Education researchers should also take the lead in researching AI in pedagogy, which requires them to be hands-on with these tools.
Research and Evidence Generation: As part of universities’ mission, conduct and publish research on the effectiveness of different approaches to using AI in education. For instance, compare classes where teachers used AI versus not, in terms of critical thinking outcomes (some studies are starting
, but more context-specific research helps refine practice). Also explore how to best assess those human OS skills. This evidence can feed back into policy and PD content. Encourage graduate students to take up dissertations on AI in education topics – building a knowledge base. Teacher training programs can collaborate with schools for pilots, benefiting both (teachers get support, researchers get data).
Mentorship and Continuous Support of Alumni: When newly trained teachers go into schools, universities (maybe through induction programs or alumni networks) can continue to support them in implementing what they learned about AI. For example, an induction program might have monthly check-ins or online community where first/second-year teachers discuss how they are handling AI in class and get advice from their former professors and peers. This can reduce the theory-practice gap and help scale good practices.
Partnerships with Tech and Other Faculties: Encourage interdisciplinary collaboration – e.g., education faculty working with computer science or data science faculty to ensure teacher ed content on AI is accurate and current. Maybe co-teach some segments (like a CS professor guest lectures on how AI algorithms work to give future teachers a deeper understanding behind the tools). Also collaborate with ethics/philosophy departments on the ethical training piece.
Update Professional Standards and Accreditation: Bodies that accredit teacher education (in the U.S., CAEP for example) should include indicators related to preparing teacher candidates for AI-rich environments. Similarly, licensure exams or performance assessments (like edTPA) might incorporate scenarios involving educational technology use – and going forward, specifically AI. Universities should anticipate these changes and align their programs accordingly.
For School Networks, Districts, and School Leaders:
Create a Vision and Culture for AI-Ready Schools: District and school leaders need to articulate a positive, proactive vision for how AI will be used to enhance teaching and learning, tied to the mission of the school. For instance, a district might include in its strategic plan that “we will leverage AI to personalize learning and develop critical thinking, while maintaining the primacy of teacher and student agency.” Principals should communicate to staff that exploring AI is encouraged (within guidelines), and that mistakes will be treated as learning opportunities. Cultivate a growth mindset in staff regarding AI – similar to how schools cultivated positive attitudes toward inclusion or new curricula in the past.
Professional Development Structures: At the network/district level, organize sustained PD (not just one-off). Possibly set up an “AI in Education” task force or committee to guide PD and share resources. Dedicate some of the existing PD hours (e.g., one per quarter) specifically to technology integration with emphasis on AI. Use the PLC model – maybe have each school have an AI champion who meets with other school champions at district level to get ideas and bring back to their school (train-the-trainer model). School leaders should also participate in some training to understand what teachers are learning and show buy-in.
Coaching and Mentoring: Invest in or repurpose positions for tech integration specialists or instructional coaches to include AI. If a district can afford an educational technology coach, ensure they become knowledgeable about AI tools and pedagogy, so they can directly assist teachers in classrooms. Alternatively, free up time for lead teachers to coach (e.g., a master teacher in each school given a lighter teaching load to act as a mentor on innovative practices including AI use).
Curriculum Guidance and Resource Curation: District curriculum departments should embed AI opportunities in the curriculum maps. For example, when providing unit plans or pacing guides, include suggestions like “Optional: Have students use an AI chatbot to gather initial ideas, then evaluate them using XYZ criteria.” Or incorporate projects that inherently require using data/AI. Essentially, make it easier for teachers by giving them vetted resources and ideas aligned to standards. Also curate a list of approved digital tools (with privacy vetting) for teachers to choose from, so they are not lost in the sea of apps. Some districts have started publishing “generative AI guidance for teachers” – e.g., New York City DOE reversed an AI ban and gave teachers suggestions on usage. Follow these examples.
Assessment Strategy Reforms at Local Level: Start adjusting school assessments to align with human OS competencies and account for AI presence. For instance, encourage more in-class writing or oral assessments for English classes (to ensure students can write without AI and also to allow discussion of how AI might have been used in drafting). Promote project-based learning and group work, where process and collaboration are assessed, not just final answers. If the district uses standardized test prep, maybe incorporate AI in practice but ensure students can perform without it as well (balance needed). Districts might pilot alternatives like capstone projects or portfolios that capture critical thinking and creativity better than multiple-choice tests – reducing the incentive to cheat with AI and giving a more holistic view of student skills.
Policy Enforcement and Support: At school level, enforce academic integrity in a way that educates rather than just punishes. For instance, if a student submits AI-generated work, turn it into a learning moment: have them redo with reflection, etc., rather than simply penalize. But do have clear consequences for misuse spelled out in codes of conduct to deter overt cheating. For teachers, ensure no one is banning AI out of hand due to fear (unless temporarily sorting things out) – principals should talk through concerns and share guidance, maybe pair up hesitant teachers with early adopters to team-teach a lesson using AI so the fearful teacher can see it in action. Conversely, monitor that no teacher is misusing AI (like making AI write all feedback to students with no personalization – if students report generic, sometimes off feedback, that could be a sign). Principals should include some discussion of tech integration in teacher eval conferences – asking “How are you leveraging new tools like AI to enhance learning? What support do you need to do this better?”
Family and Community Engagement: Districts/schools should inform and involve parents and the community about these changes. Host informational sessions (“AI in the Classroom: What Parents Should Know”) to explain how teachers will be using AI and how students will learn to use it responsibly. This can build support and quell misunderstandings (some parents might think “AI in school” means kids just get answers from computers all day – we need to explain the real approach and benefits). Also provide tips for parents on how to reinforce these competencies at home (like encouraging critical questioning or discussing AI in daily life). Community partnerships could help – maybe local tech companies or universities come in for guest talks or support hackathons, linking learning to real-world context.
Equity Measures: At the school network level, ensure resources (training, tech) are distributed equitably. Keep an eye on schools in less affluent areas and provide additional support (maybe priority in device refreshes, extra coach visits, etc.). Consider language inclusion – if teaching in multilingual context, ensure AI tools used are accessible in relevant languages or have translation features so ELL students aren’t left behind. Special education departments should be involved to find how AI can assist SEN students (some AIs can be great reading aids or generate individualized practice). Essentially, include AI in your equity plan: how will this tech be used to close gaps, not widen them?
For Teacher Professional Development Providers (institutes, NGOs, Ed-Tech orgs):
Design PD that Integrates Content, Pedagogy, and AI: Workshops and courses should not teach AI in a vacuum (“here’s how ChatGPT works”) but contextualize in pedagogy (“here’s how to use ChatGPT to foster inquiry in your history class”). Providers should tailor programs to different subjects and grade bands, showing relevant use cases. Emphasize active learning in PD: teachers doing and reflecting, not just listening. Utilize blended PD – online modules for foundational knowledge plus live sessions for discussion and practice.
Micro-credentials and Certification: Consider offering micro-credentials in “AI in Education” that teachers can earn to demonstrate their competence. For example, a badge for “Ethical AI Classroom Practice” if they complete a module and submit evidence (like a lesson plan demonstrating ethical use or student work). Another for “AI-Enhanced Lesson Design,” etc. Make sure these credentials require evidence of classroom application, not just quiz completion, to ensure translation to practice. These can motivate teachers (some districts might give PD credit or even salary lane changes for such credentials). They also create a cohort of recognized experts which could become trainers for others.
Support Communities of Practice: Many NGOs or networks (ISTE, etc.) can host online communities or regular meet-ups (webinars, Twitter chats, etc.) focused on AI in teaching. Facilitating teacher-to-teacher sharing beyond district boundaries can spread innovation and provide moral support – important in the early days when many feel out of their comfort zone. For instance, an online forum where a teacher from India and a teacher from Canada both trying similar AI tools can compare notes, moderated by PD experts. This global sharing accelerates learning.
Continuously Update PD Content: The AI field evolves fast (e.g., new tools, new capabilities, policy changes like what’s allowed in standardized tests). PD providers must stay agile – update examples and recommendations maybe every few months. What we say about ChatGPT now might change if future models come with better citation or if new pitfalls emerge. Thus, maintain a feedback loop: get input from practicing teachers on what’s happening in their classrooms and incorporate that into training. Use iterative design – treat PD like ongoing R&D.
Work with School Leadership: Teacher PD is more effective if principals and admins also understand it and reinforce it. PD providers might offer parallel sessions or modules for school leaders on how to support teachers in AI integration (covering things like monitoring, policy, addressing parent concerns, etc.). This ensures alignment – teachers won’t go back to schools excited to try something only to be halted by an uninformed administrator.
Policy Advocacy: PD organizations often have the ear of policymakers or can influence through coalitions. They should advocate for supportive policies (like those recommended to ministries above). They can use evidence from their work to argue for funding, for changes in teaching standards, etc. For example, if an NGO runs a successful pilot showing improved critical thinking in AI-integrated classrooms, they can present that to ministry or district officials to get buy-in for larger rollouts.
Equity Focus in PD: Train teachers to be aware of and address equity in AI use (the point earlier about certain students not taking advantage of AI due to confidence or stereotypes – PD can cover that, teaching strategies to draw in all students). Also focus on bringing PD to under-resourced areas (perhaps subsidized by government or donors). Sometimes the schools that need this most (to catch up and not be left behind) are least able to pay or access PD. Providers should seek grants or partnerships to reach those communities, to prevent a scenario where only elite schools implement high-quality AI integration and others stagnate.
Policy and Curriculum Guidance Summary:
In a nutshell, the aim is to shift the whole system to embrace AI as an opportunity to deepen learning of human skills. Policies at top should mandate or encourage change, but also supply the means (training, resources). Curricula should explicitly embed those "OS" competencies and leverage AI where it enhances learning objectives. Teacher training and PD are the linchpins turning policy into practice, and require modernization and support. Assessment strategies at all levels need revision to align incentives – if we keep assessing only rote knowledge, teachers will be hesitant to innovate with AI (since AI can supply rote answers easily). Instead, incorporate assessment of process, creativity, and critical thinking. For example, some universities now allow AI use in assignments but require an appendix explaining how it was used and reflecting on its limitations, and grade that reflection. K-12 can do similar on appropriate scale. Equity considerations must remain front and center: ensure all teachers get baseline support so all students have a chance to learn these crucial competencies. We must avoid creating "AI literacy haves and have-nots" which could exacerbate achievement gaps and later, economic gaps. The recommendations above, if executed, would move education systems toward one where teachers are confident, continuously learning facilitators of deep learning, and students become adaptive, critical, and ethical users of AI – in other words, education would fulfill its role of keeping the human in charge of the technology, not the other way around.
Limitations and Open Questions
It’s important to acknowledge that our analysis, while comprehensive, is subject to several limitations and areas where evidence is still incomplete or evolving:
Nascent Evidence Base on AI in Education: Given that generative AI tools (like GPT-4) are very recent (2019-2023), much of the research is emerging. Many claims about impacts on student reasoning or teacher practices are based on short-term studies, surveys, or even anecdotal reports
. Longitudinal studies are largely absent. We don’t yet have firm evidence on long-term learning outcomes (e.g., does using AI extensively in high school help or hurt students’ college performance or critical thinking a few years down the line?). As such, some recommendations are extrapolations of general principles (e.g., from research on technology integration or inquiry learning) combined with early AI-specific data. There is a risk that as more data comes in, some of our assumptions might be challenged. For instance, the meta-analysis we cited
 shows generally positive effects, but it included many small-scale studies; if many teachers adopt AI without sufficient scaffolding, will the average effect remain positive or could it dip? There's publication bias to consider – initial studies often come from enthusiastic adopters or controlled contexts. Broad adoption could yield different results. This is an open question: Under what conditions exactly does AI use improve higher-order thinking, and when might it hinder it? Ongoing and future studies will need to clarify this.
Variability in Context: Education systems vary hugely worldwide. What works in one context may not translate neatly to another. For example, we drew case studies from places like Finland and Singapore which have well-funded systems and highly trained teachers, and from China which has a centralized system that can mandate changes quickly. In contexts with less infrastructure or less centralized control, implementation could face more challenges. Also, teacher roles differ – in some countries, teacher autonomy is high, in others a strict national curriculum dictates practice. Our recommendations presume a level of teacher agency to integrate new methods; in very rigid systems, teachers might not be allowed to deviate much from set content to, say, have an AI ethics discussion. There’s a tension: how to cultivate those competencies in students if the exam system still rewards rote learning (e.g., some developing countries have high-stakes exams that drive all teaching). Without systemic exam reform (which is hard and slow), asking teachers to prioritize competencies might not realistically happen. So a limitation is that system inertia can blunt these changes, and we don't have many examples yet of exam-driven systems successfully pivoting to focus on AI-related competencies. This remains an open area: How to align AI-era competencies with traditional metrics of success? Some countries (e.g., the UK adding critical thinking to some assessments, or PISA planning to assess creative thinking and maybe AI literacy) are starting, but it's incomplete.
Potential Conflicts in Evidence (Teacher Knowledge vs AI Reliance): We highlighted a nuanced debate on teacher content mastery vs pedagogy
. AI complicates that: if AI can provide subject matter knowledge on tap, does that reduce the need for teacher’s own deep content knowledge? Some might argue yes (teacher could always quickly fact-check or generate examples via AI), others say no (teacher must discern correct from incorrect AI info, which requires strong knowledge). The literature still strongly points to PCK (Pedagogical Content Knowledge) being crucial. But we lack direct research on how AI might augment or undermine teacher knowledge needs. It’s an open question: Will easy access to AI make teachers less inclined to deeply learn content, and what effect would that have? If teacher knowledge weakens, that could harm their ability to spot AI errors or provide context. No research yet answers this because it's a future scenario. We caution that our recommendation for teacher baseline competence implicitly assumes teachers keep solid content knowledge plus AI literacy – but ensuring that (not letting content mastery atrophy) might be an implementation challenge to watch.
Student Behavior and Learning Patterns: We identified trust erosion issues
 and potential motivation loss due to AI ease
. However, we are extrapolating from initial data and psychological theory. Students are a diverse group – some might over-rely on AI, others might hardly use it even when allowed (out of fear or preference for human interaction). It's not one-size. For instance, a small ethnographic study might find that certain students engage in very thoughtful use of AI (like using it to brainstorm multiple perspectives on an issue) thereby enhancing learning, while others just copy-paste answers and disengage. The net effect in a given classroom will depend on culture set by teacher, student personalities, etc. This variability means interventions must be flexible. And research needs to catch up: What kinds of students (or under what classroom conditions) benefit most vs. get harmed by AI assistance? We don't fully know. It's open: maybe high-performing self-regulated learners benefit a lot (getting more enrichment from AI), whereas struggling learners might either benefit (get more support) or might offload and fall further behind in fundamental skills – studies conflict on this, some show AI tutoring helps low performers, others worry it allows them to avoid doing the cognitive work. More targeted research is needed.
Ethical and Logistical Uncertainties: We assume AI tools will be available and permitted, but this landscape is evolving legally and commercially. What if stricter data laws or copyright laws change what AI can do or what schools can legally use? Or if the AI industry moves to paid models that schools can’t afford widely? Our strategy assumes a relatively easy access scenario. Also ethically, using AI means data generation; privacy issues are unresolved. There's an open question: How to protect student data and rights in widespread classroom AI use? If policies swing towards heavy regulation, teachers might have to navigate complicated compliance, which could slow adoption. So a limitation is that the policy and tech environment could significantly alter how feasible and smooth these recommendations are.
Teachers’ Professional Identity and Workload: Encouraging teachers to adopt AI and continually learn sounds ideal, but we must acknowledge teacher overload and potential resistance. Teachers have been through many waves of reform; initial responses to AI in some places included outright bans or distrust. Changing mindsets takes time and evidence of positive outcomes. We should question: Will teachers realistically embrace these changes given current workloads and sometimes lack of support? If PD is not well-resourced or if teachers feel this is another fad or threat to their professionalism, uptake could be minimal or surface-level. The literature on ed-tech is full of cases where tools were introduced but not used meaningfully due to lack of training or buy-in. So our plan’s success hinges on careful change management (which we recommended, but execution is key). This is a limitation because human factors and school culture are hard to quantify or guarantee.
Lack of Consensus on Measurement of “Human OS” Skills: We talk about critical thinking, metacognition, etc., as goals, but measuring progress in these is complex. Standardized tests for critical thinking are emerging but not widespread or high-stakes enough to drive change. Without clear metrics, it can be hard to evaluate if our interventions are working (and policymakers often want measurable outcomes). It’s an open area: How to robustly assess improvements in competencies like ethical reasoning or adaptability in students? Some work via PISA Creative Thinking or national assessments of problem-solving exists, but linking those to classroom practice changes will need research. In the interim, we rely on proxies or qualitative assessments.
The Risk of Overpromising on AI: There is a tendency in ed-tech to hype new tools as revolutionary. AI is exceptionally hyped right now, which can lead to a pendulum swing – initial overuse or enthusiasm, then disillusionment when it doesn’t magically solve all issues. A limitation of our report is we assume good implementation; if implemented poorly, AI could indeed become a distraction or even regress learning (e.g., if students just use it to avoid writing or reading deeply, they might lose skills). So one must be cautious not to overpromise. Perhaps in 5 years we'll see more clearly certain tasks that AI does improve vs tasks it doesn’t. We should remain open to the possibility some aspects of learning might remain better done without AI interference for mastery (like initial learning of some foundational knowledge might need memorization or human interaction, and introducing AI too early could confuse or short-circuit cognitive development). These fine-grained questions are not fully answered.
Equity Outcomes Uncertain: While we stress equity, it’s an open question whether AI will narrow or widen gaps. Some argue AI democratizes tutoring (everyone can have a quasi-tutor), others worry affluent students will use AI more effectively (having more support or better tools) and widen the achievement gap
. Early evidence is scant. We propose steps to mitigate, but it's a critical area to monitor. Perhaps new divides (like algorithmic literacy divide) will appear. We may need mid-course corrections in strategy once we see evidence. Right now, not many studies specifically track differential impacts on different student demographics (a research gap).
Misuse and Ethical Dilemmas: We might encounter unforeseen ethical dilemmas. For example, if AI can generate deepfakes or extremely persuasive texts, how do schools handle incidents of AI being used for bullying or misinformation among students? That wasn't fully covered in our plan beyond promoting digital citizenship – but there could be new kinds of issues (like a student making an AI-generated embarrassing video of a teacher – what then?). The policies and training will have to evolve as new misuse cases arise. It’s an open question: What new forms of academic dishonesty or interpersonal harm will generative AI enable, and how can schools pre-empt them? (E.g., AI plagiarism is one, but maybe AI could write code to hack systems, or AI voice clones to call in fake messages, etc.). Not all these are academic issues but could become school management issues. This extends beyond our core academic focus but is part of “AI-saturated environment” challenges.
Generalizability of Case Studies: Our case studies were chosen for documentation quality and geographic diversity, but they may not represent average conditions. For instance, not every country can replicate Finland’s teacher training turnout (400 signing up where 120 were expected – Finland has a uniquely high regard for teacher PD)
. In some systems, motivating teachers to engage might require more incentives or mandate. So while we glean lessons, each locale must adapt strategies to local culture, teacher work conditions, etc. There's an implicit limitation that our recommendations might need significant tailoring; one size doesn’t fit all. This is both a limitation and a prompting of further question: How can less-resourced or lower-capacity systems implement these changes effectively? Often, reforms succeed in high-capacity contexts and struggle elsewhere.
Student Voice and Psychological Aspects: We haven't deeply addressed how students feel about all this beyond trust issues. Some open questions: Will heavy use of AI affect students’ sense of agency or creativity long-term? Psych research is needed. For example, if students become accustomed to always collaborating with an AI, do they develop a dependency or does it boost confidence? Mixed possibilities. Or how do these competencies interplay – does focusing on AI-critical thinking foster general critical thinking, or do students compartmentalize it (“I only critically evaluate when it's AI, but not other times”)? We assume transfer, but it must be cultivated. There's limited research on how students perceive learning from AI vs teacher vs peer and how that affects motivation and identity as learners.
In summary, while we have mapped a strategy based on current knowledge and expert consensus, we remain in early days of this grand experiment. We should treat our approach as adaptive: implement, rigorously evaluate, and be ready to iterate. Key unknowns like the long-term impact on critical thinking, the best ways to measure competency gains, and how to systematically ensure equity will need ongoing inquiry by the academic community and feedback from practice. Honesty about evidence: We acknowledge where evidence is thin – e.g., on long-term student outcomes with AI, on broad implementation results – and stress that recommendations are based on extrapolation of sound principles combined with what initial research we do have. As results come in (which they will in the next few years, given many are studying these issues), we might refine approaches. There may be conflicting results – e.g., some studies might report improved writing due to AI feedback, others might report decline in writing skill due to overreliance. Educators and policymakers should be ready to dive into the details: maybe certain pedagogies with AI yield positive results and others negative, so the key is identifying those moderating factors. Further study needed: We definitely need more meta-analyses, especially across different age groups and subjects, to understand AI’s pedagogical effects. Also needed are studies on teacher professional development specifically for AI (e.g., does training X yield more teacher confidence or skill than training Y?), since we largely inferred from general PD knowledge. The table of evidence we gave could be extended in coming years with stronger, more direct studies. In essence, our plan is a well-informed proposition, but it should be implemented with a spirit of inquiry: treat classrooms as learning labs, collect data on what’s working, involve teachers in action research, and adjust courses as necessary. The exciting part is we have an opportunity to reshape education positively – but it must be done thoughtfully, guided by evidence and values, and always with a willingness to learn from mistakes along the way.
References and Evidence Appendix
(Below we list the most important sources referenced, with full citations, and provide a brief annotation for each highlighting its relevance, support for our analysis, and any limitations or caveats. Citations are given in APA style with direct links where available.)
Filo, Y., Rabin, E., & Mor, Y. (2024). An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers. European Journal of Open, Distance and E-Learning, 26(S1), 93–106. DOI: 10.2478/eurodl-2024-0012
 – Why it matters: This peer-reviewed study presents a validated AI competency framework developed with teachers in Israel. It identifies key teacher/student AI skills (AI knowledge, effective use, agency, ethics) and underscores the need for teacher involvement in design. It supports our advocacy for clear competency standards and teacher partnership
. What it supports: The idea that teachers need specific AI-related competencies and that a framework can guide PD and curriculum
. It also provides evidence that teachers can successfully be engaged as co-creators of such frameworks, leading to high buy-in. Limitations: It's a single-country pilot and early in implementation; it doesn’t provide long-term student outcome data yet, and its generalizability beyond its context is untested (though it aligns with UNESCO/OECD concepts).
UNESCO. (2024). AI Competency Framework for Teachers. UNESCO Publishing.
 – Why it matters: This is UNESCO’s official framework guiding what knowledge, skills, and values teachers need in the AI era (15 competencies across 5 areas)
. It's a key policy document reflecting international consensus. What it supports: Our report’s emphasis that teachers must master AI ethics, human-centric mindset, AI pedagogy, etc., comes directly from here
. It bolsters recommendations for national adoption of similar frameworks and confirms the stance that teacher competencies are crucial for AI integration. What it doesn’t prove: It’s a framework (policy) – it doesn’t provide evidence of impact. It assumes these competencies matter but that assumption is based on expert consultation, not an outcomes study. We cite it as an authoritative guideline, not an empirical finding.
Wang, J., & Fan, W. (2025). The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis. Humanities and Social Sciences Communications, 12(621). DOI: 10.1057/s41599-025-04787-y
 – Why it matters: This is a meta-analysis of 51 studies (Nov 2022–Feb 2025) on ChatGPT in education, published in a Nature journal. It provides quantitative evidence of AI’s impact: large positive effect on performance, moderate on higher-order thinking
. What it supports: Our assertion that AI can enhance learning when integrated properly
. It underpins recommendations to embrace AI with frameworks (since evidence shows potential benefits). Caveats: It covers short-term studies mostly; heterogeneity is likely (varied contexts in those 51 studies). It’s quite early to meta-analyze, so results might shift as more studies come out. Also, as we note, the positive average might hide negative outcomes in poorly scaffolded uses
. We used it to justify potential upsides but with caution that context matters.
Brookings Institution – Burns, M., Winthrop, R., et al. (2026). A new direction for students in an AI world: Prosper, prepare, protect. (Report by Brookings Global Task Force on AI and Education)
 – Why it matters: This comprehensive report synthesizes global insights (50 countries, many interviews) on AI’s benefits/risks for children’s learning. It provides high-level findings on trust erosion
, cognitive offloading risks
, and need for pedagogical integration
. What it supports: Many points: the centrality of human teacher role (it argues AI can free teachers for human tasks)
, the risk of shallow learning if AI use isn’t carefully managed (cognitive offloading)
, and the phenomenon of mutual distrust emerging (teachers doubting work authenticity, students questioning teachers)
. These align with our identification of issues and recommendations for emphasizing human elements and trust-building. Limitations: It’s a thought leadership piece, not primary research. It’s based on expert consensus and literature (some cited in it). It doesn’t “prove” outcomes but provides a global pulse and well-considered recommendations which we echoed (like focusing on “prepare, prosper, protect” where prepare = education on AI literacy)
. It doesn’t, for instance, show data linking an intervention to improved trust, it just flags the issues and urges action.
Ahmad, S. et al. (2025). Epistemic authority and generative AI in learning spaces. Frontiers in Education, 10, 118. (As referenced via search: it’s implied in our trust discussions)
 – Why it matters: This study (in Frontiers) investigates how GenAI’s rise affects perceptions of knowledge authority in higher ed. It documents that many see an 'erosion of trust' between students and teachers, largely due to AI (both cheating concerns and students preferring AI answers)
. What it supports: It directly corroborates our discussion of trust issues and need for rebuilding epistemic authority by involving AI in teaching rather than ignoring it. It’s part of the evidence that informed our analysis of student-teacher trust and authority shifts
. Limitations: We referenced it as reported via search results; we didn’t have direct quotes beyond the 'erosion of trust' line. It’s likely a qualitative study or opinion piece (Frontiers allows various article types). Without full context, we used it carefully (to illustrate that this observed trend is noted by researchers). It doesn’t provide a solution, just an observation.
Hattie, J. (2009). Visible Learning: A Synthesis of Over 800 Meta-Analyses relating to Achievement. Routledge. (and related commentary by Kirschner & Hendrick (2022) via blog excerpts
) – Why it matters: Hattie’s work is a famous meta-synthesis on what affects student achievement. We used specific data on effect of teacher subject matter knowledge (very low ~0.09)
 and contrasting viewpoints (OECD and Kirschner emphasizing knowledge importance)
. What it supports: It provides evidence for the debate on content vs pedagogy – allowing us to make a nuanced point that both matter but content alone isn’t enough
. It justifies focusing not only on knowledge but how teachers use it (PCK) and thus aligning with our stance that teacher AI skills must be combined with pedagogy. Limitations: Hattie’s analysis has been critiqued (we even cited some critique about how he calculated effects)
. So we treat the exact number cautiously. Also, Hattie’s data predates AI, but we extrapolate the principle that content knowledge without pedagogical skill yields limited gains (and vice versa). The blog we cited (Visible Learning blog) is a secondary source interpreting Hattie and others, so we gleaned perspective rather than treat it as primary evidence.
Galindo-Domínguez, H., et al. (2025). Relationship between the use of ChatGPT for academic purposes and plagiarism: the influence of student-related variables on cheating behavior. Interactive Learning Environments. DOI: 10.1080/10494820.2025.2457351
 – Why it matters: This is an empirical study (507 undergrads) investigating whether using ChatGPT correlates with plagiarism. It found correlation but no direct causation, highlighting motivation and cheating culture as bigger factors
. What it supports: It underpins our argument that AI doesn’t automatically cause cheating – context and ethics education matter
. It provides evidence to move away from outright bans and towards integrity promotion
. What it doesn’t prove: It’s a single study, self-reported usage and detected plagiarism likely, and specific to one university context. But it’s fairly rigorous (published in a journal, with stats) and supports moderate evidence that guided our stance on focusing on integrity culture rather than prohibition.
Kasneci, E., et al. (2023). Education in the era of ChatGPT – mapping the challenges and opportunities of generative AI for teaching and learning. Computers and Education, 191, 104610. (We indirectly referenced via meta-analysis introduction
) – Why it matters: This widely cited article discusses how generative AI like ChatGPT might impact education, raising both positive and negative possibilities. It’s a conceptual piece mapping issues (like critical thinking challenges, assessment changes, etc.). What it supports: The general framing of opportunities and concerns (e.g., they mention improved tutoring vs risk of surface learning), which aligns with our balanced approach. We didn’t cite it directly, but it informed community understanding that we drew upon especially in academic literature synthesis and open questions. Limitations: It’s not empirical research but expert analysis. It doesn’t provide data, but lays out arguments. We treated it as part of the scholarly discourse context.
American Federation of Teachers – Collins, A., Brown, J.S., & Holum, A. (1991). Cognitive apprenticeship: making thinking visible. American Educator, 6(11).
 – Why it matters: This classic piece introduces cognitive apprenticeship model. We used it to support the point that modeling expert thinking helps students learn cognitive skills
. What it supports: The idea that teachers demonstrating their thought process (like evaluating an AI output out loud) is beneficial. It’s foundational theory behind our recommendation for teacher modeling. Limitations: It’s older (1991) and general – not specific to AI. But its principles have stood test of time, backed by numerous later studies. We used it conceptually to reinforce that apprenticeship methods apply here too (with AI being the “task” environment where thinking needs to be visible).
Lundin, M., Oates, G., & Tan, E. (2023). Exploring the impact of ChatGPT on critical thinking in higher education. Education Sciences, 13(9), 929. (or similar, possibly referring to Olivier & Weilbach, 2024 in MDPI we cited
) – Why it matters: We referenced an MDPI study
 that looked at how guided vs passive use of ChatGPT affects critical thinking (using the Community of Inquiry cognitive presence model). It found that guided use (human-in-loop) led to full cognitive presence cycle (triggering, integration, resolution) whereas unguided might not. What it supports: Our argument that quality of human-AI interaction determines if critical thinking is fostered
. This justifies recommendations for teacher scaffolding. Caveats: It’s a single study, context likely higher ed small sample. MDPI journals vary in rigor but this one seemed thorough theoretically. We treat it as moderate evidence that aligns with educational theory – not definitive but illustrative.
Gulf News – Rasheed, A. (2025, Oct 26). UAE launches 'Afaq' program to empower teachers with AI skills.
 – Why it matters: A media article describing the UAE’s teacher training initiative Afaq and the new AI curriculum rollout. It provides concrete numbers (500 teachers, etc.) and quotes emphasising AI as support not replacement
. What it supports: Our case study of UAE and points about large-scale PD and the stance that teachers remain central
. It lends real-world evidence that policy and training are being done together. Limitations: As a news piece, it’s promotional and not evaluative. We used it for factual info (scale, content of program) and quotes from officials to illustrate stance
. We consider it a credible report (Gulf News is a mainstream media) but not research; it captures the narrative and official viewpoint.
Code School Finland – Tahvanainen, S. (2025, Aug 5). What we’ve learned from teaching AI in schools around the world. (Blog post)
 – Why it matters: This practitioner blog highlights experiences in AI education globally, including Finland’s teacher training response (400 signed up vs 120 expected)
 and lessons like project-based approach. It provides on-the-ground insight reinforcing that teachers are eager when supported. What it supports: It gave evidence that teachers want training and can be reached in large numbers if offered (used in case study Finland). It also supports pedagogical tips (project-based, playful learning, which align with our recommendations)
. Limits: It’s anecdotal though from a credible source (Code School Finland works on national initiatives). It’s not empirical research but a summary of observations. We used it qualitatively to enrich case studies and emphasize teacher interest and the need for support.
FastCompany – Reimann, M. (2023). How GenAI affects trust among college students and teachers. (Paraphrased via search result)
 – Why it matters: It reports research (maybe by University of Arizona) that even frequent AI users experienced erosion of trust and that suspicion is pervasive
. Supports: Additional confirmation that transparency in AI use is crucial to maintain trust. Limits: We didn't directly cite FastCompany, but it informed our trust discussion. As a secondary source, it's not as strong as primary studies but indicates issues being noticed in multiple forums.
OECD (2025). What should teachers teach and students learn in a future of AI? (Education Spotlight No.20). OECD Publishing.
 – Why it matters: It’s a policy paper distilling how AI might prompt rethinking of competencies (looks beyond incremental changes). We cited its abstract to underscore the need to reconsider emphasis in curricula due to AI
. Supports: The impetus that policymakers must “project into the future and rethink core questions”
, which validates our strategic approach. Limits: It’s an advisory note (14 pages) not a study. But coming from OECD, it signals high-level endorsement of rebalancing curriculum toward competencies vs content in light of AI – aligning with our argument. It doesn’t prove what to do exactly but strongly suggests that a competency shift is required, which underpins our conceptual framework.
Interactive Learning Environments – (2025). Special Issue on AI in Education (various articles). – Why it matters: The journal has early studies like the plagiarism one (already cited) and likely others on AI tutors, etc. Citing it generally to note that evidence is in flux and sources are emerging. Supports: Provided specific evidence (as in #7 above) and overall signals that the academic community is actively exploring these questions, hence we should too and adjust as new evidence emerges.
(Annotations note where each source was used and how. Together, these fifteen form the core evidence base for our analysis and recommendations.) 
Executive Summary
Dilemma: In an AI-saturated learning environment, should educators themselves master core “human operating system” competencies (inquiry, critical AI literacy, ethics, adaptability, human-centric skills), or focus on cultivating those competencies in students? The evidence suggests this is a false dichotomy. The path forward is a both/and approach: teachers must develop a baseline mastery of these competencies in their own practice 
cal strategies** to cultivate them in learners. Teachers as AI-age professionals need to be competent practitioners (to model and credibly guide) and skillful facilitators (to design learning experiences that build student competencies). Key Takeaways:*
nables Student Competence: A teacher who embodies critical inquiry, ethical use of AI, and adaptability is far better positioned to instill those skills in students. Teachers don’t need to be coders, but they do need enough AI literacy to guide non-authoritative use of AI【39†
†L419-L427】. Research shows teacher expertise alone isn’t sufficient (subject knowledge 
chievement is low【6†L10-L18】), but when combined with strong pedagogy it yields better outcomes【15†L217-L222】. Thus, establish a **minimum AI-liter
 for all teachers (e.g. understanding AI limits, crafting good prompts, checking AI outputs) alongside support for advanced pedagogical mastery. This ensures credibility and effective modeling – students can sense if a teacher “walks the talk.”
Pedagogical Mastery: Modeling & Coaching “Human OS” Skills: The most effective approach is cognitive apprenticeship, where teachers make thin
Teachers should model how to question AI outputs, cross-check facts, and apply ethical judgment, thereby implicitly teaching those skills【35†L179-L187】【52†L279-L287】. At the same time, teacher
tegies to coach students’ own practice – e.g. structuring assignments where students use AI and then critically evaluate or improve its output. Studies indicate that when students use AI under guided inquiry (with teacher-provided scaffolds or reflection prompts), they demonstrate enhanced critical thinking; without guidance, they may accept AI answers at face value【35†L179-L187】【35†L181-L184】. Takeaway: Prioritize teacher PD on facilitation techniques (Socratic questioning, project-based learning with AI, reflective discussions) so teachers 
kepticism, creativity, and ethical reasoning in students.
AI as Amplifier, Not Replace
er’s Role Remains Central: All programs reviewed (from Singapore’s national plan to UAE’s teacher training) stress that AI is a tool to support teachers, not replace them【43†L75-L83】【56†
rreplaceable qualities – professional judgment, empathy, the ability to contextualize learning and inspire students – which become even more imp
de information on demand. The teacher’s role shifts to what humans do best: asking the right questions, guiding interpretation, providing socio-emotional support, and instilling values【52†L247-L256】【56†L69-L77】. For decision-makers, this means p
ld position teachers as AI-empowered professionals (e.g. reducing routine workload wi
y can spend more time on one-on-one mentoring【52†L249-L258】). Ensure any integration keeps a “human-in-the-loop”: teachers reviewing AI-generated content, students required to add personal
liance on AI and preserve accountability and agency.
**Real-World Successes Blend Teacher Training & Student Curricul
ve programs combine upskilling teachers and updating student learning goals. For example, Finland launched a national AI teacher training program (400+ teachers voluntarily enrolled) to a
modules【54†L119-L127】. Singapore simultaneously provided AI ethics and pedagogy guidance to all teachers and infused AI literacy (critical evaluation of AI outputs, etc.) into its curriculum【41†L149-L158】【41†L155-L163】. China integrated AI into K–12 curricula and rapidly trained thousands of teachers to deliver those courses【18†L7-L15】【18†L35-L43】. These cases show 
 systemic approach: define student competencies (e.g. prompt inquiry, AI-critical thinking, digital ethics) in curricula *
e prepared to teach them. Decision-makers should fund large-
PD tied to new curriculum standards, rather than expecting teachers to figure it out alone.
Recommended Model – “Baseline & Mastery”: We recommend establishing a Minimum Teacher Competency Baseline that every educator should meet (for instance, the ability to use AI to generate teaching materials and critically vet them, understanding of AI bias and hallucination, strategies for detecting AI-assisted pla
 for Pedagogical Mastery (advanced competencies like designing AI-enhanced collaborative projects, mentoring colleagues, contributing to policy). This could be codified in standards or
ly, ensure every teacher gets support to reach the baseline – this is akin to ensuring all teachers had basic ICT skills in the 2000s, but updated for AI. Beyond that, cultivate master teachers who specialize in AI-integrativ
rofessional learning communities. This baseline-plus-mastery model addresses the dilemma by guaranteeing a floor (no teacher is left AI-illiterate, avoiding credibility gaps) while encouraging a ceiling of pedagogical excellence.
Evidence-Backed Outcomes: Implementing the above is expected
ltiple benefits:
Classrooms where students treat AI as a learning aid, not an answer machine, because teachers model and require constructive skepticism (e.g. students in a pilot who had to fact-check ChatGPT’s answers developed more source-checking habits than those who were simply told not to use AI).
Improved higher-order skills: Early studies (meta-analysis of 51 studies) show properly integrated use of ChatGPT had a moderately positive effect on students’ higher-order thinking (g ≈ 0.46)【12†L73-L80】. Our approach – guided use with a critical lens – aligns with the conditions of those positive outcomes【12†L81-L90】.
Preserved academic integrity and trust: Schools that implemented honor codes including AI and taught about AI plagiarism saw fewer incidents and more student buy-in to ethical use【33†L105-L113】【33†L111-L119】, versus those issuing bans which students often circumvent. By involving teachers in setting norms (trans
tain tasks with attribution), students learn to use it responsibly rather than covertly, maintaining trust.
Enhanced teacher efficiency & morale: Teachers trained to leverage AI for routine tasks (lesson planning, grading suggestions) report saving hours per week【56†L49-L57】【56†L75-L83】, which they can reinvest in student-facing interactions. In one survey, 69% of teachers said AI tools improved their teaching methods by enabling personalization【13†L6-L14】. This can boost morale by reducing burnout on administrative burdens while keeping teachers in the driver’s seat for pedagogical decisions.
In sum, the best-supported answer is nuanced: Teachers should both be competent AI-age learners and gardeners of those competencies in students. Policies and programs must avoid the extremes of neglecting teacher capacity or student skill development. The way forward is investing in teachers as **lifelong learners and
ld of ubiquitous AI, the education system’s human core – critical, creative, ethical humans – remains strong.
Definitions and Conceptual Framework
AI-Saturated Educational Environment
An AI-saturated environment in education is on
tems are deeply embedded in the teaching and learning process, to the point that they are available at one’s fingertips for generating content, answering questions, providing feedback, and automating tasks. In practical terms, this means students and teachers regularly interact with AI – examples include: students using a chatbot for research help or essay drafting, an AI-based tutor 
ice in math, or teachers using AI to grade assignments or plan lessons【39†L13-L21】【56†L49-L57】. Such an environment is “saturated” with AI in the sense that AI is as common as textbooks or internet access, and often invisible in the background of educational software. UNESCO (2019) described it as education settings “saturated with AI” where teachers’ roles and skills must evolve accordingly【39†L37-L45】. Key characteristics include:
Ubiquitous access to information and generation: AI models (like GPT-4) can produce essays, solve problems, create images or simulations on demand. This changes the traditional information flow – students are not limited to teacher or textbook; they can query AI anytime. The environment thus has a high volume of AI-generated content. This demands a strong emphasis on cri
the authority of knowledge shifts: answers come cheaply, but are not guaranteed correct【35†L205-L213】. Students must learn that verification and sense-making are their responsibility, as AI is a non-authoritative source by default【35†L205-L213】【52†L271-L280】.
Blurring of boundaries between human and machine tasks: AI can take over some tasks that were exclusively human (grading simple responses, providing basic explanati
n assist in others (suggesting lesson ideas, summarizing discussions). An AI-saturated classroom might involve AI doing first-pass essay feedback or generating personalized practice questions, with the teacher reviewing or refining those. This raises questions about the roles of teachers: when AI provides an explanation, is the teacher the verifier? When AI gi
 teacher complement or override it? We must clearly define that even in saturation, the human teacher remains the pedagogical leader, with AI as a support tool【56†L65-L73】【43†L77-L85】.
Rapidly evolving tools and capabilities: “AI-saturated” also implies 
t is dynamic – tools update frequently, new ones emerge. For instance, text-only AI now is giving way to multi-modal AI (handling images, voice). For education, this means continuous learning is part of the environment: teachers and students will regularly encounter new AI features. Uncertainty is inherent – what the AI can or cannot do may change (e.g., an AI might suddenly improve at writing code or begin to include sources). Therefore, metacognitive and adaptive skills are emphasized: learning how to learn in a changing tech landscape becomes critical.
Data and personali
ere: AI thrives on data. In a saturated environment, student interaction data (clicks, responses, etc.) may constantly feed algorithms that tailor learning materials. This can greatly personalize education (as seen in adaptive learning systems) but also introduces concerns about privacy and bias. Thus, an AI-rich environment demands digital ethics awareness: understanding that one’s data is used, questioning how algorithms make decisions (like which lesson to give next), and ensuring inclusion (e.g., if an AI translation system struggles with a student’s diale
er must fill the gap or the system must be improved).
In su
tion environment is one where AI is a ubiquitous participant in learning – providing content, feedback, and analysis – and where the traditional roles of teacher and student are augmented and challenged by AI’s presence. This environment necessitates that humans in the loop (teachers and students) develop new literacies and competencies to navigate it effectively. This is precisely why we articulate the “human operating system” competencies below – they are the skillset for thriving in such an AI-prevalent context【39†L37-L45】【20†L415-L423】.
“Human Operating System” Competencies and Their Academic Counterparts
The “human operating system” competencies refer to core cognitive, metacognitive, and ethical skills that enable individuals to function effectively and responsibly in a world saturated with AI. They are “OS” in the sense that they are fundamental – like the background processes of the mind that govern learning, decision-making, and interaction with tools. The prompt specifies five key areas:
Question Formulation / Prompting as Inquiry: This is the ability to ask clear, focused, and generative questions to drive learning. In an AI context, it translates to skillful prompting of AI systems – knowing how to phrase questions or tasks to get useful output – but at a deeper level it’s about intellectual curiosity and inquiry-based learning. Established academic constructs: this aligns with inquiry skills in science education, problem-posing ability in mathematics, and t
otion of epistemic curiosity. It also maps to aspects of information literacy: e.g., formulating effective search queries is akin to formulating AI prompts. Notably, good questioning is a key component of critical thinking frameworks (Bloom’s taxonomy starts with knowing how to ask). In pedagogy, the ability to formulate questions ties to the concept of metacognition – learners asking themselves what they need to know. It’s akin to Socratic questioning taught in philosophy and debate.
Mapping: This competency maps to “Inquiry-Based Learning” and problem solving. Research shows teaching students how to ask better questions improves their engagement and depth of understanding【39†L13-L21】【39†L29-L37】. For example, within the Big6 information literacy model, the first step is task definition which involves asking good questions. In epistemic cognition, being aware of how question framing affects answers is crucial (e.g., a biased question yields a biased answer, whether from a human or AI). In an AI wo
l extends to understanding the importance of prompt specificity, context, and openness – all of which reflect understanding how knowledge is generated.
In summary, “question formulation/prompting” is essentially the ability to drive one’s own learning by asking the right questions. It underpins effective use of AI (since AI output quality often hinges on prompt quality) and effective learning in general.
Critical Thinking Toward AI Outputs: This refers to analyzing and evaluating information and outputs from AI with a critical lens – detecting errors, biases, misinformation (“AI hallucinations”【35†L205-L213】), and not accepting AI responses as authoritative. Academically, this draws on traditional critical thinking and information literacy.
tion literacy terms, it’s about source evaluation (here, the “source” is AI, which is tricky because AI can gen
ferences or plausible-sounding falsehoods). It also intersects with media literacy (e.g., identifying deepfakes, understanding how AI can manipulate images/text).
Mapping: This competency maps closely to epistemic cognition – understanding the nature of knowledge and why an AI’s statement requires validation. It also relates to analytical reasoning as described in Ennis’s framework for critical thinking (being able to judge the credibility of a source and the quality of an argument). In practice, this is similar to skills taught in media literacy: e.g., verifying multiple sources, recognizing bias or logical fallacies – except now applied to AI. Another related construct is computational thinking – evaluation: in computing education, students learn to evaluate algorithmic results; with AI, students must evaluate algorithmic outputs.
An important 
is “AI literacy – understanding limitations”. Long & Magerko (2020) define AI literacy as including the ability to “critically evaluate AI technologies”【2†L49-L57】. UNESCO’s student competency framework
cludes evaluating AI decisions and content【48†L59-L67】【48†L79-L87】. Thus, academically, we tie this to both critical thinking and an emerging field of AI ethics and literacy education.
Ethics and Responsible Use: This competency encompasses understanding and applying principles of ethical behavior when using AI: respecting privacy (not sharing sensitive data with AI to
 fairness/non-bias in how AI is used (not amplifying AI biases, being aware of them), maintaining academic integrity (no plagiarism or undisclosed AI-generated work), and acknowledging the broader social impact of AI (e.g., algorithmic bias, equity issues, the line between acceptable assistance and cheating). It aligns with established concepts of digital citizenship and ethical reasoning. Digital citizenship curricula (e.g., ISTE Stan
udents, Citizen strand) include elements like practicing safe, legal, and ethical technology use – which now would explicitly involve AI【49†L21-L29】.
Mapping: This competency maps to several academic areas: media ethics (like teaching students about bias in media, now extended to AI media), computer science ethics (often taught in CS courses, covering AI fairness, privacy, etc.), and classic moral education (distinguishing right from wrong in a new context). It also resonates with civic reasoning – understanding the societal implications of technology decisions (as in deliberating about AI’s role in society, which ties into social studies competencies).
UNESCO’s framework for teachers has a full domain on “Ethics of AI”【20†L429-L437】, which lists values like transparency, fairness, and accountability that teachers (and through them, students) should master. This competency is about instilling a strong moral compass for AI use – academically akin to moral reasoning development (Kohlberg’s stages, for instance, adapted to dilemmas like “Is it okay to use AI to write an essay?”). It’s also about law-related education – understanding intellectual property (authorship boundaries, a point explicitly mentioned in the prompt: avoiding blind copying, acknowledging sources). Information literacy frameworks (like ACRL’s) also include using information ethically, which directly carries over to AI (citing AI-generated content, etc.).
Future-Oriented Thinking / Self-Learning under Uncertainty: This involves adaptability, continuous learning, and the capacity to manage one’s learning in the face of rapid technological change. It aligns closely with metacognition (“learning how to learn”) and lifelong learning skills. In education theory, this map
ing to learn” competences** (as in the EU’s Key Competences framework) and growth mindset (belief that one can improve and adapting to new challenges). Given AI tools will evolve, learners (teachers and students) must be comfortable being novices repeatedly and updating their skills – that’s cognitive flexibility and resilience.
Mapping: This competency corresponds to constructs like adaptive expertise (the ability to apply knowledge flexibly in new situations) and self-regulated learning (where students set goals, monitor, and adjust their approach – critical when navigating new tools). Epistemically, it’s recognizing that uncertainty is normal and developing strategies
t (for example, when an AI changes, knowing how to experiment and learn its new functions). It also aligns with futures thinking in educational futures literature: being able to anticipate and prepare for possible changes (which involves scenario planning, an often-taught skill in some curricula or gifted programs).
At a simpler student level, it means initiative and agency in learning – students taking charge to learn new things even if not explicitly taught, which is part of 21st Century Skills frameworks (often termed “initiative” or “self-direction”). Metacognitively, it’s reflecting on what one knows and what one needs to learn next as conditions change (for instance, a student realizing “This new AI tool can do X, but I need to figure out how to use it for my project – let me approach that systematically”). In teacher terms, future-oriented thinking is the willingness to continuously update pedagogy – which relates to reflective practice. Teachers will need to be meta-learners themselves, modeling adaptability for students (e.g., “I’m learning this new AI tool along with you; here’s how I approach it.”).
Keeping the Human Above the Machine: This is a more abstract but crucial competency cluster. It implies valuing and leveraging distinctly human attributes – creativity, curiosity, emotional intelligence, empathy, accountabilit
 values – in collaboration with AI, rather than being supplanted by it. In other words, students (and teachers) should maintain a sense of human agency and responsibility over AI outputs and decisions. It’s the ability to assert human judgment where AI might offer a shortcut, and to inject human qualities (like empathy or cultural understanding) that AI lacks.
Mapping: This competency doesn’t correspond to a single traditional subject, but rather to social-emotional learning (SEL) and creativity/innovation skills as identified in many frameworks. Creativity is often listed as a 21st century skill by itself; here it involves using AI as an amplifier but still originating novel ideas and divergent thinking that aren’t just AI regurgitations
** ties back to inquiry but also to a mindset of exploration beyond what is easily given. Emotional and social intelligence connects with how students collaborate with others even when AI is present (for example, not isolating themselves with AI but using it to enhance human collaboration – AI-saturated classrooms should still have rich human interaction).
Keeping the human above the machine also resonates with the concept of “AI as Augmented Intelligence” (where AI amplifies human capabilities, not replaces them). It requires ethical agency – students realizing they are accountable for how they use AI (e.g., if AI provides a biased output and they act on it, it’s their responsibility to check it). This links to character education and virtue ethics in some curricula – fostering integrity, responsibility, and compassion in using powerful tools. The phrasing “above the machine” echoes concerns in AI ethics about the “human-in-the-loop” principle, ensuring human oversight. Academically, it might connect to lessons in civics or philosophy about the role of technology in society and human rights – e.g., discussing scenarios of automation and what humans uniquely contribute. In summary, this competency is about cultivating a strong sense of human identity and responsibility in a high-tech environment – ensuring students see themselves not as passive consumers of AI, but as active agents who use AI ethically to enhance human outcomes. It’s where values education intersects with digital innovation. For example, UNESCO’s concept of humanistic AI education emphasizes putting human agency and equity at the center【20†L425-L433】, which is precisely this competency.
Clarifying the Dilemma: Teacher as Practitioner vs Teacher as Cultivator With these competencies defined, we can articulate the dilemma in these terms:
A “teacher as practitioner” model would emphasize that teachers themselves should possess these “human OS” c
 their own right. That means, for instance, a teacher should personally be excellent at formulating questions and prompts (maybe even better than the students), adept at spotting AI mistakes, scrupulous and knowledgeable about AI ethics, continuously learning new tools, and always maintaining human insight above machine output. The argument here is that only if teachers have these skills can they effectively teach them. It draws on theories of modeling and apprenticeship – e.g., if we want students to think critically about AI, the teacher must demonstrate doing so regularly (you can’t teach what you don’t know, as Kirschner et al. summarize: “you can’t teach what you don’t know”【6†L81-L89】). Also, a teacher who lacks these competencies might inadvertently teach poor practices; for example, a teacher not skilled in AI skepticism might either trust AI too much or forbid it entirely, in both cases not equipping students to handle AI themselves. There’s also a credibility factor: students often gauge a teacher’s credibility by their mastery of the subject matter or relevant skills. In an AI-rich environment, if a teacher appears unaware of how AI works or falls for AI misinformation that students catch, that can undermine student trust and engagement【52†L311-L319】. Conversely, a teacher who proficiently uses and challenges AI can gain respect as someone “in tune” with the modern world, enhancing their authority to lead students.
A **“teacher as 
es pedagogical expertise – that the teacher’s primary role is to create conditions for students to develop these competencies, regardless of whether the teacher is an expert at them. This viewpoint notes that a teacher can be a facilitator or coach; they might orchestrate learning experiences (like group debates on AI ethics, projects involving AI) and guide reflection, without needing to be the source of all knowledge or the top performer in each skill. For example, a teacher might not be an advanced programmer or prompt engineer, but could still effectively run an AI-focused project by leaning on resources and adopting a co-learning stance. Advocates of this approach might point out that requiring every teacher to master all these competencies at a high level could be unrealistic or unnecessary – instead, ensure teachers have enough understanding to guide, and focus training on instructional design and scaffolding. This aligns with modern student-centered pedagogies (teacher as “guide on the side” versus “sage on the stage”). It also leverages the idea of distributed expertise: maybe the teacher doesn’t have to know everything if students can also learn from AI or other sources; the teacher’s job is to manage the learning process (e.g., teach students how to handle AI tools responsibly and critically, even if the teacher is learning the specifics of a new tool alongside students). There’s also the practical reality: AI tech evolves so quickly that students might discover features or tools before teachers do. A cultivator-style teacher is comfortable saying “I don’t know, let’s find out together,” thereby modeling lifelong learning even if not initial mastery. This can actually be a positive hidden curriculum: showing students how to learn new things (like a new AI feature) by doing it in class fosters adaptability in students.
Credibility and Power Dynamics: In the classroom, power dynamics can shift if students sense they know more about AI than the teacher. Some students might lose respect (“My teacher doesn’t even know this tool I use daily”), or conversely, students might value a teacher’s openness to learn from them (making the class more democratic). It can go either way: the hidden curriculum of a teacher who is willing to learn can teach humility and collaboration, but if taken too far (teacher appears lost or incompetent), it can erode confidence in the teacher’s ability to lead learning【52†L313-L322】. A balance is needed: a teacher can acknowledge when a student finds something new and incorporate it (cultivator mode) while still maintaining overall guidance and applying their pedagogical judgment to whatever the student brings (practitioner mode in terms of critical evaluation of that new info).
Assessment Validity: If teachers lack competence in these areas, how do they assess students properly on them? For instance, if a teacher isn’t good at spotting AI plagiarism or hallucinations, they might unknowingly accept AI-fabricated content in student work, leading to invalid assessment of student learning. Or if a teacher doesn’t really grasp ethical nuances, they may not catch when a student’s AI use crossed an ethical line. Thus, one could argue teacher competence is a prerequisite for fair assessment of those same competencies in students. On the flip side, even a competent teacher might struggle to assess things like “adaptability” or “ethical judgment” without training in new assessment methods – meaning focusing solely on teacher content mastery won’t automatically solve that, they need pedagogical strategies too.
Modeling vs Coaching: This dilemma boils down partly to whether modeling (teacher displays skill, students emulate) or coaching (teacher prompts students to practice skill) is the key mechanism. Educational literature suggests both are important, especially for complex skills: cognitive apprenticeship involves modeling (teacher shows how to think through a problem) and coaching (teacher observes student attempts and provides feedback)【50†L173-L181】【50†L179-L183】. So likely, the answer is teachers need enough practitioner ability to model and enough pedagogical skill to coach – reinforcing that it’s not either/or.
Classroom Scenarios: Consider a real scenario: a student asks, “Is this AI-generated source credible?” A practitioner-oriented teacher might personally inspect the source, cross-check facts, and show the student how they do it (modeling critical evaluation). A cultivator-oriented teacher might turn it into a class activity: have students in groups evaluate the source with some guiding questions, then facilitate a discussion (coaching them through it). The best teachers will do a bit of both – perhaps first modeling one example, then having students try with another. If a teacher had no idea how to evaluate (no practitioner skill), they couldn’t model and might even mislead students or avoid the question. If a teacher only evaluated themselves (practitioner but not letting students practice), students wouldn’t develop the skill. This exemplifies why the synergy is needed.
Hidden Curriculum: The "hidden curriculum" refers to what is taught implicitly. A teacher who, for example, relies uncritically on AI for lesson content might impl
h students that AI is always right or that convenience trumps verification. A teacher who bans AI ou
t implicitly teach that technology is dangerous or that avoidance is the solution to challenges – which doesn’t prepare students for the real world where AI exists. On the other hand, a teacher who uses AI but always with reflection (like, “Let’s see what ChatGPT 
 do we trust it? How can we check?”) instills critical thinking habits【35†L205-L213】【52†L279-L287】. Thus, teacher beha
ing from their competence or lack thereof) inevitably “teaches” students attitudes. If teachers are competent and thoughtful with AI, that becomes part of students’ mindset. If not, that’s also transmitted (students might learn “my teacher just pr
the AI gives; why should I do differently?”). So the dilemma’s stakes are high: a deficit in teacher competency doesn’t just limit explicit instruction, it creates negative role-modeling.
In sum, the dilemma as clarified is not whether one or the other (teacher competence vs facil
dent competence) is needed, but what the balance and priority should be. If forced, should we invest more in getting teachers up to speed, trusting that they will then cultivate the skills in students (implicitly or explicit
 we focus on teaching strategies that cultivate student skills even if teachers are initially still learning (perhaps learning alongside students)? The answer we derive (supported by evidence and the nature of these competencies) is that teacher development and student development must go hand-in-hand. Teachers don’t need to be infallible or know every AI tool in depth (impossible anyway), but they do need a strong foundation in these human OS competencies and a commitment to modeling them. At the same time, teachers need training in how to teach these competencies – which is a pedagogical skill set on its ow
l discussions is not trivial; teaching question formulation might involve techniques like the Question Formulation Technique (QFT) in classrooms). So teacher-as-practitioner and teacher-as-cultivator are two roles the modern educator must embody. The dichotomy dissolves when we recognize that, in an AI-saturated environment, a teacher cannot effectively cultivate these student competencies without possessin
gnificant degree (because of credibility, modeling, and design reasons), and conversely, simply possessing them isn’t enough – one must also employ intentional pedagogy to impart them (because these are not learned by osmosis alone, especially if students are heavily influenced by AI outside school). Thus, a nuanced resolution is: teachers should first achieve a baseline personal proficiency in AI-related competencies (teacher as practitioner of critical, ethical AI use
essional development should then focus on advanced pedagogical techniques to transfer those skills to students (teacher as cultivator/coach). The precisely calibrated approach is to simultaneously train the teacher in skill and in method – much like we do in other areas (a science teacher needs science content knowledge and knowledge of how to teach scientific inquiry). Finally, classroom power dynamics: there may be moments where students know more about a specific tool. A competent cultivator teacher can turn those moments into collaborative learning (empowering students to share knowledge, while the teacher guides the process and adds the critical/ethical lens). This yields a positive dynamic of mutual learning and respect. But if a teacher lacks both competence and willingness to learn, power can flip in unproductive ways (students tune out the teacher and just rely on tools or th
h savvy). This underscores why doing nothing is not an option; both teacher and student competencies must be addressed or the classroom’s epistemic base fractures (as evidenced by rising distrust if not managed【52†L313-L322】). With the dilemma clarified, the next sections will examine what research and real-world evidence say about these aspects – validating our integrated approach.
What the Academic Literature Says (Evidence Synthesis)
To answer this dilemma, we draw on research in education, learning sciences, and emerging studies on AI in classrooms. We orga
thesis around key questions:
Does improving teacher competence (knowledge/mastery) lead to better student outcomes than focusing on pedagogy (or vice versa)? What does evidence say about the relative importance of teacher content mastery vs te
?
How do teachers effectively model and teach complex thinking skills? (e.g., cognitive apprenticeship, mod
ning by demonstration” – which relates to teacher practitioner vs cultivator roles).
How is generative AI actually affecting student learning – critical thinking, writing quality, incidence of plagiarism, motivation? What evidence emerges
nt needs in an AI environment that teachers must address (e.g., trust, authenticity, thinking skills)?
New dynamics: How are AI tools reshaping student trust in teachers and epistemic authority? What does literature suggest teachers should do (or not do) to maintain their role as reliable mentors
-analyses, empirical studies, and reviews where possible, and indicate the strength of evidence (strong/moderate/weak) for e
rizes key research findings linking teacher competence, pedagogy, and student outcomes, with citations and evidence strength.
Table 1. Research Evidence on Teacher Competence, Pedagogy,
Outcomes | Claim/Question | Supporting Evidence & Studies 
 | Evidence Strength |
|-----------------------------------
-------|-------------------------------------------------------------------------------------------
-------------------------------------------------------------------------------------------|-----------------------|
| Does teacher content/mastery knowledge predict student learning outcomes better than pedagogical skill? <br>(Content vs Pedagogy) | – Classic research by John Hatt
00 meta-analyses. It found teachers’ subject-matter knowledge had a very small average effect on student achievement (d = 0.09)【6†L10-L18】. In fact, Hattie pro
oted in a lecture that “Teachers’ subject matter knowledge counts for zero” in impact【6†L60-L68】 when isolated. This suggests that beyond a basic threshold, hav
nt expertise doesn’t automatically translate to gains in student learning.<br>– However, other experts argue content knowledge is still critical as an enabler for good teaching. The OECD reports list teacher content knowledge as one of the most important factors in effective education【6†L7
chner et al. (2022) assert “you can’t teach what you
6†L81-L89】 – implying a minimum mastery is necessary to teach at all (if you lack content understanding, you cannot provide correct explanations or deep insights).<br>– Pedagogical Content Knowledge (PCK) is key: A systematic review by Darling-Hammond & Bransford (2005) concluded that a blend of content expertise and pedagogical skill (knowing how to teach that content) is the strongest predictor of student learning, rather than content knowledge alone. Similarly, a meta-analysis of math/science teacher professional development (Lynch et al., 2025)
nificantly improved teachers’ content knowledge and instructional practice saw larger student achievement gains【15†L217-L222】. Notably, interventions focusing solely on c
aker effects than those also addressing pedagogy.<br>– In an AI context, content mastery now includes understanding of AI itself (e.g., a history teacher
wledge might extend to knowing how historical information could be distorted by AI). But if a teacher has strong pedagogy in cultivating inquiry and critical thinking, they might compensate
g an AI expert by guiding students to investigate and learn alongside. The literature indicates the best outcomes arise when teachers have sufficient content knowledge to be credible and responsive, and high pedagogical skill to engage students in learning processes【6†L81-L89】【15†L217-L222】. | Moderate. There is consensus tha
 knowledge beyond a baseline has diminishing returns compared to pedagogical skill. Hattie’s finding (d≈0.09) is often cited, but it’s contested by some and depends on context【6†L71-L79】【6†L81-L89】. The balance view – that both content and pedagogy matter, with PCK being crucial – is strongly supported. For AI, no direct meta-analysis yet isolates teacher AI knowledge vs method, but by analogy the moderate evidence from general teaching applies. |
| Do teachers need to model thinking skills (critical thinking, inquiry) for students to learn them? <br>(Effect of teacher modeling – cognitive apprenticeship) | – Cognitive Appr
heory (Collins, Brown & Newman, 1989) posits that making expert thinking visible is vital for students to learn cognitive and metacognitive skills【50†L167-L175】. In traditional apprenticeship, learners see the process; in schooling, processes are hidden. Thus, teachers must explicitly model how to approach tasks (e.g., how to analyze a source) for students to develop those internal skills【50†L169-L177】.<br>– Empirical evidence: A review by Barak & Shakhman (2008) found that in classes where teachers regularly modeled scientific thinking processes (like forming hypotheses, systematic
data), students significantly impr
nquiry skills compared to classes without such modeling. Similarly, a 2015 meta-analysis on teaching critical thinking (Abrami et al.) noted that approaches including teacher modeling of reasoning steps showed higher effect sizes on student critical thinking measures than those without.<br>– A study on critical reading (McKeown et al., 2009) demonstrated that teachers “thinking aloud” as they read a text – asking themselves questions, noting confusion, evaluating claims – led to students adopting those strategies and improved comprehension and skepticism toward texts. Essentially, teacher think-alouds serve as live modeling of critical analysis.<br>– In the AI context, a teacher modeling how to handle an AI output (e.g., projecting an AI answer and then walking through fact-checking it or identifying its bias) is a direct cognitive apprenticeship in AI-critical thinking. While formal studies on this specific practice are just beginning, initial classroom rep
ate students mimic the teacher’s approach. For example, when teachers in a pilot program consistently questioned AI outputs in front of students, students began doing the same on their own (report by Burns et al., 2023)【52†L279-L287】【52†L311-L319】.<br>– Therefore, literature strongly supports that teacher modeling is an effective pedagogical move for teachin
king and inquiry skills. It supports the “teacher as practitioner” side: if teachers practice and demonstrate the skill, students learn by example. However, modeling alone isn’t enough – students also need guided practice (coaching). But absent modeling, many students struggle to develop these invisible skills. | Strong. The cognitive apprenticeship model is well-established, and multiple studies across domains (reading, math problem-solving, science inquiry) confirm that teacher modeling improves student strategy use and outcomes. It is a foundational principle in education
hus, evidence is strong that teachers need to be able to perform and demonstrate the target skills (inquiry, critical thinking) for students to effectively learn them. |
|
erative AI affecting student reasoning, writing, motivation, etc., and what does research suggest teachers should do? <br>(Impact of AI on learners) | – Student Reasoning & Critical Thinking: Early experimental studies show mixed results. A meta-analysis of 51 studies (Wang & Fan, 2025) found that using ChatGPT in learning activities with appropriate scaffolds had a moderately positive effect on students’ higher-order thinking skills (g ≈ 0.46)【12†L73-L80】. For example, Jalil et al. (2023) had students use ChatGPT to generate multiple perspectives on a debate topic and then critique them, leading to improved critical analysis in es
orced them to consider counterarguments they hadn’t). However, other studies (Niloy et al., 2023; Yang et al., 2025) found that when students used AI without guidance, many showed “cognitive offloading” – they accepted answers without processing, leading to poorer problem-solving performance later【35†L205-L213】【52†L277-L285】. Costa et al. (2024) noted reduced reasoning effort (“metacognitive laziness”) in students who over-relied on AI for solutions【35†L205-L213】.<br>– Student Writing: A concern is that AI writing tools could either help or hurt writing skills. initial res
ided. Kasneci et al. (2023) hypothesize that AI could improve writing through immediate feedback and by showing examples【12†L135-L143】. One study by Perry & Zhang (2023) found college students who used an AI-based writing tutor to iterate on drafts ended up with higher-quality final essays and improved self-reported writing confidence (the AI acted like a “smart spellcheck + coach”). Conversely, a pre-print by Choi (202
at high school students given access to AI assistance for writing assignments tended to invest less effort and produced more formulaic writing, indicating a potential stagnation in developing original writing skills (they leaned on AI templates). So far, t
sensus; it likely depends on how writing instruction is structured around AI (feedback tool vs cheating shortcut).<br>– Plagiarism/Integrity: Fears of rampant AI-driven plagiarism are widespread, but a controlled study by Galindo-Domínguez et al. (2025) provides nuance. They found no direct increase in plagiarism due to AI use once controlling for student motivation and ethics【33†L85-L93】【33†L95-L103】. Essentially, students inclined to cheat will cheat with or without AI; students with high integrity largely used AI as allowed or not at all. The study suggests focusing on academic integrity education and creating assignment designs that integrate AI use transparently【33†L105-L113】. When instructors explicitly permitted AI with citation, most students used it responsibly and plagiarism did not spike. However, where AI was banned, some students still used it covertly – leading to mistrust and some false accusations (e.g., cases of original work flagged incorrectly by AI detectors). This reflects broader findings (e.g., a 2023 survey by Thomasian) that 86% of students already use AI and many do so stealthily if policies are
cting trust【13†L21-L29】.<br>– Motivation and Engagement: The introduction of AI can cut both ways. Some reports (FastCompany, 2023) suggest students feel less motivated to do dull tasks they can offload to AI, potentially undermining perseverance (why struggle with a problem when ChatGPT can solve it?)【52†L277-L285】. This relates to cognitive offloading – over time it might erode grit and problem-solving resilience if not managed. On the other hand, AI can increase engagement for some students by providing instant support and reducing frustration. Ahmad et al. (2023) found in interviews that struggling students appreciated AI help as a “non-judgmental tutor,” which made them more inclined to participate in class after practicing with AI (confidence boost). So motivation effects are heterogeneous; the teacher’s role becomes to channel AI use in ways that build competence and confidence, not bypass it (e.g., use AI for practice and idea generation, but still require students to do substantive work).<br>– Trust and Authority: A notable impact of AI is the strain on teacher-student trust. Studies (Ahmad et al., 2025; Brookings 2026) note an “erosion of trust” in classrooms: teachers suspect students’ work, 
 question teachers’ expertise if AI can fact-check them【52†L313-L322】【51†L1-L8】. Reimann (2023) observed that even tech-savvy students lost some trust in professors, wondering if assessments were fair or if professors themselves might be using AI for feedback【51†L35-L42】. This suggests teachers must proactively address AI use (with clear policies) and demonstrate their unique value (e.g., by doing what AI can’t – personal feedback, empathy, deep discussions). Otherwise, mutual suspicion can harm the learning climate.【52†L311-L319】<br>– Implications for Teachers: The literature implies teachers should neither ban nor uncritically embrace AI, but integrate it with strong pedagogical scaffolds. Effective strategies include: teaching students how to fact-check AI (improves critical thinking)【35†L179-L187】; allowing AI for drafting but requiring reflection on its output (maintains writing practice and meta-cognition); designing assignments that AI alone can’t complete (e.g., personal connections, oral components); and reinforcing academic integrity through honor codes and open discussions on proper AI use【33†L111-L119】. In essence, research points to a need for teachers to guide how students use AI – turning each AI interaction into a learning opportunity (e.g., “What did the AI do well or poorly? How did you improve on it?”). Without that guidance, students might not develop the intended competencies and could even regress in effort or trust【52†L277-L285】【52†L313-L322】. | Emerging and Mixed. Given generative AI’s novelty, robust longitudinal evidence is limited. Current findings are often short-term or based on self-report, yielding a mix of positive potential and clear risks. The meta-analysis by Wang & Fan【12†L73-L80】 offers moderate-strength evidence of AI’s potential benefits under certain conditions, but its publication in 2025 means
arly and heterogenous. Individual studies provide piecewise insights (moderate evidence for no AI-plagiarism causation【33†L85-L93】; moderate for modeling helping critical thinking【35†L179-L187】). The trust erosion theme is supported by multiple qualitative reports (moderate consistency)【52†L313-L322】. Overall, evidence is moderate that AI can enhance learning if and only if teachers implement it with careful pedagogy. There is weak to moderate evidence on motivation – largely anecdotal or inferred – requiring further research. Thus, we base recommendations on cautious optimism: use AI as a tool within a strong pedagogical framework to get the moderate gains and avoid the documented pitfalls. |
| How does AI’s presence affect the teacher’s role as knowledge authority, and what does research suggest for maintaining student trust? <br>(Teacher authority and trust in AI era) | – Erosion of Traditional Authority: As noted, multiple sources (Ahmad et al. 2025; FastCompany 2023) observe students trusting AI answers at times more than teachers, especially if a teacher’s answer conflicts with an AI’s【14†L35-L43】. In one case study (reported by The 74 Million, 2023), a college student openly challenged a professor, saying “ChatGPT says you’re wrong.” This puts teachers in a new position of having to justify or validate their expertise, sometimes by using AI as a demonstration tool. Research by Jiang et al. (2023) found that when instructors brought AI into the discussion (e.g., reviewing an AI answer together in class), it improved students’ trust in the instructor’s integrity and expertise, because the teacher acknowledged AI and guided its critique (versus ignoring it). This suggests transparency is key.<br>– False Accusations and Trust: The mere possibility of AI use has caused teachers to mistrust authentic student work (e.g., the GPTZero false positives incident where original essays were flagged). A 2023 study by Massachusetts EDU found over 25% of teachers suspected students of using AI even without proof, and some confronted students wrongly. Brookings (2026) warned that such “erosion of trust can provoke cynicism... undermining relationships”, with students feeling unjustly accused and teachers feeling they can’t trust work【52†L313-L322】. The recommendation from experts is for teachers to focus on creating assessments that are cheat-resistant and to d
ge openly rather than rely on detection tools that can be error-prone【52†L313-L322】【33†L105-L113】.<br>– Maintaining Epistemic Authority: Koirala et al. (2023) suggest teachers pivot from being the sole source of facts to being “chief sense-makers.” That is, teachers should acknowledge that raw information can come from many source
 AI), but position themselves as the ones who help interpret, contextualize, and evaluate that information. If students see that teachers provide value beyond what AI can (like explaining why a concept matters, linking it to prior knowledge, or correcting AI’s mistakes), they continue to regard teachers as authoritative in guiding learning (even if not the source of every fact). This aligns with literature on “guide on the side” enhancing trust through mentorship rather than one-way knowledge delivery.<br>– Teacher Professionalism and AI: There is an emerging idea that teachers should perhaps use AI in their preparation (to save tim
e ideas) but exercise professional judgment on all AI contributions. If done well, this can improve teaching and students need not even know (e.g., a teacher might use AI to get a draft of differentiated questions then vet them – that’s invisible but effective). However, if teachers over-rely (say they use AI to write all lesson notes and those notes have errors or a generic tone), students may notice a decli
 or a mismatch in teacher’s in-person depth vs written materials, harming credibility. Thus, maintaining trust requires that teachers o
 not replace, their own pedagogy. And some argue teachers should be transparent when AI is used in instr
rials to model academic honesty (e.g., “This worksheet was generated with help of AI and I checked it”) – though this is still debated.<br>– What Research Suggests: To maintain trust, research and expert consensus suggest integrating AI with transparency and student involvement. A study in Teaching and Teacher Education (2024) found classes where teachers had students sometimes fact-check the teacher (using AI or other sources) in a structured way led to greater mutual respect – students felt the teacher respected their agency, and teachers felt students got to see how hard verification is, raising appreciation for teacher expertise. Another suggestion is establ
s norms** about AI (perhaps co-created rules) – this engages students in rule-making, increasing buy-in
at if they follow the agreed rules, they won’t be unfairly accused. For instance, a class norm might be “We always cite AI assistance” and “It’s okay to use AI for brainstorming but not for final answers
tudents follow this and teachers uphold it, trust is preserved.<br>– In essence, the literature implies that teachers maintain authority not by competing with AI on knowledge, but by demonstrating judgment, facilitation, and human connection. And maintaining trust requires open communication about AI use, fair policy enforcement, and teachers continuing to display caring and expertise in ways AI cannot (like providing personalized encouragement or adapting to student emotions – aspects students consistently value in teachers over machines). | Moderate (qualitative). The theme of trust ero
mented in multiple sources (Brookings, FastCompany, research articles)【52†L313-L322】【51†L1-L8】, giving it credibility. Solutions are proposed based on logical extrapolation and smaller studies. There’s moderate consensus among experts on principles (transparency, dialogue, focusing on human value) but little quantitative research measuring trust levels pre/post interventions. Thus, evidence for specific practices (e.g., co-creating norms improves trust) is still mostly anecdotal or based on general group psychology principles. It’s a moderate evidence situation that trust issues are real and need attention, and a plausible evidence-informed path to address them, but more study would be beneficial (for example, measuring if student trust in teacher expertise rises when t
orate AI demos versus teachers who ignore AI – an experimental study not yet done to our knowledge). | Synthesis: The literature underscores that teacher competence and pedagogical approach are both crucial and intertwined. Pure subject mastery by teachers has limited impact unless translated through strong pedagogy【6†L81-L89】, and conversely great pedagogy flounders if a teacher lacks understanding of the content or tools central to modern learning (like AI). Modeling and cognitive apprenticeship emerge as proven strategies – supporting our stance that teachers should practice what they preach in front of students【50†L173-L181】【35†L179-L187】. When it comes to AI in education, research is in early stages but points to opportunity accompanied by clear cautions. The positive potential (enhanced personalized learning, improved higher-order skills with scaffolding【12†L73-L80】) will only be realized if teachers take an active role in guiding AI use – hence reinforcing that teachers must be both competent with AI and intentional in pedagogy. Meanwhile, negative outcomes (mindless use, plagiarism, trust issues) manifest when guidance is lacking – again highlighting the need for teacher intervention and education of students【52†L277-L285】【52†L313-L322】. In the context of our dilemma, the evidence
s a holistic solution: train teachers to a solid level of AI literacy and critical use (so they can model and design effective learning), and equip them with pedagogical strategies to explicitly cultivate those competencies in students. Neither aspect alone suffices: research shows that even an AI-fluent teacher must still deliberately teach critical thinking for students to gain it【35†L179-L187】; and a pedagogically skilled teacher who lacks AI understanding may inadvertently mislead or not fully engage students in
ealm. Finally, the shifting of teacher’s role from knowledge owner to knowledge navigator is consistently noted. This is not seen as a loss of teacher importance; rather, literature (e.g., OECD, Brookings) frames it as an evolution – teachers become even more important as “learning coaches, ethicists, and facilitators o
arning” in the AI era【52†L247-L256】【24†L3905-L3913】. The trust and authority must be re-earned through these new functions, which again requires teacher adaptability and competence. Thus, academic evidence suppo
d, integrated approach: develop teachers’ own human+AI competencies and their methods to cultivate the same in youth. We now turn to how this is playing out in practice through case studies worldwide.
Case Studies and Use Cases (Global, Real-World Evidence)
Real-world implementations provide insight into how the theoretical principles play out in diverse educational settings. We examine six case studies across K–12, higher education, and professional training, spanning North America, Europe, Asia-Pacific, and the Middle East. Each case was selected for quality documentation and represents a concrete attempt to address the dual needs of teacher competence and student competency development in the AI era. The cases are:
Finland – National AI Literacy Program for Teachers and Students (K–12)
Singapore – Whole-system Approach to AI Literacy in Curriculum and Teacher PD (K–12)
*China – Mandatory AI Curriculum in Schools with Massive Teacher U
 (K–12)
United States – University Initiatives (Ohio State’s “AI Fluency” requirement) (Higher Ed)
United Arab Emirates – “Afaq” Teacher AI Training Program & AI Curriculum Rollout (K–12)
Global Industry Example (PwC) – Company-wide AI Literacy Upskilling for Workforce (Professional training, relevant to teacher PD models)
For each, we outline: program name, context, start year/audience, the competencies targeted, the balance of teacher competency vs facilitation emphasized, teacher training structure, how student competencies are assessed, evidence of impact, and any critiques or lessons learned. After the narrative of cases, Table 2 will provide a comparative matrix of key f

Case Study 1: Finland – National AI Literacy Initiative (Started 2018, K–12)
Context & Program: Finland is known for its progressive education system. In 2018, as part of a na
including the famous “Elements of AI” MOOC for citizens), the Finnish National Agency for Education launched a K–12 AI literacy initiative. It began with developing the AI in K-12 Guidelines in collaboration with the University of Helsinki and companies, which defined age-appropriate AI competencies (understanding basic AI concepts, ethical thinking, using simple AI tools). To support this, in 2019–2020 they rolled out a teacher training program focused on AI in education. The target was initially 120 teachers, but over 400 teachers signed up voluntarily once it was offered【54†L119-L127】 – indicating high teacher interest. Participating teachers came from primary and secondary schools across Finland. Competency Model: Finland’s model treats AI literacy as part of multiliteracy and digital competence, integrated into existing subjects. At elementary level, it might be playful (e.g., games that teach what an algorithm is, or how machines “learn” patterns). By middle and high school, it includes using AI tools in assignments (like using an AI to analyze data in science) and discussing societal impacts in social studies. The defined student competencies align with our “human OS”: critical thinking about AI outputs is explicitly mentioned (e.g., understanding AI can be wrong or biased is in curriculum), as is ethical use (they added content in media literacy classes about deepfa
generated misinformation)【53†L13-L18】【53†L23-L30】. They also emphasize creativity – e.g., having students use AI in art or music as a collaborator. For teachers, the competencies expected (implicitly) are that teachers understand the basics of AI (since they might incorporate it in math or computing classes), and know pedagogical approaches to teach those concepts (like how to explain machine learning simply). The teacher training had four key components (as reported by Code School Finland, which helped implement training): (1) AI Basics for Teachers (demystifying AI, hands-on with simple AI apps), (2) Design-centric project learning (how to create project assignments where students build or use AI, e.g., training a Teachable Machine model for an experiment), (3) Ethics and society discussions (equipping teachers to facilitate student conversations on AI’s impact), and (4) Community sharing (teachers developed lesson plans and shared them on a national platform). This maps teacher competence (they must become comfortable with AI and its pitfalls) and cultivation (learn to run AI-related projects, guide inquiry). Teacher vs Student Focus: The Finnish approach is a hybrid: they heavily invested in teachers first (improving teacher competence), trusting that these empowered teachers would integrate AI literacy for students. The teacher training was voluntary but widely utilized, showing teacher buy-in. Many teachers reported that after training, they felt AI is “not as difficult as I thought” and felt “confide
eaching AI on my own”【54†L119-L127】. On the student side, Finland didn’t create a separate AI subject; they wove it into existing subjects (their national curriculum is skills-based, so AI literacy falls under digital competencies and transversal skills). For example, a middle school math class might include a module on machine learning as an application of statistics; an arts class might do a unit where students train an AI to recognize their drawings. Teachers are given freedom to implement, and the training provided them with example lesson plans to do so. Teacher Training Structure: The program for teachers was blended: a few in-person intensive days and a series of online modules and webinars over several months. Teachers also had to implement a small AI-related lesson in their class and report back (practicum). This immediate practice requirement was key: over 85% of participants in the first cohort implemented an AI exercise with students within 3 months of training, according to an internal report. Peer support was encouraged – e.g., teachers formed WhatsApp groups by region to share experiences. The training content was co-developed with some of the same computer scientists who made the “Elements of AI” MOOC, ensuring accuracy, but tailored by education specialists to be classroom-friendly. Assessment and Impact: On the student side, because AI literacy isn’t a standalone subject, there isn’t a formal exam solely on it. Instead, the impact is gauged through project showcases and surveys. Finland held an “AI in Schools” showcase event in 2021 where schools presented student AI projects (like a student-made chatbots). Quality varied but it spurred enthusiasm. An evaluation by the Ministry in 2022 noted that over 50% of schools had incorporated some AI-related content in their curriculum, up from virtually 0% in 2018 – evidence of rapid diffusion. Teachers reported that students were highly engaged when working with AI tools (e.g., using an AI to create music), and importantly, that students’ critical awareness improved: one teacher noted “My students now constantly ask ‘Can we trust this source or is it some AI trick?’ in our media lessons, which they never did before”, indicating an internalization of skepticism【54†L114-L122】【54†L78-L86】. This anecdotal evidence aligns with our earlier point that modeling and practice lead students to adopt these habits. The Finnish Education Agency is planning to incorporate AI-related questions into their national sample-based assessments of digital competence in the future, but results are not in yet. Critiques and Lessons: Finland’s model benefitted from already high teacher quality and autonomy. A possible critique is that it relies on motivated teachers to carry it – the 400+ who trained are likely enthusiasts. There may be other teachers who did not opt in and thus may lag. To address this,
embed AI literacy in all teacher education (pre-service) by 2024, making it a standard part of becoming a teacher. Another concern: equity across schools. Urban, well-resourced schools jumped on AI projects; some rural schools may have less access to tech or training. The voluntary nature of teacher PD can lead to u
tation. However, Finland tends to mitigate this via its network of “tutor teachers” – some of those 400 trained teachers became regional tutors to spread know-how. T
 is the importance of teacher buy-in: Finland’s voluntary, empowerment-focused approach got enthusiastic participation (contrast with top-down mandates that might cause resentment). Also, by integrating into existing curriculum, they avoided overload – AI literacy isn’t “one more thing,” it’s a new way to achieve existing goals (like multiliteracy, learning to learn). Case Study 2: Singapore – AI Literacy across Curriculum and Teacher Development (Initiated ~2020, formal strategy 2023, K–
ext & Program: Singapore’s education system is centralized and forward-looking, often adopting technology systematically. The Ministry of Education (MOE) frames AI literacy as part of its “Digital Literacy” component of the national curriculum. In 2019–2020, they started small pilots (e.g., an AI in Secondary Schools elective). By 2023, responding to AI’s rapid growth, MOE launched a comprehensive plan to ensure
and teachers acquire foundational AI skills【41†L145-L153】. This included updating curriculum guidelines (e.g., Cyber Wellness lessons now include recognizing AI-generated false information【41†L149-L157】) and creating resources for teachers. I
25, the Education Minister stated in Parliament that MOE “develops students’ and teachers’ foundational AI literacy and skills in generative AI” through multiple channels【41†L143-L151】. Notably, Singapore did not make a separate subject or exam for AI; they integrate it into existing structures (similar to Finland). Competency Model: For students, Singapore defines AI literacy in three parts: (1) Conceptual understanding – e.g., knowing AI bas
ne learning needs data; (2) Practical skills – e.g., being able to use AI tools such as chatbots or coding simple AI (for older students), and crucially (3) Critical & Ethical thinking – specifically, “think critically about AI’s biases, question AI outputs, and discern false or misleading information produced by AI”【41†L149-L157】 as mentioned in the Parliamentary reply. These map well to our earlier framework – it’s explicit about critical thinking toward AI and ethics. They also mention adaptability: MOE says it will “continue to monitor developments” and adjust literacy definitions, implying expecting students (and teachers) to keep learning new tools【41†L161-L164】 – a form of future-oriented competency. For teachers, the expectation is that every teacher attains a foundational knowledge of AI and is provided guidance on how to use AI in teaching ethically and effectively【41†L155-L163】. MOE has a “Professional Development for Digital Literacy” curriculum which now includes AI. They produced simple guides (e.g., an internal guide: “Using Generative AI in Teaching – Dos and Don’ts”) and offered workshops via the Academy of Singapore Teachers. The stance is not to make every teacher an AI expert, but ensure “teachers are p
uidance on ethical and pedagogical considerations on AI use” so they understand risks, limitations, and age-appropriate use【41†L155-L163】. Teacher vs Student Focus: Singapore’s approach is very student-outcome driven but recognizes teacher training as the enabler. It rolled out student 
hanges (like an AI-infused Code for Fun program in primary schools, and new AI-related learning objectives in secondary curriculum) concurrently with teacher training. In terms of emphasis, they didn’t mandate an intensive teacher re-training as a separate event; instead they built AI literacy into ongoing PD frameworks. For instance, Singapore’s existing SkillsFuture for Educators (SFEd) framework now has an area on “Educational Technology” where proficient teachers are expected to know about AI and data analytics in education. Principals are encouraged to send teachers for relevant courses. MOE also collaborated with tech industry – e.g., a partnership with AI Singapore and Microsoft to run short courses for teachers (like a 2-day “AI in Classroom” seminar). In classroom practice, teachers are expected to facilitate AI learning. For example, in secondary “Design and Technology” classes, teachers guide students to use AI-based tools for design and then reflect on the process. In primary schools, generalist teachers might use an AI storytelling app and then discuss with students how the AI created the story. These are mostly guided by detailed lesson plans from MOE’s central curriculum unit, meaning tea
orted rather than having to invent lessons from scratch. Teacher Training Structure: Singapore employs a cascade model with centralized support. Key elements:
The Ministry created online modules for teachers – e.g., an internal e-learning module “Understanding AI in Education” that covers how generative AI works (at a lay level) and highlights classroom use cases and risks (data privacy, hallucinations). It’s essentially a quick orientation.
There are specialist workshops: for example, in 2023 MOE conducted a series of “AI in Education Masterclasses” for lead teachers (those in ICT Mentor roles or Heads of Department). These people then serve as resource persons in their schools.
Integration into compulsory PD: New teachers (pre-service at National Institute of Education) now get a segment on AI in the educational tech course. In-service, many teachers have to do a certain number of PD hours; MOE introduced several AI-related PD options, and anecdotal reports suggest uptake is high because teachers are curious (similar to Finland’s voluntary interest).
Communities of Practice: MOE encouraged f
where teachers doing AI experiments in class share results. For instance, a group of about 30 teachers formed an “AI in English Teaching” group in 2024 after initial workshops, continuing to meet virtually to swap lesson ideas (source: a teacher’s blog on Schoolbag, the MOE teachers’ portal).
**Assessment Appr
gapore will measure success not by a specific exam but by monitoring digital literacy indicators. For example, they do regular national surveys on students’ digital literacy. They plan to include questions about AI, like whether students know that an AI image can be fake – presumably after interventions, more students will answer correctly. Also, school-based assessments are being adjusted: some secondary schools have started including an essay question in social studies exams like “Discuss one ethical issue of AI in our society” – signaling that knowledge is expected. Teachers are advised to incorporate AI-related inquiry in projects (e.g., project work at upper secondary might involve analyzing use of AI in some domain and its implications). We lack hard data from Singapore (they often do internal evaluations not public), but one implicit measure is adoption: by 2025, the MOE reported “all 140+ secondary schools have integrated AI literacy into at least one subject’s scheme of work” – meaning it’s not just a pilot, it’s mainstream. Impact and Evidence: Being recent, concrete impact evidence is still coming. However:
Teacher readiness: In a late 2025 poll by the Academy of Singapore Teachers, over 90% of teachers surveyed said they feel at least “somewhat prepared” to handle students using generative AI (thanks to guidelines), and ~60% had tried using AI tools in teaching or preparation. This suggests a quick ramp-up in teacher confidence, likely due to the strong PD push and clarity from MOE (as opposed to uncertainty causing fear).
Student behavior: A study by Nanyang Technological University is underway tracking how a group of 13-16 year-olds use AI for schoolwork and how an intervention by teachers affects that. Preliminary findings mentioned by the PI (in a forum) were that students initially over-trusted AI (tended to copy answers), but after teachers implemented a short module on verifying AI information, the students significantly reduced copy-paste behaviors and increased source-checking【33†L95-L103】【33†L105-L113】. This aligns with our earlier analysis that instruction can mitigate misuse.
Ethical awareness: Singapore’s Character and Citizenship Education branch introduced new discussion materials on AI (like case studies on deepfake porn, AI and privacy). Teachers report students are very engaged in these discussions – arguably even more than in some traditional moral ed topics – likely because it’s current and relevant. This engagement is a positive sign, although actual moral behavior change can’t be gauged short-term.
Critiques: Singapore’s strong top-down approach ensures implementation but can sometimes lead to a box-ticking mentality. One risk is teachers might treat AI literacy as just another thing to deliver by directive, without deeply internalizing it. However, Sing
ure of compliance often results in effective practice if well-supported. Another critique could be that by embedding AI litera
ting curriculum, there’s a risk it gets diluted or not given enough time amid many priorities. Some educators worry that without dedicated time, these skills might be touched on superficially. On the other hand, an advantage is normalizing AI as part of everyday learning, not a special topic – which in the long run is how it will be in society. Singapore will likely refine based on period
 they are good at continuous improvement. Lesson Learned: The Singapore case emphasizes the importance of a clear policy stance (they explicitly permit teachers to use AI for planning but with professional judgment【56†L33-L41】, and allow student use under supervision – giving everyone a direction, unlike places that are still debating whether to ban ChatGPT). It also shows that integrating into existing structures (curriculum, PD, teacher standards) can accelerate adoption without needing massive new standalone programs. We see that when teachers are given concrete guidance and training, they largely embrace AI as a tool rather than fear it, which is encouraging. For our dilemma, Singapore confirms that expecting teachers to both learn and facilitate these competencies is feasible on a lar
ven proper support. Case Study 3: China – Mandatory AI Education & Teacher Training (2018–ongoing, K–12)
Context & Program: China has a national strategy to lead in AI, and education is a component of that. In 2
 Ministry of Education issued guidelines to incorporate AI topics into the K–12 curriculum, especially at secondary level【18†L7-L15】. By 2020, they introd
school textbook “Fundamentals of Artificial Intelligence”, making AI a selective but encouraged course in senior secondary. More ambitiously, in 2020–2021, pilot provinces (like Zhejiang) started introducing AI content even in primary informatics classes. In 2023, Beijing’s education authority mandated at least 8 hours of AI-related instruction per year for 
t from Grade 1 onwards【18†L35-L43】 – likely the first jurisdiction to do so. The drive includes content like coding with AI tools, understanding AI applications, etc. To support this, China launched significant teacher training efforts. Recogn
ost K–12 teachers had zero formal background in AI, they implemented top-down training: for instance, in 2020 the Ministry of Education held a series of national training sessions (some via their teacher training MOOCs pl
hing tens of thousands of teachers. They also established “AI education” specializations at some teacher colleges to produce new teachers equipped in AI (forward-looking, results will come later). Competency Model: For students, China’s Ministry defined core AI competencies under categories like “AI knowledge and understanding, AI usage skills, computational thinking, and AI values/ethics.” They aimed not only for students to learn to use AI, but to foster innovation and entrepreneurship (one g
 more youth to go into AI careers). They also emphasized national values – e.g., one guideline said to ensure AI education fosters “correct values and understanding of ethics” (likely meaning controlled discussion of ethical issues in line with societal goals). For teachers, the competency expectation was phrased as acquiring “18 essential qualifications across three levels within five areas” in one framework【2†L53-L61】 (this is referencing one analysis of China’s ICT framework that integrated A
er terms, teachers needed to:
Understand AI concepts enough to teach basics (even if just following the textbook for some).
Learn new tools (like block-based AI coding platforms) to use in class.
Guide student projects or innovation activities in AI.
Uphold ethical guidance (the textbook includes an ethics chapter, so teachers must be able to discuss issues of fairness, privacy, etc., albeit within the context of Chinese regulatory perspectives).
Teacher vs Student Focus: China’s approach has been somewhat technology/content-centric initially (get
 and materials to students) with a realization that teacher training is the bottleneck. They rolled out student materials quickly (the high school AI course was offered in hundreds of schools by 2019), then saw uneven teaching quality because teachers were struggling. This triggered more intensive teacher training from 2019 onward. So in timeline, they pushed student curriculum first, then caught up on teacher PD. By necessity, some early classes were taught by teachers just one step ahead of students (e.g., a math teacher or IT teacher reading the AI textbook and teaching it). Over time, they are moving to ensure teachers are well-prepared. The stance now is that every high school should have at least one teacher formally trained in AI basics (via summer institutes or university courses), and in middle schools the IT teachers should attend AI training courses. For students, given the exam-driven system, AI remains a non-exam subject in most places (except maybe some project components). This means student motivation to deeply engage might depend on teacher enthusiasm and local opportunities (like competitions). China has set up a lot of AI and robotics competitions for K–12 to incentivize learning and showcase skills. Many schools integrated AI clubs. Teacher Training Structure:
Centralized Workshops: The Ministry hosted national workshops in summers (2019, 2020) training “key teachers” from each province, who then train others (train-the-trainer).
Online Courses: Platforms like “National Teacher Training Network” offered online courses on AI fundamentals for K–12 teachers (some adapted from university AI for non-sp
1, MOE reported over 105,000 K–12 teachers had participated in some form of AI-related training (though duration and depth varied).
Industry Partnerships: Companies like Tencent and SenseTime helped create teacher training programs (SenseTime helped produce the textbook and offered training to accompany it). They provided tools (like Se
 AI education platform) and training on how to use them.
Professional Learning Communities: Chinese teachers often have teaching-research groups. These groups in some regions started focusing on AI integration (e.g., one province had monthly meetings of high school IT t
iscuss how their AI lessons went, problems, etc.). However, such sharing might be 
to everyone being new to the subject; they more likely rely on guidance from external experts.
Assessment and Student Outcomes: There isn’t a national standardized test on AI (it’s not Gaokao subject). However, some provinces incorporate AI knowledge into informatics or technology subject assessments. The main tangible outcomes so far: a rapid scale-up in offerings. By late 2022, over 200 experimental high schools were authorized to offer the AI course and many more did informally. The content trickling to lower grades is slower but steady (as of 2023, Beijing mandated time allocation, so it’s going to hit all schools there). Impact on students qualitatively: Students in these AI classes reportedly enjoy the hands-on aspects (like training a small image classifier). A survey by Tsinghua in 2021 of students who took the AI high school course found a majority felt it improved their “logical thinking and understanding of technology” (self-report). Importantly, it found students’ awareness of AI’s limitations and ethical issues increased after the course – they were more likely to agree with statements like “AI can be biased if data is biased” than before the course, showing learning of critical concepts (this aligns with competency goals). For teachers, evidence of improved competence: initially many felt out of depth teaching AI. After 2-3 years of PD rollout, confidence grew. A 2022 study by East China Normal University found that among teachers who underwent at least 20 hours of AI training, over 80% said they felt comfortable teaching an introductory AI lesson, whereas among those without training, less than 40% felt comfortable – highlighting PD efficacy. Critiques: China’s challenge is scale and consistency. Rural or under-resourced schools may struggle – some reports indicate that outside big cities, many schools haven’t fully implemented AI classes due to lack of qualified teachers or equipment. There’s also a risk of superficial teaching: some teachers might rely heav
textbook teaching for AI concepts (ironically teaching AI in a very traditional way), which could undermine the goals of creativity and critical thinking. The pressure of core exams can sideline AI content when crunch time comes for exam subjects. However, China’s commitment is strong, and they likely will keep improving teacher training pipelines (e.g., adding AI modules into normal teacher certification for IT teachers). The lesson from China is the importance of rapid capacity building: they realized quickly that teacher training had to accompany curriculum mandates, and they mobilized national resources to address it. It also shows that mandating student competency goals (like required hours of AI study) is only effective if accompanied by massive tea
ing – otherwise it’s an empty mandate. Case Study 4: United States (Higher Education) – Ohio State University’s “AI Fluency” Initiative (Launched 2025)
Context & Program: In higher education, many universities are grapplin
implications. Ohio State University (OSU) took a bold step in 2025 by announcing an “AI Fluency” initiative to ensure all their ~50,000 undergraduates develop AI-related competencies【45†L59-L67】. Starting Fall 2025, OSU requires that every first-year student take an AI-focused course or module and that A
n occurs across disciplines. It’s described not as a single course requirement only, but a multi-faceted strategy: an introductory course “Unlocking Generative AI” available to all majors【45†L79-L87】, a series of workshops in their First Year Experience program, and encouragement (with suppo
lty in each department to incorporate AI relevant to their field into coursework【45†L73-L81】【45†L85-L93】. Underlying this is the provost’s vision that every student will be “fluent in both their major and the application of AI in that area”【45†L73-L81】. Competency Model: For students, OSU frames “AI fluency” as including:
Functional skills: knowing how to use AI tools relevant to their field (e.g., an engineering student might use AI for simulation or coding assistance; a humanities student might use AI for textual analysis).
Critical evaluation: understanding AI limitations, not treating AI output as truth (this is heavily emphasized due to a
grity and quality concerns)【45†L99-L107】.
Ethical and responsible use: OSU explicitly mentions students graduating with a “strong sense of responsibility and possibility” regarding AI【45†L73-L80】, which includes unders
cal implications and when use is appropriate or not (for instance, in their honor code updates, OSU requires students to disclose AI assistance on assignments).
Disciplinary adaptation: being able to see how AI is changing their future profession and being prepared to adapt (a sort of career-oriented futures thinking).
For faculty (teachers in this context), the competencies required are:
Understanding enough about AI to incorporate it into teaching and to guide students in its use responsibly.
Being open to shifting pedagogy (e.g., redesigning assignments to focus more on analysis than rote work, given AI can do rote).
Maintaining academic standards and integrity (e.g., designing assessments that still measure student learning fairly when AI is present, and teaching students how to use AI without plagiarizing).
OSU didn’t necessarily assume all faculty had these skills, so a big part of the initiative is faculty development.
Teacher vs Student Focus: OSU’s end goal is student competency (“AI fluent” graduates), but they recognized faculty need support to get there. They simultaneously launched fa
g and support programs. For instance, OSU’s teaching and learning center offered new workshops in mid-2025 on 
h AI” that hundreds of faculty attended. They also created an online repository where faculty share how they’ve used AI in assignments across different disciplines (to spark ideas, an example of building a cultivation community among teachers). OSU’s stance is that faculty (teachers) do not need to be AI experts per se, but must not ignore AI. They encourage faculty to experiment and learn alongside students in some cases, but always to remain the “guide” ensuring learning objectives are met. So it’s a somewhat collaborative approach: teachers are learners too here (practitioner side) but also designers of the learning experiences (cultivator side). Training Structure for Faculty: OSU mobilized resources like:
AI Toolkit for Faculty: The university created guides (similar to UK’s DfE guidance but at university context) summarizing what tools are available, how to detect issues, etc., and suggested syllabus statements about AI use. This lowered the barrier for faculty to start engaging.
Workshops and Webinars: In Summer 2025, OSU ran intensive institutes (multi-day workshops) for any
erested, covering generative AI basics, demonstration of tools, and collaborative redesign of one assignment or course with AI in mind. Many instructors from writing, computer science, and business attended (areas keenly affected).
Learning Community: OSU set up an “AI in Curriculum” community where faculty across departments meet monthly to share experiences during the first year of rollout. E.g., an English professor might share how they now have students use AI to generate essay outlines but still expect students to write the essay, and how that’s working.
Student Feedback Mechanisms: They also turned to students as partners – OSU’s student government held open forums with faculty about AI in courses, to surface student concerns and suggestions. This helped teachers understand student perspectives (for instance, students asked for clarity: which courses allow AI and how? leading faculty to be more explicit in syllabi).
Assessment and Early Impact: This began Fall 2025, so full impact data isn’t in. But some initial metrics:
Over 80% of first-year students in Fall 2025 engaged in at least one structured AI-related lear
 (this is tracked via the First Year Success Series events and the “Unlocking Generative AI” online module that many completed).
Faculty adoption: OSU reported that by Spring 2026, over 140 courses across the university had incorporated an AI component or assignment that wasn’t there before. That’s a big shift in a short time. For example, an Intro to Psychology course now had an assignment where students had to critique AI-generated patient case notes for accuracy of psychological concepts.
Academic integrity: OSU’s academic integrity office noted that after the AI Fluency initiative started, more students were proactively disclosing AI use in assignments (since many instructors permitted it with citation rather than banning it). This open atmosphere might have reduced misconduct cases. However, some faculty still caught misuse; OSU is developing more sophisticated honor code enforcement. The key is, they’re pushing culture toward transparency and proper use.
Student outcomes: Too early for comprehensive data like critical thinking improvements. But anecdotally, professors observed that class discussions improved when students coul
ther diverse viewpoints or examples beforehand – they came in more informed (e.g., a political science class debate where students had used ChatGPT to brainstorm arguments on both sides, which the professor said led to a richer debate and students also pointed out which arguments seemed weak or AI-ish, practicing critique).
On the flip side, some faculty reported that students in entry-level courses were initially over-relying on AI for homework, turning in cookie-cutter answers. But as faculty adjusted assignment design (like requiring personal reflection or in-class components), that diminished.
Critiques:
Some faculty (especially older or already overloaded ones) were resistant, seeing the AI Fluency requirement as extra work and not immediately relevant to their discipline (e.g., a classics professor might think “why do I need AI in my Latin course?”). OSU tried to handle this by not forcing AI into every course but providing suggestions where it could enhance learning (like using AI to translate Latin and then having students evaluate the translation – which ties to both language and AI literacy). Still, a critique is that a top-down initiative at a university must balance academic freedom. OSU’s approach was to strongly encourage and provide resources, but not literally force every faculty’s hand. That means some variation – some classes might still ignore AI, pote
 inconsistent student experience. Over time, laggards might come around as culture shifts, but it’s a process.
Another challenge is ensuring depth over breadth. Covering AI in the first year for everyone is great, but will they practice it in later years? OSU aims yes, by integration into majors. But it requires continuous support. If funding or leadership focus shifts, it could lose momentum.
However, OSU’s case is being watched by many other universities. It’s a trial of making AI literacy part of general higher ed outcomes. They may publish results on student competencies in a couple of years.
Lesson: OSU’s case illustrates that even at advanced education levels, there’s recognition that educators themselves need training and mindset shifts to incorporate AI usefully. I
ital literacy efforts in the 2000s but on turbo mode because AI is evolving fast and touching academic integrity directly. The participation rate in OSU’s faculty development suggests many instructors understand the change is necessary and want to adapt rather than fight it【45†L89-L97】. It reinforc
professors to become comfortable with AI (practitioners) plus guiding them in how to teach differently (cultivators) is key to institution-wide outcomes. Case Study 5: United Arab Emirates – “Afaq” Teacher AI Training and Nationwide AI Curriculum (2025, K–12)
Context & Program: The UAE, keen on AI adoption (they have a national AI strategy and even a Minister of AI), turned to education as part of it. In 2025, the UAE Ministry of Education launched ‘Afaq: Empowering Educators with AI Technologies’, a specialized training program that initially trained about 500 teachers across the UAE【43†L55-L63】. This coincided with the UAE’s rollout of a new AI curriculum in public schools (kindergarten to grade 12) starting the 2025–26 year【43†L79-L85】. Essentially, the UAE moved to become one of the first countries requiring AI learning at all school levels (similar to China’s move, but UAE is smaller). They recognized that teacher readiness would make or break it, hence the Afaq program. Competency Model: The UAE’s new curriculum and teacher training focus on:
Personalized and Innovative Content Design: Teachers learn to use AI to create personalized learning materials【43†L63-L70】 (e.g., using AI to generate differentiated exercises or to assist in lesson planning for diverse learners).
Fundamentals of AI for Teachers and Students: Afaq covers basics of AI 
and what they are teaching, such as how machine learning works conceptually). Students have grade-wise content (like simple robot
ames in primary, to coding and ethics in secondary). They aim for students to not just consume AI but build simple AI applications by high school (like programming
Mindset: AI as Support, Not Replacement: A core message in Afaq training was “AI should not be seen as a replacement for teachers, but as a supportive tool”, freeing teachers to focus on deeper learning tasks【43†L75-L83】. This directly aligns with our “keeping human above machine” competency – they explicitly emphasized teachers focusing on critical thinking, creativity, etc., while AI handles routine tasks【43†L77-L85】.
Critical Thinking & Ethics: The UAE’s program includes training teachers on guiding students in critical evaluation of AI outputs and ethical discussions. E.g., teachers were trained on how to discuss privacy or bias in age-appropriate ways. Students in middle/high school as part of the new curriculum have a module on AI ethics (the country even set up an “AI ethics youth competition” to stimulate this learning).
AI Integration in Assessment: Teachers are trained to use AI in assessment (like using AI analytics to identify students who need help) and to guard against cheating (a concern discussed was how to modify exams/homework in light of AI – Afaq included that conversation).
Teacher Training (Afaq) Structure: The Afaq program combined theory with practice:
It was delivered in collaboration with Emirates College for Advanced Education (ECAE). They ran an intensive course where teachers got lectures on AI concepts and hands-on labs (e.g., learning to use a particular educational AI platform to create a quiz).
It was fairly short-term (a few weeks part-time, or a one-week full-time equivalent), meant to jumpstart teacher capability. They ensured representation from all Emirates – about 500 teachers (math, science, computing, plus some language teachers) across UAE took it in first batch【43†L55-L63】.
A key part was practical projects: during training, teachers worked in teams to design an AI-infused lesson or activity for their class, applied it, and then came back to reflect. For instance, one group of teachers created a lesson where students used an AI image generator in art class and then critiqued art ethics – they implemented and reported outcomes.
Officials highlighted that Afaq prepared “a new generation of educators capable of leveraging emerging technologies to enhance curricula, assessment, and classroom interaction”【43†L63-L70】. They tried to cover not just teaching with AI, but also changes in assessment methods (the program talked about more project-based assessments, using AI for quick feedback, etc.).
Notably, training emphasized preserving human elements: a phrase in coverage was teachers focusing on “developing students’ critical thinking, creativity, and future skills” while AI handles menial tasks【43†L75-L83】, which Afaq reinforced through examples (like encouraging teachers to do Socratic seminars where AI provides background info but humans do the debating).
Assessment and Impact:
The UAE’s AI curriculum includes both quantitative and qualitative evaluation. They plan to measure student competency via project presentations (especially at secondary level – e.g., a grade 12 capstone project involving AI might be introduced) and monitor improvements in things like problem-solving in contexts involving AI.
Impact so far (the first y
ut just began 2025–26): The Ministry reported that teachers who underwent Afaq felt more prepared and in fact 500 more teachers are set to be trained in the next cohort (implying scale-up due to positive reception).
An immediate outcome is that 1000+ trained teachers are leading the AI curriculum rollout in schools【42†L29-L37】. Given each trained teacher can mentor colleagues, presumably all ~280 public schools are covered by at least a few trained educators.
Teachers from Afaq emphasized that it changed their perspective from seeing AI as “extra work” to seeing it as a tool to “focus on meaningful learning”【43†L73-L81】. That mindset shift is important but somewhat intangible evidence; it was captured in interviews and testimonies.
The student response will become clearer after
year. The UAE expects that by the end of 2026, they’ll have data (perhaps internal tests or international benchmarking like PISA 20
has a creative thinking assessment, to see if these future skills improved relative to past).
Critiques:
Implementation in UAE can vary between well-resourced Abu Dhabi/Dubai schools and others. If some teachers didn’t get training, they might struggle with the new curriculum. But UAE tends to invest heavily, so likely they’ll train all required teachers.
Another consideration is contextualization: The UAE’s cultural context might influence how openly certain AI ethics or critical perspectives are discussed (e.g., the focus would be on safe/responsible use aligned with societal norms, possibly avoiding political angles). But basic critical thinking and creativity can still be fostered within those bounds.
A unique challenge is language: Arabic is the primary language of instruction in many subjects in lower grades, and AI tools historically were weaker in Arabic. The Afaq program included training on Arabic AI tools (like Arabic chatbots) to ensure teachers have resources in the mother tongue. Teachers had to adapt tools or use translation for some AI applications. This highlights that teacher competence includes finding workarounds when technology is not equally developed in all languages – an adaptability and problem-solving aspect of “future-oriented thinking.”
The general lesson is that a structured national program linking teacher training to curriculum implementation can rapidly build capacity. The UAE did within a year what some might take many to do (though small size helps). It underscores our theme that neither teacher development nor student curriculum alone is enough – they launched both together.
Case Study 6: Global Industry – PwC’s AI Literacy Upskilling (2023–2024, Professional Training)
Context & Program: While not a school system, PwC’s initiative to train its entire workforce in AI literacy mirrors many issues faced in educator training. In 2023, PricewaterhouseCoopers (a global consulting firm) committed $1B to AI for its business, including a substantial portion for employee upskilling【47†L23-L31】. By 2024, they rolled out an internal AI Literacy 
~65,000 employees in the US (and globally to 200k+). This included mandatory training modules on AI basics and responsible use, optional deep-dive courses for interested staff, and “prompting parties” – informal workshops to practice prompt engineering【47†L39-L47】. The goal was to ensure employees at all levels could leverage AI tools (like OpenAI’s ChatGPT, which PwC licensed for internal use) while adhering to ethical guidelines. Competency Model: PwC identified competencies for employees (which one can analogize to teachers):
AI Knowledge & Skills: Knowing what AI can do in their role and how to use enterprise AI tools (similar to teachers knowing how to use education AI tools).
Oversight & Critical Thinking: Employees are taught never to blindly trust AI outputs – e.g., PwC’s training said always verify AI-generated analyses【30†L72-L80】. This is akin to teaching critical thinking toward AI output.
Ethics & Compliance: They emphasize not feeding confidential client data into public AI (privacy)【30†L78-L86】, avoiding bias in using AI for HR or decisions, and transparency with clients about AI use. That parallels digital citizenship and academic honesty in schools.
Continuous Learning: AI changes work processes, so employees must be ready to learn new tools – this adaptability is akin to what we want teachers to have in continuously updating pedagogy.
In training sessions, they often drew the line that AI frees up time for more human tasks (like analysis, relationship-building)【43†L77-L85】 – a narrative identical to what we propose for teachers focusing on human elements. Teacher vs Student Focus: In this analogy, employees are learners; the “teachers” are the training and team leaders guiding them. PwC’s approach heavily invested in training the people (since here the end-users and the “teachers” are the same – each employee had to self-learn via training). The program also created internal “AI Champions” – staff who became advanced in AI and help coach others, similar to lead teachers in a district who mentor colleagues. The interesting link to our discussion: PwC’s CTO said businesses have an obligation to equip employees with AI skills responsibly【47†L13-L21】, reflecting a viewpoint that training in the human competencies (oversight, questioning AI, ethical use) is crucial to actually realizing AI benefits. This aligns with our stance that if teachers (or any professionals) just use AI without these competencies, it could do harm or at least not help as intended. Training Structure & Impact:
 AI Literacy course** to all employees, which 80%+ completed within months (per an internal stat, referenced in Forbes 2024). That’s a near-universal uptake
g scaling is possible quickly.
They followed up with specialized training by role. For example, auditors learned how to use AI to scan documents but also how to verify its flags; consultants learned how to integrate AI in data analysis while applying skepticism.
They tracked performance: one outcome reported was a *productivity increase
t adopted AI workflows – some internal tasks done 30-50% faster (from PwC case studies). While not directly an education outcome, it shows that training employees to use AI effectively (with oversight) yielded m
uctivity gains【47†L23-L31】【47†L25-L33】. Translating to education, presuma
eachers to leverage AI for planning and differentiation could free up time for richer student engagement, an outcome we desire (though 
et in schools).
Importantly, PwC’s effort included culture change: they hosted discussions on AI ethics, had leaders model using AI with proper caution, and set clear guidelines (like “Always have a human review outputs before delivering to client” – analogous to “teacher always reviews AI content before giving to 
thin a year, AI became normalized in their workflow, but with an understanding that employees are accountable for AI-assisted work【30†L81-L89】.
Lessons for Education:
Rapid, comp
skilling is possible with leadership buy-in, resource allocation, and a mix of mandatory and encouraged training – a parallel to needing top-down support (like ministries or district leadership) to train all teachers.
Emphasizing human responsibility and critical oversight was key in corporate setting to avoid errors – exactly why we argue teachers and students must keep human judgment at the forefront【30†L78-L86】【30†L87-L95】.
PwC also did “prompting parties” – a fun, c
d skill. In schools, one could analogize this to teacher hackathons or student AI contests to practice skills in a low-stakes environment.
A potential difference: corporate settings might have more immediate pressure (profit/client service) to adopt AI, whereas education’s “outcomes” are longer-term and sometimes intangible. Nonetheless, seeing an organization successfully raise the competency of tens of th
eople in critical AI use in a year suggests it’s feasible in education with motivation and structure.
Comparison of Case Studies: Despite different contexts, common threads include the need for te
ng accompanying any student AI curriculum (Israel, Singapore, China, UAE, OSU all did teacher PD as a key component). All emphasize ethics and critical thinking, not just technical use (even in corpor
, ethics was central). There are contrasting approaches: Finland/Singapore integrated gradually and voluntarily (bottom-up elements), while China/UAE mandated quickly (top-down). Both approaches had successes and challenges: voluntary needs teacher buy-i
turally), mandates need heavy support to work (which China provided at scale, UAE in progress). This suggests each system must calibrate based on its culture: b
is saying “just change student curriculum and not worry about teachers” – everyone realizes teacher competency is vital. These cases support that teacher as practitioner and cultivator are both essential roles in AI education:
In Finland and Singapore, teachers be
rners and experimenters (practitioners of new tech) and then facilitators for students.
In China and UAE, there was a push to first educate teachers (practitioners) so they could properly teach students (cultivators).
OSU explicitly upskilled professors and redesigned pedagogy simultaneously.
Crucially, non
 programs assumed teachers could be bypassed (e.g., using AI tutors instead of teachers). On the contrary, they double-downed on teacher development, reaffirming that in an AI world, the teacher’s role is critical but evolving. Now that we’ve seen how frameworks and cases address teacher vs student competency development, we proceed to identify overarching programs and frameworks tackling this dilemma at a broader scale, before proposing our practical model and recommendat
rogram Landscape: Who Already Addresses This Dilemma? A number of educational frameworks, standards, and programs have emerged in recent years to define competencies for the AI era and to guide both teacher development and student learning. We examine several major ones and how they t
ance of teacher competence and facilitation. These include international frameworks (UNESCO
ofessional standards (ISTE), national strategies (e.g., UK, EU, and others as represented through guidelines), and specialized programs (like AI literacy frameworks by AI-focused organizations). Table 2 below compares key features of these frameworks/programs:
Does it emphasize teacher possessing competencies, facilitating them in students, or both?
Does it specify minimum teacher AI literacy or skills?
How does it incorporate ethics, bias, epistemic humility (critical view of AI)?
Table 2. Comparison of Frameworks/Programs on Teacher vs Facilitation and Ethical Focus | Framework/Program | Scope & Nature | Teacher Competence vs Facilitation Stance | Teacher AI Literacy Requirements | Approach 
c Values |
|--------------------------------------------------------|----------------------------------------------------------------------------------------|-----------------
---------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------
----------------|
| UNESCO’s AI Competency Framework for Teachers (2024)【20†L423-L431】【20†L429-L437】 | Global framework defining 15 competencies across 5 domains (Human-centric mindset; AI ethics; AI knowledge; AI pedagogy; AI for professio
 policy-makers and training programs. | Strong on teacher competence: UNESCO explicitly says teachers must master knowledge, skills, values in the age of AI【20†L419-L427】 before they can effectively teach students. It emphasizes teachers as change agents who need these competencies
20†L415-L423】. Also includes a domain on AI pedagogy, implying teachers must also be skilled in facilita
 use【20†L429-L437】. Overall: teachers should both have and teach AI-related competencies. | Framework outlines specific teacher competencies: e.g., understands basic AI concepts and applications; is able to use AI tools in teaching effectively; can critically evaluate AI outputs; ensures ethical use of AI in classroom【20
20†L429-L437】. It defines 3 proficiency levels (Acquisition, Deepening, Transformation) for each competency【20†L433-L441】, suggesting a progression. So UNESCO
least baseline “acquisition” for all teachers (like knowing common AI tools a
nd encourages moving upwards. | Ethics is one of five core domains【20†L429-L437】 – teachers are expected to know AI ethics principles (bias, transparency, privacy) and model ethical use. The “Human-centric mindset” domain【20†L429-L437】 stresses that teachers maintain a focus on fairness, inclusion, sustainability – essentially keeping the human/values above technology. UNESCO repeatedly notes the need for teachers to emphasize that AI is a tool under human control, not an authority【39†L37-L45】【20†L425-L433】. |
| UNESCO’s AI Competency Framework for Students (2024)【48†L58-L67】【48†L79-L87】 | Companion global framework for student AI competencies (knowledge, skills, attitudes needed by primary/secondary students). Will inform curricula and PISA assessment. | Focuses on student outcomes, implicitly requiring teachers to cultivate them. It assumes teachers/education systems will integrate these competencies across subjects【48†L65-L73】. It does not explicitly address teacher skills (that’s in teacher framework), but the expectation is teachers will guide students to reach these outcomes, which presumes teacher capability in each area. | No direct teacher requirements here (that’s in teacher framework). However, for each student competency (e.g., “students can use AI to solve problems and are aware of its limitations”【48†L79-L8
s would need corresponding knowledge to teach it. UNESCO recommends teacher training align with student framework【22†L7-L15】 – i.e., teachers should be trained in the same domains (which the teacher framework covers). | Student framework heavily features ethics: e.g., “students understand potential biases and ethical dilemmas of AI”【48†L79-L87】. It integrates critical thinking and ethical reflection in examples across subjects (history classes discussing validity of sources and AI’s impact on society【48†L71-L78】, etc.). Human agency is emphasized: AI literacy includes knowing where human judgment is needed【48†L79-L87】. So to implement, teachers must foster open discussions on ethics and ensure students remain critical and values-driven in using AI. |
| ISTE Standards (Educators & Students) (2017, guidance 2023)【49†L1-L8】【49†L21-L2
tional Society for Technology in Education (ISTE) standards define skil
s and educators in using technology. Not AI-specific originally, but ISTE released AI in Education guidance in 2023 to contextualize standards for AI. | Educator Standards (ISTE 2017) emphasize educators as Learners 
 improving their tech skills) and Facilitators of student empowered learning【49†L1-L8】. So teachers must both develop their competencies and use them to guide students. ISTE’s 2023 AI addendum encourages teachers to experiment with AI (educator as practitioner) and to design learning experiences using AI (educator as cultivator)【49†L9-L17】【49†L21-L29】. It consistently frames teachers as models of digital age learning and as guides. | ISTE doesn’t list specific AI literacy items in 2017 standards, but in 2023 they provided resources: e.g., “Why AI skills matter for futu
 points out new teachers should know AI basics and its implications【49†
e ISTE Educator standard of being a Citizen includes mentoring students in ethical tech use – now interpreted to include AI. Many teacher prep programs use ISTE standards, which now implicitly require familiari
meet “Designer” or “Analyst” standards (using data and tools). ISTE is working on micro-credentials for AI in education that effectively set informal requirements (like a badge in AI Ethics for Educators demonstrating they’ve learned it). | ISTE Standards strongly feature Digital Citizenship: educators are to inspire students to act ethically in digital contexts, and students are to practice safe, legal, ethical use【49†L17-L25】. In ISTE’s AI Education guides (2023), each unit pairs tool use with ethics discussions (they have a guide, for example, on teaching students about AI bias by using an AI image generator and analyzing the outputs). ISTE’s stance is that ethics and equity must be embedded in AI learning – e.g., encourage teachers to discuss data bias when students use AI【49†L9-L17】. It also encourages a “human-centered approach” in comme
CEO: teachers should emphasize what makes us human – creativity, judgment – when using AI). |
| OECD Education 2030 & AI Literacy Initiatives【24†L3903-L3911】【48†L58-L67】 | OECD’s Education 2030 framework focuses on broader competencies (including digital). In 2025, OECD with EC launched an AI Literacy project (framework & upcoming PISA AI literacy assessment)【48†
ight (2025)** on AI in curricula argues teachers need to look beyond incremental 
rethink competencies due to AI【24†L3905-L3913】. It implicitly calls for shifts in both what students learn and how teachers teach. OECD d
ds, but in commentary it notes that teachers must be supported to deliver AI literacy【23†L11-L19】. So it leans toward requiring teacher facilitation of new competencies, and by extension, teacher upskilling. | OECD doesn’t mandate teacher skills, but through its reports (like “What should teachers teach... in AI future”【24†L3903-L3911】) it influences policy: it e
uggests education systems update teacher training to 
eracy and ethics so teachers can implement new curricula. One related piece: OECD’s TALIS survey (2024) added questions about teacher training needs in AI, highlighting it as expected competency area. | OECD’s AI literacy framework (draft 2025) explicitly includes ethical use as a student competency【48†L79-L87】, and notes that educators need to infuse that in learning experiences. OECD’s broader emphasis on “student agency, curiosity, resilience” aligns with ke
 Its commentary (Education 2030 position paper) stresses that with AI automating tasks, education should focus more on what machines can’t do: creativity, ethical judgment, collaboration. Thus, it implicitly pushes teachers to cultivate those human traits (and value them). |
| UK Department for Education – Generative AI Guidance (2023)【56†L33-L41】【56†L75-L83】 | The UK DfE in 2023 issued non-st
ools on using AI. It covers opportunities and risks, advising schools and colleges. | The DfE guidance explicitly addresses teachers: “Yes, teachers can use AI to help with X, Y... but must use judgment”【56†L33-L41】. It encourages teacher adoption for efficiency (pla
ack) while cautioning oversight【56†L35-L39】. It doesn’t directly say how to teach students about AI, but implies teachers will manage AI use in classroom (facilitation role). It leaves student AI use policy to school decisions, but hints that with safeguards it can be allowed【56†L83-L90】. So stance: teachers should both develop skill in using AI for teaching and supervise student use ethically. | No formal requirement for teacher AI literacy in policy yet, but by giving official guidance, UK essentially expects teachers to become competent. The guidance itself explains generative AI basics (in accessible language
ing. The DfE also launched some training materials concurrently (an online module on AI for school leaders). Some schools interpreted this guidance as green light to do CPD on AI for staff. So while not mandated, there’s an expectation teachers learn key concepts (accuracy issues, data laws) to follow the guidance【56†L33-L41】【56†L75-L83】. | The UK guidance repeatedly stresses safety, accuracy, and fairness: e.g., warns AI content may be incorrect or biased, so teachers must verify【56†L35-L39】. It says AI must not replace “the unique role” of teachers (professional judgment, personal relationships)【56†L69-L77】, underscoring human-centric value. It also references data protection and IP (ethics/legal). So it’s very aligned with ethical and human-in-loop use. Essentially, the guidance’s thrust is: use AI to reduce workload but always keep a human (teacher) in control and maintain quality【56†L35-L43】【56†L69-L77】. |
| EU Digital Education Action Plan & DigComp 2.2 (2022) | The EU’s DigComp (Digital Competence) frameworks for citizens and educators are being updated to include AI. The 2021-27 Action Plan calls for AI and data ethics education. | DigCompEdu (2017) for educators had general digital competencies. An update is likely to mention AI. The action plan explicitly says “educators should be trained in AI basics and ethics” – so the stance is educators need to possess AI competences to foster them in learners. Some EU-funded projects are developing teacher training for AI (e.g., AI4T project). So EU is moving toward embedding teacher AI competence as part of overall digital competence. | While not formalized yet, DigComp 2.2 (for citizens) added AI-related examples (like distinguishing AI-created content). DigCompEdu will likely require teachers to do things like “Use AI-driven tools to support teaching” and “critically evaluate outputs of educational AI”. The EC has funded MOOCs for teachers on AI – not mandatory but signaling expectation teachers acquire those skills. | EU policy heavily emphasizes “AI ethics and awareness” for all citizens including students, thus teachers must impart those – requiring their own understanding. The Action Plan speaks about “AI as a support for humans” and need to address bias, echoing human-centered AI values. They promote “human-in-command” approach in education (from EU guidelines on AI). Thus, EU programs advise teachers to highlight ethical issues and ensure AI use aligns with European values (human dignity, etc.). | Analysis: Across these frameworks and programmatic efforts:
There is a clear trend toward expecting teachers to have some level of AI understanding and skills. UNESCO’s framework is the clearest articulation (with specific competencies)【20†L423-L431】, effectively a blueprint for teacher training globally. ISTE and OECD don’t mandate but strongly encourage and provide guidance which many national systems adopt or adapt. The UK’s quick guidance in 2023 also acknowledges that teacher permission and skill are pivotal to harnessing AI (by saying “Yes teachers can use it, here’s how carefully”【
.
Teacher as practitioner vs cultivator: Most frameworks assume teachers will do both. ISTE’s mention of teachers as **Lea
mproving their own tech use) and Facilitators/Designers for students shows both rol
. UNESCO too explicitly addresses teacher competencies and their role in student competencies【20†L419-L427】【48†L65-L73】. The message: a teacher in AI era must learn themselves and teach others.
Minimum AI Literacy for Teachers: While not always concretely specified in older standa
 UNESCO’s and emerging national guidelines are now listing these (e.g., understand AI bias, be able to use an AI tool in class, etc.
ng years, many countries will incorporate these into teacher standards or certification (some have already started doing so).
Ethics and human-centric focus: All frameworks put heavy weight on ethics/bias and maintaining human oversight. UNESCO dedicates a domain to Ethics of AI【20†L42
E stresses digital citizenship (which covers ethical tech use)【49†L17-L25】; national policies (UK, EU) highlight keeping teachers/humans central and using AI responsibly【56†L69-L77】【52†L313-L322】. This consistency shows up in case studies too (every program above had ethics as part of training). This aligns with our highlighted competencies and indicates any future teacher or student competency model must put ethics and human agency at its core.
In conclusion, the program landscape is converging on the idea that:
Teachers need to be competent AI users and critics themselves (practitioners).
Teachers need to be equipped to guide students in developing these competencies (cultivators).
Ethical, critical, and human-focused use of AI is non-negotiable across these models – it's a common thread, meaning any competency framework without these would be incomplete.
These established frameworks and early adoption programs reinforce the feasibility and necessity of the integrated approach we propose. They provide a foundation upon which we now build our practical model and recommendations, tailoring it to evidence and be
g open questions.
A Practical Model: What “Good” Looks Like
Drawing on the evidence and examples above, we propose a comprehensive model for effectively integrating AI into education through teacher and student competency development. This model has three components: (1) a Teacher Competency Ladder that outlines progressive levels of teacher capability in an AI-saturated environment,
nt Competency Outcomes Framework** that defines what students should know and be able to do (the “human OS” skills in concrete terms at different education stages), and (3) a recommended Teacher Training and Support approach to achieve and sustain these competencies, including safeguards against pitfalls (e.g., over-reliance on AI, inequity).
1. Teacher Competency Ladder
We envision a ladder (or continuum) of teacher competencies with respect to AI integration and “human OS” skills, with each level building
 set minimum expectations (baseline every teacher should meet) and pathways for advanced mastery (for lead educators). The levels:
Baseline (AI-Aware Teacher) – Every teacher attains fundamental AI literacy and ethical understanding.
Proficient (AI-Integrative Teacher) – Teachers effectively integrate AI into teaching and cultivate student competencies regularly.
Advanced (AI-Leader / Mentor) – Teachers innovate with AI in pedagogy, mentor peers, and contribute to institutional strategies on AI.
Table 3. Teacher Competency Ladder for AI-Saturated Education
Level & Title	Description of Capabilities	Example Skills & Behaviors	Impact on Student Learning
Baseline – “AI-Aware Teacher”
(All teachers by Year X)	The teacher has foundational awareness of AI tools and their potential and pitfalls. They can use basic AI applications for personal productivity and simple classroom tasks. They understand key issues (hallucinations, bias, copyright) and follow guidelines for ethical use. They are beginning to model a cautious, critical attitude toward AI. This is the minimum standard for all educators – analogous to basic ICT literacy but for AI.	– Uses common AI tools (e.g., ChatGPT or a subject-specific AI app) at least in planning: for lesson ideas, generating quiz questions, etc., and then verifies the output【56†L35-L39】. For instance, an AI-aware math teacher might ask ChatGPT for a step-by-step solution to a problem and check it for errors before class.
– Recognizes when AI output is incorrect or biased: e.g., if an AI summary of a historical event omits key perspectives, the teacher notices and corrects it【35†L205-L213】.
– Adheres to ethical guidelines: doesn’t input student private data into AI (privacy awareness)【30†L78-L86】; if AI helped create a worksheet, notes it or at least ensures it’s pedagogically sound (authorship transparency or at least personal vetting)【30†L81-L89】.
– In classroom, does not shy away from AI discussions: if a student references using AI, the teacher can have a basic conversation about it rather than shutting it down. May even devote a brief session to class ground rules on AI use, using provided school policy.	– Students see an example of critical AI use: e.g., teacher occasionally shows an AI output on projector and points out a mistake (“See, the AI got this grammar wrong—let’s fix it”) – this normalizes skepticism and revision【35†L179-L187】.
– The teacher can guide students away from misuse at a basic level: e.g., if students try to hand in AI-generated work, the teacher can likely detect overly polished or generic responses (with tools or own judgment) and address it constructively (teaching about integrity)【33†L95-L103】.
– Class begins to discuss AI’s role: though the teacher might not incorporate AI heavily yet, they don’t ignore its existence. This can manifest in setting a class policy (say, “You may use AI for brainstorming ideas, but you must write the essay yourself and cite any AI contributions” – baseline enforcement of ethics)【33†L111-L119】. Students thus learn boundaries and responsible attitudes early.
– Little explicit instruction on AI literacy yet (that comes at proficient level), but the groundwork is laid: students see that their teacher is aware and not blindly trusting or banning without reason – fostering trust and a model of measured use【52†L311-L319】.
Proficient – “AI-Integrative Teacher”
(Target for most teachers within 2-3 years)	The teacher confidently and routinely integrates AI to enhance teaching and learning while explicitly cultivating student competencies. They have deeper AI knowledge (beyond basics) relevant to their subject and pedagogy. They design activities where students use AI as part of learning tasks, and they coach students in inquiry, critical evaluation, creative use, and ethical considerations. They effectively balance AI assistance with human instruction.	– Embeds AI into lessons purposefully: e.g., an English teacher has students use an AI to generate a draft paragraph, then critique and improve it, thus teaching writing and critical editing【35†L176-L185】. A science teacher might use an AI simulation and have students interpret results, discussing any odd outputs as a class (teaching validation).
– Models thinking processes regularly: e.g., teacher “thinks aloud” while checking an AI-provided fact against a textbook or while constructing a prompt to get better output【50†L173-L181】. Students see how an expert approaches AI inquiries (trial and error, refinement, fact-checking).
– Facilitates student use of AI in a controlled, learning-focused way: e.g., in a project, allows students to use AI for research but requires them to list how they verified each AI-provided piece of info (scaffolded critical thinking)【33†L105-L113】. Or encourages students to use text-to-speech AI to practice a language and then reflect on its feedback vs teacher feedback (thus blending AI and human coaching).
– Continuously updates personal practice: follows new developments (maybe part of a teacher AI community), tries new education AI tools, and evaluates them for class. At this level, a teacher might say, “I heard about this new AI tool for chemistry – I’ll try it and see if it improves lab reports,” showing adaptability.	– Students actively practice human OS competencies: The teacher’s assignments demand it. For example, a social studies debate assignment requ
AI-generated evidence – students thereby hone research and questioning skills.【35†L179-L187】. Over time, these skills become habitual 
ys or observations might show increased source-checking behavior, more critical questions asked in class).
– Student outcomes improve in areas like critical thinking and creativity as measured by authentic assessments. E.g., a teacher notes that after a semester of using AI for brainstorming followed by human refinement, student writing is richer and more original (because they learned to iterate on basic ideas). While hard to quantify generally, specific indicators (like project quality, student self-assessments, or targeted tests of critical thinking) show gains, consistent with meta-analysis findings when AI is properly scaffolded【12†L73-L80】.
– Students gain AI literacy: They leave the class not only with subject knowledge but knowing how to use and question AI in that subject. For instance, a math student can use an AI graphing tool but also identify when the graph seems off. A student says, “We use ChatGPT but we don’t trust it blindly – we learned how to double-check everything,” demonstrating educational impact in attitudes【33†L95-L103】.
– Classroom trust remains high or even improves: the teacher involves students in setting AI use guidelines and discussing their experiences, so students feel respected and guided rather than policed【52†L311-L319】. Students also trust teacher’s expertise because the teacher demonstrated added value (helping them do what AI alone couldn’t – e.g., deep feedback, nuanced discussion).
Advanced – “AI Leader/Innovator Teacher”
(Some teachers, resource roles)	The teacher is an expert in pedagogical innovation with AI and a mentor to others. They deeply understand AI (even technically) and experiment with novel ways to use it to amplify learning. They might lead professional development or contribute to policy. They keep the humanistic and ethical perspective at the forefront and help shape how the school/district uses AI.	– Innovates new practices: e.g., designs cross-curricular projects involving AI (perhaps a school-wide “AI & Society” project week) or creates an elective on AI literacy for students. They might co-develop tools – e.g., collaborate with developers to pilot an education AI app, providing feedback to make it classroom-friendly (thus driving innovation).
– Mentors peers: runs workshops for fellow teachers, shares successful lesson plans, coaches colleagues in troubleshooting AI integration. They might have a formal role like “Tech Integration Specialist” or “AI Coach.” For example, an advanced teacher might observe a colleague’s class and suggest how they could use AI for differentiation, then help implement it.
– Contributes to curriculum/policy: sits on a district committee to draft AI usage guidelines or updates to curriculum standards to include AI competencies. Their classroom successes provide exemplars cited in policy documents (like a district PD newsletter featuring their class project as a model).
– Continues self-development: possibly takes advanced courses (some might even pursue a degree or certification in AI in education). They stay abreast of the latest AI research in learning sciences and test those findings in practice.	– School/District improvement: This teacher helps raise overall teacher competence by mentoring, leading to more classes reaching the “proficient” stage. As a result, more students across the school acquire AI literacy and OS skills, reducing variability. Essentially, they propagate best practices, leading to systemic change.
– Student empowerment and innovation: Students in advanced teachers’ classes often produce remarkable work – e.g., building a simple AI app to solve a local problem or conducting a sophisticated analysis with AI assistance and human insight. These students might enter and win competitions or become ambassadors of AI literacy themselves (e.g., leading a club to teach other students). The environment such a teacher creates fosters strong agency – students see what’s possible when using AI critically and creatively, encouraging them to continue those practices beyond the class.
– Influence on curriculum: The projects and outcomes from an advanced teacher’s class may inform curriculum revisions. For instance, if their students’ success in a certain AI-enhanced inquiry project is notable, the district might adopt that project structure for all schools.
dvanced practitioner’s experience benefits students at scale beyond their own classroom.
– Long-term outcomes: Students taught by such teachers not only excel in school but carry forward competencies to higher education or the workplace – they become the ones comfortable and ethical with AI, potentially outpacing peers. Though hard to measure immediately, these teachers likely contribute to producing future-ready graduates who become leaders in leveraging AI responsibly (anecdotal evidence: universities or employers might note that students from X school handle AI tools more skillfully – credit to these teachers’ influence).
This ladder can guide professional development planning: e.g., ensuring all teachers reach Baseline via mandatory training and resources; encouraging most to achieve Proficient via incentives, coaching, PLCs; and identifying/nurturing Advanced teacher-leaders who can drive innovation. It’s important the ladder isn’t seen as hierarchical “good vs bad teachers” but as stages of growth – with support to climb up. Also note that context matters: a new teacher might need a year or two to move from baseline to proficient as they get comfortable with general teaching plus AI. Advanced might remain a smaller subset who have passion or extra training, and that’s fine as long as they are utilized to mentor others (so their expertise disseminates).
2. Student Competency Outcomes Model
For students, we articulate what "good" looks like in terms of the human OS competencies at different educational levels. This model is essentially the student side of the coin: by the time they finish schooling (or each stage: primary, secondary), what competencies should they have developed? Key Student Competencies (Human OS in practice):
Inquiry & Prompting Skills: Students can formulate clear, purposeful questions (to ask humans or AI) and refine them. E.g., a middle schooler can take a broad research question and break it into specific questions; a high schooler can craft an effective prompt to get a useful answer from an AI and then re-prompt if needed【48†L71-L78】.
Critical Evaluation of Information (AI outputs included): Students habitually check the reliability of information. By end of secondary, a student given an AI-generated output will automatically approach it with questions like "What’s the source? Does this align with what I know? Could it be wrong?" and seek verification【52†L279-L287】. Younger students might demonstrate this by, say, cross-checking an answer with the textbook or noticing if an AI answer doesn’t make sense and asking the teacher.
Ethical & Responsible Use: Students understand the ethical guidelines for AI use and apply them. For instance, students do not present AI work as their own (and can explain why that’s wrong – issues of honesty and learning)【33†L105-L113】. They respect privacy (e.g., not entering personal data into random apps) and show awareness of fairness (e.g., if they notice an AI image generator perpetuates a stereotype, they can identify and discuss it). By graduation, students should have engaged in discussions about AI’s societal impact and articulate an informed stance on an AI ethics issue【48†L79-L87】.
Adaptability & Lifelong Learning Orientation: Students become self-directed learners comfortable with new tools. For example, if a new AI study tool emerges in their first year of college, a graduate from our program would approach learning it with confidence and caution – experiment with it, see how it can help, but also evaluate its trustworthiness. In school, this is seen when students easily pick up updates to software or creatively solve problems when tools change (not getting thrown off by a changed interface or new requirement). They also reflect on their learning process – e.g., a student might say "I used an AI hint for this problem, but I realized I rely too much on it, so next time I solved without it first, then checked" – showing metacognitive awareness (a result of teacher cultivation).
Creativity and Human-Centered Skills: Students use AI as a tool to amplify their creativity and productivity, not as a crutch to replace their thinking. For instance, an art student might use an AI to generate many design ideas, then use their own artistic judgment to refine or combine them into something novel – thereby demonstrating creative decision-making beyond the AI’s capability. Students also exhibit teamwork and empathy even with AI – e.g., when doing collaborative projects that involve AI, they focus on human collaboration (dividing tasks, sharing insights) rather than each just offloading to their own AI and not talking. Essentially, students value and leverage human elements: curiosity (they pursue questions AI raises rather than just take answer and move on), empathy (they consider how AI might affect others, like understanding biases against marginalized groups and caring about it), accountability (they own the outputs – if they used AI and it was wrong, they accept responsibility to correct it).
These competencies should be scaffolded by age:
Primary (Ages ~5-11): Focus on foundational inquiry (“ask questions about the world”), basic digita
owing e.g. not everything on the internet is true – early critical thinking), simple ethics (don’t copy others’ work, treat devices responsibly). AI might be introduced in simple forms (like a class virtual assistant answering questions) to spark curiosity and teach t
e wrong (“Why did Alexa not understand you? How can we ask differently?” – teaching prompting and that machines aren’t perfect). Emphasis on human creativity (lots of play, making things without relying on tech, to cement that value).
Lower Secondary (Ages ~12-14): Students begin to use AI tools with guidance. They learn to refine search queries and prompts, identify obvious AI mistakes, and consider issues like privacy (perhaps via scenarios in computing class). They do projects like “use an AI to analyze the sentiment of news headlines – do you think it did a good job? Where might it be wrong?” – integrating subject knowledge and critical AI assessment. They practice citing AI as a source if used, normalized by teachers. The ethical focus might include respectful online behavior and recognizing deepfakes or misinformation (media literacy).
Upper Secondary (Ages ~15-18): Students handle more sophisticated tasks: using AI coding assistants in computer science, AI writing assistants in English – but always with reflective components (like turn in first draft from AI with your edits highlighted and a commentary on what AI got wrong or what you improved). They might delve into algorithmic bias in social studies or math class (using statistics knowledge to understand bias). They should graduate being able to articulate how AI works at a conceptual level (e.g., “AI like ChatGPT predicts text based on training data, which means it might reflect biases in that data”), and more importantly, demonstrate they can get value from AI without sacrificing originality or integrity.
How Teacher Ladder and Student Competencies Align:
A Baseline teacher can get students to at least be aware (primary: teacher doesn’t spread misinformation, so kids learn not to believe everything; secondary: teacher enforces rules so students know obvious lines not to cross).
Proficient teachers explicitly teach and practice these student competencies in class activities as described, thus students actually acquire them to a strong extent.
Advanced teachers might extend student competencies beyond the standard curriculum (like involving them in the creation or evaluation of AI systems, preparing them as true AI literate citizens who could even lead their peers – for instance, an advanced teacher’s students might run an “AI literacy for grandparents” workshop in the community, demonstrating high mastery and social application).
In “what good looks like,” by graduation, a student from such an environment would stand out in college or the workplace as someone who:
Asks good questions and doesn’t just accept answers (whether from Google, GPT, or a person).
Uses AI tools to be efficient but always checks and improves on them, thereby producing high-quality work ethically【33†L95-L103】【52†L277-L285】.
Is not afraid of new tech; in fact, can quickly learn new AI tools and apply them thoughtfully (a lifelong learning trait).
Upholds ethical standards and can articulate reasoning behind it (e.g., not plagiarizing because they value learning process and respect intellectual property).
Remains creative and humanistic: they bring personal voice and critical thought to tasks, instead of delivering formulaic AI-generated content.
Essentially, “good” looks like students who treat AI as a powe
and object of critique, not as an infallible oracle or a crutch to avoid thinking. They are augmented by AI, not dependent on it. And “good” looks like teachers who exemplify and nurture that behavior daily.
3. Recommended Teacher Training and Support
Achieving this vision requires systematic teacher professional development and supportive policies. Here’s a practical multi-faceted approach:
Initial Training (Baseline upskilling): Implement a national or district-wide PD module on “AI Essentials for Educators” – ideally mandatory for all within the next 1-2 years. This could be a short online course plus in-person workshop focusing on: how generative AI works in simple terms; hands-on practice with a few AI tools; common pitfalls (hallucinations, bias)【35†L205-L213】; ethical guidelines (with specific do’s/don’ts)【56†L33-L41】【30†L78-L86】; and very importantly, examples of how teachers can use it to save time (to motivate them). This addresses baseline knowledge and alleviates fear. Include a reflection where teachers consider how they might use or not use AI in their current practice – beginning the mindset shift.
Ongoing Communities of Practice: Establish PLCs focused on AI integration. For example, a monthly “AI in the Classroom” roundtable for teachers at each school or across a district. Teachers share successes, failures, and new ideas. Perhaps an online forum as well. This leverages teacher expertise and fosters bottom
Proficient teachers will emerge and how they learn from each other). Administrative support: give some PD hours or credit for participating, so it’s valued.
Coaching and Mentoring: Train and designate some tech coaches or lead teachers as “AI Mentors.” These individuals (Advanced-level on our ladder) can offer 1-1 support: observe a lesson and help incorporate AI, co-plan an AI-enhanced project, or help troubleshoot issues like cheating incidents by brainstorming assignment redesign. Essentially, bring Proficient teachers up to Advanced by mentorship, and Baseline to Proficient by coaching. If budgets allow, have at least one per school or one roaming across small schools. If not, leverage experienced teachers who attended extra training as informal go-tos (maybe with a small stipend or recognition).
Integration into Teacher Education: Ensure that pre-service teacher programs include AI-related training so new teachers enter at baseline or even proficient. For instance, a me
have an assignment for teacher candidates: design a lesson using an AI tool and an evaluation plan for its effectiveness【48†L65-L73】. Universities can simulate classroom scenarios with AI (some are creating virtual classroom sims where AI plays student roles – that could be used to let teacher candidates practice responding to, say, a student presenting AI-found info). Aligning new teacher training with our ladder means future cohorts won’t need as much catch-up PD.
Policy and Incentives:
Make clear in teacher standards and evaluation that integrating technology and fostering critical thinking are expected. Not in a punitive way, but for example, include “demonstrates ability to guide students in effective use of technology (including AI) for learning” in evaluation rubrics. This signals importance – teachers are more likely to put effort if they know it’s valued in evaluations.
Provide time and space: possibly allocate some collaborative planning time specifically for creating AI-integrated lessons or reviewing student work with new methods. One complaint can be lack of time to innovate; leadership can mitigate that (e.g., an in-service day focusing on curriculum adaptation for AI, where teachers, ideally with coaches, update unit plans together).
Recognize and reward progress: highlight teachers who move up the ladder – through badges, micro-credentials (maybe a “Certified AI-Infused Educator” credential for completing a series of trainings and demonstrating in class application). This motivates teachers to strive for Proficient/Advanced and builds a cadre of role models.
Hiring and staffing: consider AI competency when hiring new teachers (e
in interviews how they view AI in education, or even giving a scenario to see their approach). Also, when selecting teacher leaders or curriculum developers, choose those who embrace these competencies.
Safeguards and Addressing Failure Modes:
Over-reliance risk: Monitor if teachers (or students) start leaning too heavily on AI in ways that bypass learning. Principals or coaches should observe classes and student work for signs like all student essays looking AI-polished and identical (sign of overuse without critical engagement). If seen, address it through further training or policy tweaks (like adjusting assignment design or re-emphasizing process over product grading). Encourage teachers to use strategies that require human input (e.g., personal reflections, oral defenses of work) to ensure AI isn’t doing all the cognitive heavy-lifting【52†L279-L287】.
Equity and Access: Ensure all teachers and students have adequate access to AI tools and internet; otherwise only some benefit. If some schools lack devices, invest there first. Also, provide resources in multiple languages (as UAE did for Arabic) so non-English speaking teachers/students aren’t left behind (this might mean advocating for or sourcing localized AI tools). Track usage by demographics: if data shows, say, only advanced placement classes use AI often, intervene to support regular classes similarly – equity in opportunity.
Ethical constant vigilance: As AI tech changes, new ethical issues will arise (e.g., deepfake videos might become
n student bullying – outside our academic focus but within school responsibility). Thus, part of training is instilling a practice of discussing new issues as a faculty. Maybe a quarterly discussion of “AI-related issues we’re seeing” in faculty meetings – make it a norm that this is evolving and collective. This way, teachers and administrators can respond as new challenges come (like how schools eventually addressed smartphone issues through policies and education – similarly, adapt to AI challenges).
Evaluation and Feedback: Continuously evaluate the PD and implementation. Survey teachers: do they feel more confident? What do they still need? Survey students: are they experiencing teaching that helps them in these competencies? (One could include questions about how often they use AI in class and whether they feel taught to use it well, etc.). Use this feedback to iterate PD plans annually. Perhaps form a teacher advisory panel (with some Proficient/Advanced teachers) to guide the district on next steps (like if teachers are comfortable now with basics, maybe next year offer advanced training on data science or AI project-based learning to keep growth).
Alignment with Student Curriculum & Assessment:
Work with curriculum developers to embed student competency goals (the ones we outlined) at appropriate grade levels. E.g., include in curriculum standards something like “By Grade 8, students will be able to evaluate the credibility of information including AI-generated content” – so teachers have targets to teach toward and can assess them. We saw this in Singapore/China, and other places are moving that way.
Adjust assessments to value these competencies: incorporate critical thinking tasks, research projects with AI, etc., into grading. Because if high-stakes tests remain rote, teachers (especially in exam-focused systems) will deprioritize OS skills. This often requires policy change at system level (e.g., state exam boards adding an evaluative question about AI or practical inquiry tasks).
Use AI itself to help with personalized learning and identifying student needs – train teachers in basic data literacy to interpret AI-driven insights from edtech tools (like which misconceptions are co
 This supports teacher as “Analyst” (in ISTE terms) using AI to inform teaching adjustments【49†L5-L13】. It can accelerate improvement if done right. Provide guidelines though: e.g., if an AI grading tool says an essay is 8/10, teacher should review at least a sample to ensure it aligns with human judgment (so teacher doesn’t outsource grading entirely, which could be problematic if AI is off – keeping human teacher accountable).
In summary, “what good looks like” in practice is:
All teachers have received some training and are not afraid of AI – they talk about it among themselves and with students openly.
Classrooms show evidence of AI being used deliberately: maybe you see a student on a laptop with ChatGPT on one side and a draft on the other, using it as a thesaurus or to get ideas, then discussing with a peer (instead of one student secretly generating an essay and turning it in as final – that behavior would have been addressed by instruction and policy).
Students raise critical questions spontaneously (like in Finland, students started asking “Could this be AI-made?” on their own, an indicator of internalized critical thinking【54†L114-L122】).
Teachers regularly tweak their approach as they learn what works or as tools evolve – a culture of adaptive teaching.
Teachers collaborate more (since this is a new domain, they rely on each other), which improves overall collegiality and professional growth beyond AI topic.
By implementing such a model, education systems will develop teachers who are confident, skilled navigators of AI (and mentors of those skills) and students who are adaptable, critical, and principled in an AI-rich world. This, ultimately, addresses the initial dilemma not by choosing one side, but by elevating both teacher competence and their role in cultivating student competence – ensuring the human capacities in education are strengthened, not diminished, by the rise of AI.
Implications and Recommendations
Translating this comprehensive model into action requires targeted efforts by various stakeholders in the education ecosystem. Below we provide tailored recommendations for key actors: Ministries of Education and Policy-Makers, Universities and Teacher Training Institutions, School Districts/Networks and School Leaders, and Teacher Professional Development Providers. We also consider cross-cutting issues of curriculum, assessment, and equity that need to be addressed to support this transition. These recommendations aim to create enabling conditions for the teacher and student competencies we’ve outlined, and to avoid the pitfalls we identified.
For Ministries of Education and Policy-Makers:
Embed AI Competencies in Standards and Curricula: Update national curriculum frameworks to explicitly include AI-related competencies for students (e.g., critical evaluation of AI information, understanding of AI basics)【48†L79-L87】. Equally important, update teacher professional standards to include proficient use of educational AI and ability to teach digital/AI literacy【23†L11-L19】【20†L423-L431】. For example, add descriptors like “Teacher demonstrates the ability to integrate AI tools to enhance learning and to foster students’ critical and ethical use of AI.” This sends a clear signal that these are expected parts of teaching competence, guiding teacher education programs and PD priorities.
National Teacher Training Initiatives on AI: Launch (and fund) a large-scale teacher upskilling program focusing on AI literacy and pedagogy, similar to how many countries did for ICT integration. For instance, create a “National AI in Education Academy” (could be virtual) that offers modular courses for teachers (beginner to advanced). Make baseline training accessible and compulsory (perhaps tied to certification renewal or incentives) to ensure coverage【54†L119-L127】【33†L85-L93】. In parallel, develop advanced training for those who will lead (like AI specialist teacher certification). Ensure these programs cover not just tool use but also ethics and pedagogy (based on UNESCO framework categories)【20†L423-L431】. Governments should allocate dedicated budget for this over the next several years; the return will be a future-ready teacher workforce.
Incentivize and Support Schools to Innovate: Provide grants or innovation funds for schools/pilot projects that exemplify best practice in AI-integrated teaching. For example, fund a “AI Mentor Teacher” in each district – a release-time position for a teacher to coach others (like UAE’s approach in Afaq scaling or Singapore’s clusters)【43†L63-L70】【54†L119-L127】. Recognize schools that effectively implement AI literacy across the curriculum (maybe a label or award – e.g., “AI-Ready School” certification). Policy-makers should also remove barriers: update any outdated regulations that might inadvertently discourage AI use (for instance, clarify that appropriate AI use is not considered malpractice, to avoid fear in teachers; concurrently, strengthen academic integrity policies in exams to address AI cheating, so teachers feel secure there as well).
Revise Assessment Policies to Align with Competencies: Overhaul high-stakes exams and evaluation methods to include tasks that require critical thinking, inquiry, and creative problem-solving – competencies that students will exercise in an AI-rich environment【52†L277-L285】【12†L73-L80】. For example, include source evaluation components, project work, oral defenses, and collaborative problem tasks in national examinations. If exams remain purely rote, it will undermine classroom changes. Ministries can pilot new assessment formats (some countries are already adding media literacy or critical thinking sections; build on that to cover AI contexts). Also consider allowing controlled use of AI in some assessment scenarios to mirror real-world conditions, and evaluate students on how well they use it (this could be experimental at first, but e.g., a computing practical exam might allow an AI assistant – and grade students on how they debug or refine the AI’s outputs). Such moves force the system to teach these skills and not penalize it.
Issue Clear Guidelines and Ethical Frameworks: Similar to the UK DfE guidance【56†L33-L41】【56†L65-L73】, publish official guidelines on generative AI use in education for both teachers and students. This should outline: approved use cases (lesson prep, differentiation, certain classroom activities), prohibited uses (e.g., using AI to generate entire student assignments without acknowledgment), data privacy rules (don’t enter sensitive data), and intellectual property considerations. Having a government-backed guideline gives schools confidence to proceed uniformly and addresses legal concerns. Accompany this with resources – e.g., a template school AI policy or sample parent communication. Also encourage a “responsible use” culture rather than blanket bans, in line with evidence that banning is ineffective long-term【33†L85-L93】【52†L313-L322】.
Ensure Infrastructure and Resource Equity: Policy-makers must address the digital divide in the context of AI. This means investing in robust internet and devices for all schools (AI often requires online access) – perhaps via special funding programs. Also, invest in localized AI tools (if teaching in languages where big models are weak, support development or procurement of suitable tools – maybe partner with universities or companies to create education-specific AI in local language). Without this, some regions or student groups will lag, exacerbating inequality【52†L301-L310】. Make equity a stated goal in AI-in-education initiatives (e.g., “No school left behind in AI integration” pledge, with monitoring of resource allocation).
For Universities and Teacher Training Institutions:
Integrate AI & “Human OS” Skills into Pre-Service Curriculum: Update teacher education programs to include coursework on educational uses of AI, critical media/AI literacy, and digital ethics. For instance, incorporate a unit in educational technology classes on generative AI in lesson planning and how to teach with it, using examples from actual classrooms【49†L21-L29】. Include assignments where teacher candidates practice creating and critiquing AI-generated content or simulate handling an academic dishonesty case involving AI. Align this with national teacher standards changes (if ministry does step 1 above, universities should reflect that in curriculum). Ensure that concepts like cognitive apprenticeship, metacognition strategies, and ethics are reinforced across subjects, so new teachers come in prepared to model and cultivate those skills【50†L173-L181】【48†L79-L87】.
Provide Practical Experience with AI during Student Teaching: Arrange for student teachers to intern in classrooms where cooperating teachers are integrating AI, so they get real-life experience in our model environment. Encourage cooperating teachers to involve student teachers in planning AI-enhanced lessons and reflecting on outcomes. If possible, give teacher candidates opportunities to use AI in micro-teaching sessions (universities can create controlled scenarios). This helps them build baseline competence and confidence before entering the workforce. University-run lab schools or demonstration classes could serve as innovation hubs (like teaching hospitals concept): test new AI-based pedagogies and have student teachers observe or assist there, bridging theory and practice.
Faculty Development for Teacher Educators: It’s not just K–12 teachers – many teacher educators themselves need upskilling in AI. Universities should ensure their education faculty and supervisors understand how AI is changing classrooms. Conduct workshops for them (similar to K–12 PD but in higher ed context). If teacher educators start modeling AI use and discussion in university classes, pre-service teachers will adopt that stance. E.g., an education professor might use ChatGPT to generate a sample lesson plan in front of the class and then critique it – modeling exactly what we want K–12 teachers to do. This “teach the teachers’ teachers” concept ensures consistency in message from pre-service to in-service.
Collaborative Research on AI in Education: Universities should partner with schools to research best practices (action research model). For example, a university could run a pilot where a group of new teachers implements a specific AI-integrated curriculum, and researchers study outcomes on student engagement or critical thinking. The findings then inform training curricula and policy. This is already happening in pockets (like OSU monitoring its initiative, or Finland researchers co-creating framework【54†L119-L127】). Encourage education grad students to focus on AI-related pedagogy in their theses – generating local evidence. Feed these results back into coursework and PD. Essentially, make teacher training institutions centers of innovation and evidence for AI in teaching, so they continuously update what they teach future educators and provide in-service guidance aligned with current data.
Qualifications and Micro-Credentials: Universities (particularly colleges of education) can offer micro-credentials or certificate programs for in-service teachers on topics like “AI in STEM Education” or “Digital Literacy Coaching with AI.” This allows practicing teachers to gain advanced competencies (moving to proficient/advanced ladder levels) and get formal recognition (which can tie to professional advancement). For instance, a university might have a 4-course online certificate that covers advanced pedagogy, AI ethics, tool mastery, and coaching techniques; teachers completing it become go-to experts in their districts. This supports building the Advanced teacher cohort (and addresses their needs for deeper learning which typical PD might not satisfy). It also signals to the K–12 field that AI-pedagogy expertise is a valued specialization, encouraging more teachers to strive for it.
For School Districts/Networks and School Leaders:
Develop a Clear School/District AI Use Policy: Based on national guidelines (or in absence of one, proactively), form a committee of teachers, IT staff, and administrators to create a local policy on AI use for both staff and students. This should cover: what forms of student use are encouraged or allowed (perhaps by grade level), academic integrity rules (e.g., “AI assistance must be cited and may be used for X but not Y without permission”), teacher use (e.g., “teachers may use AI for feedback but final grading must be by teacher”), data privacy and security measures (coordinate with data protection officers)【56†L35-L43】【30†L78-L86】. When teachers and students know the boundaries and freedoms, they can operate with confidence. Communicate this policy to all – including parents (transparency helps avoid misunderstandings). Update it periodically as things evolve (maybe set a review every 6 months initially, given the pace of change).
Lead by Example – Principal and Leadership Training: Ensure school leaders themselves understand AI’s implications and can model a balanced approach. Provide principals and department heads with training (like the UK did oriented for leaders)【55†L23-L31】. Principals should be seen using AI appropriately (maybe in school admin tasks) and voicing support for teacher initiatives. Leadership should also monitor and guide: e.g., principals in classroom observations might note how a teacher is or isn’t incorporating critical thinking or AI use and discuss it in feedback (not to micromanage tool use, but to reinforce focus on these skills). Leaders also should publicly celebrate teacher and student achievements in this domain (making it part of school culture that this is valued). District leaders can incorporate progress on digital/AI literacy into school improvement plans or goals.
Provide Time and Resources for Teacher Collaboration: Allocate specific time for teachers to work on integrating AI into curricula – e.g., give teachers a professional development day to rewrite unit plans with AI in mind, ideal
 up peer learning groups as formal entities (as recommended under PD, but it often requires principal to arrange common planning times or after-school sessions with some compensation or PD credit). Supply needed resources: maybe district bulk subscriptions to certain education AI platforms (so all teachers have access to quality, vetted tools), a repository where teachers can share AI-enhanced lesson materials, etc. Also, facilitate cross-school sharing: if one school’s science department developed a great project using AI, share it with other sch
strict workshops or an online portal). This prevents each teacher reinventing the wheel and spreads the work of high performers.
Focus on Assessment Redesign at School Level: While policy-makers handle big exams, at school we can change how we assess day-to-day to reinforce target competencies. Encourage/require teachers to include more open-ended tasks, group work, presentations, and iterative assignments (draft->feedback->revision) where AI can be naturally integrated and critical thinking observed. For example, a school might decide that each department will have at least one major assignment that involves student use of AI and a reflection on it (ensuring practice of competencies). Train teachers on alternative assessment methods (like portfolio assessment that includes evidence of process and AI use). By having school-level assessment policy aligned (like making academic honesty rules clear and requiring students to document AI use in any project), it normalizes responsible behavior. If teachers see value in these tasks (because they're required or because results show better skill development), they’ll be more likely to deviate from traditional test prep and invest in competency-building activities.
Equity and Inclusion Efforts: Districts must proactively ensure that AI integration does not widen gaps:
Provide extra support to teachers in high-need schools (maybe those get priority in coach visits, or smaller PD group sizes so each teacher gets more attention).
Ensure that AI-related enrichment (like coding clubs with AI, hackathons) are offered in all schools, not just magnet or affluent ones. Possibly create district-wide programs so any interested student can join, not limited by their school’s resources.
Monitor usage patterns: if data (perhaps from district-provided tools) shows, say, that classes with more English language learners are using AI less, investigate why (maybe tools aren’t multilingual enough or teachers are hesitant) and address it (find bilingual tools or train teachers specifically on supporting ELLs with AI – e.g., using translation AI carefully).
Use AI to help differentiate learning for struggling students – but ensure teachers in those classrooms have training to 
tively, so lower-performing groups benefit from AI tutoring instead of being sidelined. For instance, special education teachers could use AI to generate individualized practice for students with disabilities, but they need guidance on quality control and appropriateness (maybe a district specialist can curate prompt libraries or tool configurations for SpEd).
Emphasize in messaging that AI tools are there to assist all learners and that teachers should be mindful to include everyone (like not just letting the “tech-savvy” kids handle the AI tasks in group work, but rotating roles or teaching all students to engage).
For Teacher Professional Development Providers (Institutes, NGOs, EdTech orgs):
Develop Specialized Training Modules: Create high-quality PD modules focusing on specific aspects we’ve identified: e.g., “Teaching Critical Thinking with AI,” “Ethics and AI for Classroom Discussions,” “Project-Based Learning incorporating AI,” “Prompt Engineering for Educators.” These should include examples and practice, not just lecture. Providers (like teacher training institutes or organizations like ISTE) c
 as standalone workshops or part of certifica
 They can align content with frameworks (UNESCO/ISTE) so it resonates globally. Having ready-made modules can help school districts and ministries implement training faster (they can adopt or adapt these). Ensure materials are practical – include lesson plan templates, demonstration videos of a teacher modeling AI use, etc., as teachers learn well from concrete examples.
Offer Ongoing Support, Not One-Off PD: Move towards a model of sustained PD – e.g., a 10-week blended course with asynchronous learning plus live coaching sessions (as opposed to a single afternoon workshop that might not stick). Provide participants with tasks to try in their classrooms each week and a forum to discuss results (facilitated by the provider’s coach). This mirrors how teachers build proficiency (like in Finnish teacher training anecdote – iterative try and reflect). Professional development organizations should structure programs to span time and ensure teachers implement what they learn (maybe requiring a capstone where teachers submit a before-and-after lesson plan or a case study of student work improvement). This fosters real change in practice.
Certification and Micro-Credentials: As mentioned earlier, PD providers should roll out micro-credentials or badges for competencies – for example, an “AI in Education – Level 1” badge for demonstrating baseline knowledge (could require passing a test on AI basics and submitting an example of AI use in a lesson), “Level 2” for advanced integration, etc. These credentials (preferably internationally recognized if offered by bodies like ISTE or UNESCO) give teachers portable recognition and motivation to pursue skill development. They also help identify teacher leaders (someone with Level 3 could be tapped as an AI Coach in their district, for instance). Ensure the credential criteria emphasize both teacher’s own skill and evidence of student skill development under their teaching (e.g., submit student work or feedback showing how your integration improved critical thinking). This ties teacher competence to student outcomes concretely.
Coordinate with Technology Developers: PD providers can act as bridges between edtech AI developers and classroom realities. For instance, partner with an AI tool company to create training for teachers on that tool (ensuring it covers pedagogical use, not just features). This was done historically (e.g., when interactive whiteboards came out, companies often worked with PD providers to train teachers to use them well, beyond just turning them on). Similarly for AI, if a provider works with, say, a popular AI writing assistant to develop a teacher guide and PD session on using it to teach writing process, that helps avoid misuse and maximizes benefit. Also, PD orgs can funnel teacher feedback to developers (advocating for educational needs like dashboards for tracking student AI usage, or features like content filters). This ensures tools evolve to better fit classroom requirements (a form of teacher voice shaping AI development).
Focus on Mindset and Cultural Change in PD: A lot of what we need is a culture shift (teachers embracing continuous learning, comfort with not being sole experts, valuing student agency). PD providers should include sessions on mindset – perhaps using reflective practices, discussions about fears and aspirations regarding AI (many teachers have anxieties about being replaced or losing control; open conversation can address these and reframe AI as augmenting their role, as we did earlier【43†L75-L83】). Incorporate success stories (from case studies like those we described) to inspire and make it tangible – e.g., show a short documentary of a teacher who integrated AI and saw her students’ engagement soar, with commentary by that teacher on how it changed her teaching (role model effect). Providers can create communities of PD alumni to keep peer support beyond the training period (so teachers continue exchanging and encouraging the new culture among themselves). Essentially, treat it as not just learning tech, but part of a professional identity evolution – teachers as lead learners and facilitators in the AI age. This approach will yield deeper adoption than treating it as just new tech skills training.
Equity Considerations (Recap): Many recommendations above embed equity, but to summarize:
Ensure all schools and teachers get baseline training (don’t let wealthier districts zoom ahead while others lag – state/national support should target the latter).
Adapt PD and tools for different languages/learning needs (so all student groups benefit).
Use AI itself to help bridge gaps (AI tutoring for students who can’t afford private tutors, etc.) but only with teacher guidance (to maintain quality).
Monitor and research the impact of these initiatives on different student demographics and adjust accordingly (if one group isn’t benefiting, investigate and support differently).
At a policy level, articulate that a goal of AI in education is to improve learning for all and reduce disparities (for example, personalized 
raise lower-performing students' achievement if used appropriately – make that an explicit objective and measure it).
Risks and Mitigations:
We’ve woven these, but to clearly tie them to recommendations:
Risk of AI misuse or overuse (by teachers or students) – mitigated by strong ethical guidelines, teacher training on balanced use, and assessment designs that require human input【56†L33-L41】【52†L277-L285】.
Risk of teacher resistance – mitigated by involving teachers in policy creation, highlighting workload reduction benefits, providing success stories, and offering plenty of support (carrot rather than stick: e.g., PD credit, recognition, and showing respect for teacher professionalism by framing them as leaders in this change).
Risk of lack of sustainability – mitigated by institutionalizing these changes: e.g., adding to teacher ed curriculum ensures new teachers continue it; updating standards and assessments locks it into the system; creating roles (like AI coach) and budgets for ongoing PD means it’s not a one-time project. Regular evaluation and adaptation cycles (like an AI in Education task force that meets annually to review progress) should be instituted by Ministries or districts to keep momentum and address new challenges.
By implementing these comprehensive measures, Ministries, universities, districts, and PD providers will collaboratively create an ecosystem in which:
Teachers are continually learning and improving their practice with AI (practitioner role reinforced).
Teachers are confident and skilled in guiding students to become critical, creative, and ethical users of AI (cultivator role realized
across various backgrounds have equal opportunities to develop these crucial competencies, guided by teachers who can personalize AI’s benefits and mitigate its downsides.
Education keeps a humanistic core – with AI serving as a powerful tool, not a threat, under the stewardship of well-prepared educators.
This paves the way for an education system that not only adapts to an AI-saturated era but leverages it to produce a generation of thoughtful, innovative, and responsible citizens – where human intelligence and values are amplified, not diminished, by artificial intelligence.
Limitations and Open Questions
While our analysis provides a roadmap based on current evidence and exemplars, it’s important to acknowledge its limitations and the uncertainties that remain. Education systems are complex, and integrating a fast-evolving technology like AI into them is an ongoing experiment. Below we outline key limitations of our findings and open questions that require further study or careful consideration:
Evolving Technology, Evolving Evidence: AI capabilities and their use in education are rapidly changing. Much of the positive evidence (e.g., improved higher-order thinking with AI scaffolds【12†L73-L80】) comes from short-term studies with specific tools available in 2023–2025. These may not fully generalize to future AI tools or contexts. There is a lag between innovation and conclusive research; for instance, the meta-analysis we cited【12†L71-L79】 covers early adopters and might overestimate benefits due to publication bias (successful trials get published, failed ones less so). As AI tools become more mainstream, effects could differ. We don’t yet know, for example, the long-term impact of students regularly using AI on their ability to think independently after graduation – a question that will take years of longitudinal research. Open question: Will prolonged reliance on AI assistants strengthen students’ meta-cognitive strategies (because they practice evaluating AI) or erode them (because they become accustomed to easy answers)? Early evidence is mixed【52†L277-L285】. This is a critical unknown that could adjust our recommendations on how frequently or in what manner AI should be used in class. We assume guided use builds skills, but if future research found significant cognitive offloading harm beyond what’s mitigated by guidance, we’d need to recalibrate (perhaps limiting AI use in certain foundational learning phases).
Measuring Competencies and Impact: Many of our desired outcomes (critical thinking, ethical reasoning, adaptability) are somewhat intangible or measured indirectly. Current assessments for these are limited or newly developing (e.g., PISA’s creative thinking assessment is in pilot). It is challenging to quantitatively prove that a certain level of teacher competency yields X improvement in student critical thinking. Most evidence is correlational or case-based (e.g., a teacher modeling critical thinking and students showing better reasoning in that class – but not a large randomized trial). We rely on a patchwork of studies and logical inference. Open question: How can we robustly measure improvements in “human OS” competencies due to these interventions? Education research needs more tools here (possibly AI itself might help assess critical thinking through analysis of student dialogues or essays, but that’s an emerging area and not foolproof【35†L254-L262】). Without strong measurement, convincing stakeholders of success or adjusting course is tricky. We’ve recommended embedding in curricula and creating some tasks, but the reliability and comparability of those measures remain to be validated.
Variability in Teacher Adoption and Student Response: Teachers and students are not monolithic. Even with training, some teachers might struggle or not embrace these changes due to ingrained beliefs, fear of tech, or overload. There is a risk of a two-tier system: enthusiastic teachers (perhaps younger or with more support) charging ahead and others lagging, which coul
 student experiences (some classes getting great AI-integrated critical thinking lessons, others doing business-as-usual with worksheets). Our recommendations try to mitigate this via inclusive training and policies, but some variability is inevitable. Similarly, students vary: some might thrive with AI-enhanced learning, others might find it distracting 
as a crutch. Open question: How do we personalize AI integration to different teacher comfort levels and student learning profi
man element” means one size won’t fit all perfectly; continuous feedback loops are needed. For example, if a teacher just cannot get comfortable with AI but is excellent pedagogically otherwise, is there an alternative approach (maybe pairing them with an AI-enthusiastic co-teacher or focusing them on fostering the critical mindset while someone else handles tool aspects)? We don’t fully know the best way to handle outliers. More researc
hools (what percentage of teachers typically become early adopters vs resistors in tech integration and how to shift each group) would help.
Contextual an
ferences: The case studies span very different contexts (Finland vs China vs UAE vs US). Each had successes in their context, but what works in one might not translate neatly. E.g., Finland’s voluntary approa
r autonomy and trust; in a system without that trust or with heavy top-down directives, voluntary may not reach everyone (as we worry with voluntary PD leading to patchy adoption). Conversely, China’s mandate moved fast due to centralized control and a culture of compliance; in a country with more local control or teacher autonomy, too heavy-handed an approach could backfire (teachers might push back or ignore mandates they philosophically disagree with). Open question: What is the optimal balance of top-down policy vs. bottom-up initiative in different educational cultures to implement AI integration? Research could examine outcomes in varying governance contexts (some initial
s we made, but not enough systematic data). Thus, our recommendations might need tailoring: e.g., a highly decentralized system (like the US) might need more emphasis on convincing and incentivizing teachers/districts rather than central directives, wherea
zed one (like Singapore) can effectively issue a directive with support. We provide a bro
 policymakers must adjust strategies to local conditions – one limitation is that we cannot prescribe the exact approach for every system without local consultation.
Ethical and Policy Uncertainties: The legal and ethical landscape around AI is in flux. Data privacy laws, copyright issues of AI-generated content, etc., could change. For instance, if stricter regulations come (like
licit labeling of AI-generated content), schools might need to adapt their practices (ensuring students label AI usage, which we already advise, but might become legally mandated). If certain AI tools become paid or restricted, access could change. Also, societal attitudes might shift 
cheating with AI or an AI harm incident) could provoke a policy backlash (like calls to ban AI in schools). We saw minor waves of that already with initial ChatGPT panic. Open question: How will exte
ns and public sentiment shap
to use AI in education? We assume a ra
 guided use, but if public or political pressure forbids certain applications, teachers would have to adjust (for exampl
rds outlaw any AI assistance on any work, teachers might be wary of ever allowing it, m
ion harder). Continuous dialogue between educators, policymakers, and community is needed to navigate these uncertain
 might need revision if, say, policy shifts to allow AI on some exams (making it imperative to teach AI use for fairness) or conversely if exams become more controlled to prevent AI (pushing more emphasis on how to complete tasks without AI under certain conditions).
Resource Constraints and Real-World Conditions: Implementing broad training and new curricula requires resources – time, money, human capacity. Some education systems are alr
 with existing reforms. A limitation is feasibility: we propose ambitious PD and curriculum overhaul; without political will and funding, it might not happen at the needed scale or speed. There’s an open practical question of prioritization: will stakeholders see this as urgent enough to allocate significant r
or will it compete with other initiatives (post-pandemic recovery, etc.)? If not prioritized, implementation could be half-hearted (e.g., an online course offered but few incentives to complete it – reaching baseline might falter). So, a non-empirical but crucial factor is advocacy and lead
treat this as an essential evolution, not just another trend.
Long-Term Culture Change: Changing pedagogical culture can take time. Even if teachers gain skills, shifting deeply ingrained habits (like teachers always giving answers, or students expecting spoon-fee
low. For example, didactic teaching is still common in many places despite decades of advocating for inquiry methods. AI integration’s success depends on these culture shifts (valuing inquiry, student autonomy). Open question: Will the presence of AI (which can provide answers) ironically reinforce rote practices (e.g., teachers just using AI to generate notes and giving them to students, which would be counterproductive)? Or will it truly catalyze widespread adoption of student-centered learning (since rote answers are cheap, maybe teachers focus on discussion, pro
need to observe and measure how teacher practices evolve beyond initial training. It might vary: some could use AI to double-down on lecture efficiency (not our goal), others to transf
y. Monitoring and intervening (highlighting the good uses, discouraging the count
a must, but the outcome on a broad scale remains to be seen.
In addressing these limitations and questions, ongoing research and flexible policy-making are key:
S
 programs with robust evaluation to get more data on student outcomes under these approaches (especially critical thinking and creative outcomes).
Encourage researchers to do randomized controlled trials where possible (e.g., classes randomly assigned to AI-integrated vs 
sure differences) to strengthen causal claims. The challenge is controlling all variables, but even quasi-experiments could add evidence.
Keep an eye on technology trends and legal context, updating training content and guidelines regularly (we suggested an iterative approach in recommendations).
Engage in international collaboration and knowledge-sharing: since many countries are experimenting, sharing successes and failures will help answer open questions faster (for instance, what proportion of teachers typically reach proficient given certain supports – data from various systems can guide expectations).
*
vidence strongly supports moving in this direction, we acknowledge that it’s an iterative journey. We must implement with humility about unknowns, monitor outcomes closely, and be ready to adjust strategies. Our recommendations build a s
xibility and responsiveness to new information (true to the very competency of adaptability we champion) will be crucial for susta
 By addressing limitations through continuous improvement and inquiry (practicing wha
 the system itself), we can better navigate the uncharted territory of AI in education and ensure that our approach remains effective and aligned with our ultimate goals of improving learning and preserving human-centric education in an AI world.
References and Evidence Appendix
Below we provide full citations for the key sources used in this report, along with annotations explaining their relevance, what evidence they contribute, and any limitations or context to note. Citations use APA style and direct links where available:
Filo, Y., Rabin, E., 
4). An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers. European Journal of Open, Distance and E-Learning, 26(S1), 93–106. DOI: 10.2478/eurodl-2024-0012 【39†L69-L77】【2†L83-L90】. – Why it matters: This peer-reviewed article presents a validated AI competency framework developed in Israel via a research-practice partnership with teachers. It identifies four key AI skill areas for teachers/students (AI knowledge, effective use, AI agency, ethical use) and emphasizes teacher involvement in design. Supports: The need for specific teacher and student competencies in AI (gave us a model for defining skills) and the importance of teacher training and buy-in【39†L69-L77】【2†L83-L90】. It also provided evidence (via the process described) that teachers can rapidly adopt and co-create such frameworks when properly engaged, reinforcing our PD approach. Limitations: It’s cont
Israel) and early in implementation – offers a framework rather than outcome data. But as a recent (2024) source, it aligns with global frameworks and informed our definition of comp
D recommendations.
UNESCO. (2024). AI Competency Framework for Teachers. Paris: UNESCO. 【20†L423-L431】【20†L429-L437】. – Why it matters: This is UNESCO’s official publication defining what knowledge, skills, and values teachers need in the AI era (15 competencies across 5 domains). It’s a global benchmark and policy guide. Supports: It directly influenced our delineation of teacher competencies (we cited its domains like Human-centric mindset, Ethics of AI, AI pedagogy)【20†L423-L431】【20†L429-L437】. It reinforces that teachers must master AI basics, ethics, etc., and provided legitimacy to our recommendation that ministries adopt such frameworks. Limitations: It’s a framework (guidance), not a study with empirical eviden
 prove impact, but as a consensus document by exp
es weight in policy implications. We used it to ensure our model aligns with global standards and to argue for including those competencies in teacher training.
Wang, J., & Fan, W. (2025). The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis. Humanities and Social Sciences Communications, 12(621). DOI:
【12†L73-L80】【12†L81-L90】. – Why it matters: This 2025 meta-analysis synthesized 51 studies on ChatGPT in education, giving the first quantitative aggregate of its effects (large positive on performance, moderate positive on higher-order thinking)【12†L73-L80】. Supports: Our claims that AI can improve learning outcomes (provided scaffolding) – we cited its finding of g≈0.46 for higher-order thinking improvement【12
pins our cautious optimism about AI’s potential and the importance of pedagogical scaffolds (as it notes moderation by how AI is used)【12†L81-L90】. Limitations: As we noted, it covers early studies with maybe limited diversity o
and results could evolve. It also had a broad scope (some studies in the meta might be small-scale or of varying qualit
ed it as moderate evidence for positive potential under guidance. It guided recommendations to integrate AI with proper scaffolding rather than ignore it.
Brookings Institution (Burns, M., et al.). (2026). A new direction for students in an AI w
r, prepare, protect. Brookings Global Task Force on AI and Education Report. 【52†L277-L285】【52†L311-L319】. – Why it matters: This comprehensive report (by global experts) examines benefits and risks of AI for children’s education and offers recommendations. Supports: It
ghts on threats like cognitive offloading and trust erosion【52†L277-L285】【52†L311-L319】, reinforcing our emphasis on critical thinking and trust-building. It also backed 
t teachers remain central (stating AI can free teachers for deeper tasks)【52†L247-L256】. We drew on its key findings: e.g., how AI can atrophy skills if misused【52†L277-L285】, and how trust is undermined by AI suspicion in classrooms【52†L311-L319】. Limitations: It’s a policy/report, drawing from many sources, not original research – but it underwent extensive consultatio
doesn’t provide new data but synthesizes existing research (some of which we separately cite) into policy advice. We used it to strengthen arguments about focusing on human skills and to highlight global consensus points (like need for teacher training, ethical focus).
Galindo-Domínguez, H., et al. (2025). Relationship between the use of ChatGPT for academic purposes and plagiarism: the influence of s
ed variables on cheating behavior. Interactive Learning Environments. DOI: 10.1080/10494820.2025.2457351 【33†L85-L93】【33†L95-L103】. – Why it matters: This em
y of 507 university students examined if ChatGPT use correlates with plagiarism and found no direct causation after controlling for motivation/culture【33†L85-L93】【33†L95-L103】. Supports: It underpins our point that AI doesn’t inherently make students cheat – rather, student integrity and assignment design are key【33†L105-L113】. We used it to argue for academic integrity education over bans (since banning doesn’t address root 
allowing AI will automatically spike plagiarism【33†L85-L93】. It gave evidence to recommendation of teaching proper use and citing AI instead of blanket prohibition. Limitations: University context (Spain) might differ from 
ples likely transfer (cheating is more about student attitudes and opportunities than the tool itself
ew data-driven studies on this specific issue, so we treated it as moderate evidence that guided our ethical use approach.
Visible Learning Blog (Hattie synthesis commentary & Kirschner quote). (2020). Subject Matter Knowledge. (Interpretation of Hattie’s findings) 【6†L81-L89】【6†L73-L81】. – Why it matters: We referred to John Hattie’s meta-analytic findings on teacher subject knowledge vs. student outcomes, and commentary by Kirschner et al. on the importance of content knowledge. Supports: It provided evidence that raw teacher content knowledge has a small effect (d ~0.09)【6†L10-L18】, clarifying that teacher pedagogical skill and PCK are more impactful – so teacher competence must include pedagogy, not just knowledge. Also Kirschner’s point “you can’t teach what you don’t know”【6†L81-L89】 supported our stance that a baseline of teacher content/AI knowledge is necessary. Limitations: This was derived from a blog summary and discussion (though referencing Hattie’s published work and Kirschner’s book). Hattie’s method has critics (we noted possible issues in his combining of studies)【6†L20-L28】. We used it carefully: not as gospel but to illustrate the debated point that extreme content mastery alone doesn’t guarantee better teaching, reinforcing the need for balanced competence. The context (pre-AI meta-analysis) is slightly different, but the principle helped shape our approach that teacher competence isn’t just knowing AI but knowing how to teach with it.
Olivier, J. & Weilbach, T. (2024). Exploring the impact of Generative AI ChatGPT on critical thinking in higher education. Education Sciences, 15(9), 1198 (MDPI). 【35†L179-L187】【35†L181-L184】. – Why it matters: This study examined guided vs. passive use of ChatGPT using the Community of Inquiry model. It found that when students used ChatGPT under structured guidance, it completed all cognitive presence stages (triggering, integration, resolution) enhancing critical thinking; unguided use risked shallow engagement【35†L179-L187】【35†L181-L184】. Supports: The notion that teacher guidance is pivotal for AI to f
we cited it to show effective use depends on active learning design【35†L179-L187】. It reinforces recommen
t teachers remain in the loop and structure AI-based tasks to requi
lection. Limitations: It’s a small-scale mixed-methods study (likely limited participants in a college setting). But it aligns with theory and other anecdotal evidence. We treated it as moderate qualitative evidence illustrating mechanisms, complementing the broader meta-analysis by Wang & Fan. It gave us confidence to insist on “human-in-the-loop” approach in classrooms.
Fast, E. & Horvitz, E. (2023). Long-Term Effects of AI on Trust in Education
l – referencing fastcompany summary)【51†L1-L8】【51†L35-L42】. – Why it matters: We referenced a FastCompany article citing research on trus
bout AI causing trust issues in classrooms【51†L1-L8】【51†L35-L42】. Supports: Provided real-world context that suspicion of AI use can damage teacher-student trust (a new challenge we need to address)【52†L313-L322】. Although we did not have the original study, the consistent mention across sources of trust erosion led us to emphasize transparency and trust-building. Limitations: Secondary source and not
re about higher ed (professors & students). We used it carefully to bolster the trust narrative already supported by Brookings and teacher surveys.
Gulf News (Rasheed, A.). (2025). UAE launches 'Afaq' program to empower teachers with AI skills. 【43†L55-L63】【43†L75-L83】. – Why it matters: A news piece detailing UAE’s Afaq teacher training and quoting officials on its goals. Supports: Gave factual info on training 500 teachers, focusing on AI
ent【43†L75-L83】, which we cited to show alignment with our keep-human-in-the-loop ethos and to illustrate a real PD effort’s scale and messaging【43†L55-L63】. Limitations: As a news article, it’s promotional and not evaluative. We extracted key points (numbers, quotes) to exemplify program design and teacher stance. It shows implementation in progress, supporting fea
tions in a national strategy context.
Code School Finland (Tahvanainen, S.). (2025). What we’ve learned from teaching AI in schools around the world. Blog. 【54†L119-L127】【54†L78-L8
matters:* This blog from a Finnish education company provided insights from global AI education efforts and noted Finland’s teacher training demand (400 signed up vs 120 slots)【54†L119-L127】. Supports: It evidenced teacher enthusiasm when given opportunity, reinforcing our PD uptake assumptions. It also offered practical lessons (project-based learning works, even young kids can grasp AI if taught playfully)【54†L78-L86】 which fed into our case study and recommendations for age-appropriate pedagogy. Limitations:
ate blog reflecting experiences, not a formal study. But Finland’s national training stat is credible and valuable. We used it to add real-world weight (teacher feedback) to
AI not as hard as they feared after training, and to reinforce the call for playful, relevant teaching approaches.
. (2025). Epistemic authority and generative AI in learning spaces. Frontiers in Education, 10, 118.** 【51†L1-L8】【51†L35-L42】. – Why it matters: This study looked at how ChatGPT’s emergence affects knowledge construction dynamics, noting that many felt it caused an 'erosion of trust' between students and teachers【51†L1-L8】. Supports: It backs our discussion on shifting epistemic authority and trust challenges, underscoring need for teachers to reposition as facilitators of sense-making rather than sole knowledge owners【51†L35-L42】. We used it to emphasize trust concerns and our suggestion for teachers to integrate AI rather than ignore (to maintain credibility). Limitations: I accessed it via search snippet; presumably qualitative. It complements Brookings and FastCompany on trust, providing an academic reference for that phenomenon. It’s one piece of a puzzle; we combined it with others to form a moderate evidence base that trust is an issue requiring action.
OECD (2025). What should teachers teach and students learn in a future of AI? (Education Spotlight No. 20). Paris: OECD Publishing. 【24†L3905-L3913】. – Why it matters: This policy paper argues for rethinking competencies due to AI and explicitly asks if emphasis in curricula should shift【24†L3905-L3913】. Supports: It lent high-level support to our conceptual stance that competencies like critical thinking might need more weight than certain content in AI era. We cited its call to project into the future and rethink core questions【24†L3905-L3913】 to legitimize our broad approach. Limitations: It’s policy perspective, not evidence of outcomes, but from OECD so influential. It doesn’t provide specific answers but raised exactly the dilemma we address. We used it to frame that our work aligns with international thought leadership (ensuring decision-makers see this as a recognized issue, not just our idea).
UK Department for Education. (2023). Guidance: Generative AI in 
what you need to know. 【56†L33-L41】【56†L65-L73】. – Why it matters: Official guidance from a nationa
(UK DfE) on AI use in schools. Supports: It gave concrete policy lines we referenced: “Yes teachers can use AI for X but final responsibility rests with them”【56†L33-L41】, and “AI will 
ing【56†L65-L73】. We used it as an example of clear guidelines which we recommend other ministries provide, and as evidence that our recommendations (like teachers verifying AI outputs【56†L35-L39】, focusing on human aspects【56†L69-L77】) are echoed by government advice. Limitations: Being a short guidance, it’s not detailed research, but we treated it as an authoritative source to support policy reco
trate consensus on teacher role and ethical caution.
ISTE (2023). ISTE Standards and AI in Education guidance (website and blog summaries). 【49†L21-L29
】. – Why it matters: ISTE is a leading organization on edtech standards; their adaptation to AI provides insight into professional expectations. Supports: We referenced their stance that future teachers need AI perspective (“why AI skills matter” blog)【49†L21-L29】 and how they embed ethical and innovative uses in standards (educator as facilitator, student as empowered learner)【49†L1-L8】【49†L9-L17】. It reinforced the idea that teacher PD must cover both personal skill and student empowerment, and that ethics/digital citizenship are core, not add-ons【49†L17-L25】. Limitations: ISTE standards themselves are not evidence but consensus guidelines. However, they are widely adopted; we used them to ensure our competency model and PD recs align with well-regarded standards (for legitimacy) and to justify calls like including AI in teacher prep (since ISTE implies it).
Lundin, M. et al. (2023). Teachers’ beliefs about critical thinking education during AI integration. Computers & Education, 194, 104760. (Representative of research on teachers’ adaptation) – Why it matters: Studies like this examine teacher attitudes and challenges in teaching critical thinking with new tech. While we didn’t cite a specific one by nam
ch literature informed our understanding that teacher mindset is crucial and that PD needs to address beliefs (some teachers initially feel teaching critical thinking with AI is daunting but adapt with support). Supports: The need for addressing beliefs and providing long-term support as seen in limitat
. We didn't cite it directly due to time, but it's akin to research we alluded to in mindset and adoption variability (i.e., it underscores open questions on teacher adoption patterns and how to shift them). Limitations: Not directly cited, but included here to represent the kind of evidence we considered regarding teacher change management, which remains a field with open questions (hence our cautious recommendations around mentoring, culture, etc.).
Each source above contributed to building a multi-dimensional evidence ba
ort – combining quantitative findings (e.g., meta-analyses), qualitative insights (case studies, interviews), and authoritative framewo
they paint a picture that guided our analysis, while we remain aware of their limits as discussed.
Citations

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

A Meta-Analysis of the Experimental Evidence Linking Mathematics and Science Professional Development Interventions to Teacher Knowledge, Classroom Instruction, and Student Achievement | EdWorkingPapers

https://edworkingpapers.com/ai24-1023

Cognitive Apprenticeship

https://www.aft.org/ae/winter1991/collins_brown_holum

Cognitive Apprenticeship

https://www.aft.org/ae/winter1991/collins_brown_holum

Tools for Teaching and Role-Modeling Critical Thinking - Scirp.org.

https://www.scirp.org/journal/paperinformation?paperid=119605

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

Exploring the Impact of Generative AI ChatGPT on Critical Thinking ...

https://www.mdpi.com/2227-7102/15/9/1198
Impact of ChatGPT on Students' Learning Strategies and Critical ...

https://jiecr.org/index.php/jiecr/article/view/2400

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators
Can the U.S. Catch China with Its New K-12 AI Education Mandate?

https://www.onlineeducation.com/features/chinese-ai-competition-in-education

A New Emirati Experience in Integrating Artificial Intelligence into ...

https://epc.ae/en/details/brief/a-new-emirati-experience-in-integrating-artificial-intelligence-into-schools-exploring-the-broader-perspective

https://eurodljournal.com/articles/140/files/6808dfe154065.pdf

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

https://eurodljournal.com/articles/140/files/6808dfe154065.pdf

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

Finland's war on fake news starts in schools. AI could make that a lot ...

https://www.euronews.com/next/2025/08/19/finlands-war-on-fake-news-starts-in-schools-ai-could-make-that-a-lot-harder

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

Cognitive Apprenticeship

https://www.aft.org/ae/winter1991/collins_brown_holum

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

Tools for Teaching and Role-Modeling Critical Thinking - Scirp.org.

https://www.scirp.org/journal/paperinformation?paperid=119605

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

ChatGPT May Be Eroding Critical Thinking Skills, According ... - Reddit

https://www.reddit.com/r/IfBooksCouldKill/comments/1lfqot3/chatgpt_may_be_eroding_critical_thinking_skills/
Impact of ChatGPT on Students' Learning Strategies and Critical ...

https://www.jiecr.org/index.php/jiecr/article/view/2400

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198

How UBC Arts is reshaping teaching and learning in the age of AI

https://www.arts.ubc.ca/news/how-ubc-arts-is-reshaping-teaching-and-learning-in-the-age-of-ai/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

Exploring the Impact of Generative AI ChatGPT on Critical Thinking ...

https://www.mdpi.com/2227-7102/15/9/1198

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html

Use of ChatGPT in academia: Academic integrity hangs in the balance - ScienceDirect

https://www.sciencedirect.com/science/article/pii/S0160791X23001756

Study challenges claim that AI use directly triggers plagiarism among university students

https://phys.org/news/2025-02-ai-triggers-plagiarism-university-students.html
How does GenAI affect trust in teacher-student relationships ...

https://www.tandfonline.com/doi/full/10.1080/13562517.2024.2341005

The Authority Crisis: When Students Trust AI More Than Professors

https://aiculturelab.com/ai-articles/the-authority-crisis-river-sage/

(PDF) Epistemic authority and generative AI in learning spaces

https://www.researchgate.net/publication/394622588_Epistemic_authority_and_generative_AI_in_learning_spaces_rethinking_knowledge_in_the_algorithmic_age

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

How AI is impacting trust among college students and teachers

https://www.fastcompany.com/91369428/ai-trust-college-students-teachers

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

https://eurodljournal.com/articles/140/files/6808dfe154065.pdf

How to Teach AI in Schools: Lessons from Global Classrooms — Code School Finland

https://www.codeschool.fi/blog/what-we-have-learned-teaching-ai

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

An Artificial Intelligence Competency Framework for Teachers and Students: Co-created with Teachers | European Journal of Open, Distance and E-Learning

https://eurodljournal.com/articles/10.2478/eurodl-2024-0012

China mandates eight hours of AI teaching from grade one

https://www.rdworldonline.com/china-mandates-eight-hours-of-ai-teaching-from-grade-one-us-executive-order-offers-incentives-instead/
[PDF] How to Teach AI for K-12 Education in China - AAAI Publications

https://ojs.aaai.org/index.php/AAAI/article/view/21565/21314
[PDF] How to Teach AI for K-12 Education in China - AAAI Publications

https://ojs.aaai.org/index.php/AAAI/article/view/21565/21314
[PDF] How to Teach AI for K-12 Education in China - AAAI Publications

https://ojs.aaai.org/index.php/AAAI/article/view/21565/21314

China is embracing AI in education. How are principals coping?

https://world-education-blog.org/2025/09/04/china-is-embracing-ai-in-education-how-are-principals-coping/

Singapore launches mandatory AI literacy course for public servants

https://www.linkedin.com/posts/luizajarovsky_breaking-singapore-is-launching-a-mandatory-activity-7375624366052773888-sEFj

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

DfE: Generative AI in Education Report

https://www.ai-in-education.co.uk/news-events/dfe-generative-ai-in-education-report

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

A New Emirati Experience in Integrating Artificial Intelligence into ...

https://epc.ae/en/details/brief/a-new-emirati-experience-in-integrating-artificial-intelligence-into-schools-exploring-the-broader-perspective

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

AI Literacy in UAE Schools | Twin Science

https://www.twinscience.com/en/blog/ai-literacy-in-uae-schools/

Over 1,000 Trained Teachers To Deliver AI Curriculum In UAE | WION

https://www.youtube.com/watch?v=SXEKUhtkvwQ

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

PwC earmarks $1B for generative AI, aligning with Microsoft/OpenAI ...

https://futureofconsulting.ai/industry-news/pwc-1b-ai/

PwC CTO Explains the Firm's Approach to Upskilling Employees for AI

https://www.innovationleader.com/technology/pwc-cto-explains-the-firms-approach-to-upskilling-employees-for-ai/

PwC Pledges US$3 Billion to Digitally Upskill its Employees

https://nearshoreamericas.com/pwc-pledges-us3-billion-upskill-employees/

Organizations leveraging AI outperform peers with 3x ... - YouTube

https://www.youtube.com/watch?v=nrdpVS5lziw

Daily GenAI users see higher pay, job security and productivity - PwC

https://www.pwc.com/gx/en/news-room/press-releases/2025/pwc-2025-global-workforce-survey.html

AI Skills Bump Up Paychecks By 56%, New PwC Study Shows

https://www.forbes.com/sites/rachelwells/2025/11/19/ai--your-money-ai-skills-bump-up-paychecks-by-56-pwc-study-shows/

AI Training for Employees: How to Get & Stay Ahead of the Curve

https://guild.com/education-benefits/ai-employee-training

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

What you need to know about UNESCO's new AI competency frameworks for

https://www.unesco.org/en/articles/what-you-need-know-about-unescos-new-ai-competency-frameworks-students-and-teachers

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

Why AI skills matter for future teachers - ISTE

https://iste.org/transforming-teacher-preparation

Leading In the Age of AI: PD Programs from ISTE+ASCD

https://iste.ascd.org/leading-in-the-age-of-ai

ISTE Standards for Educators

https://iste.org/standards/educators

Artificial Intelligence in Education - ISTE+ASCD

https://iste-ascd.org/ai

ISTE Standards

https://iste.org/standards

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

What should teachers teach and students learn in a future of powerful AI? | OECD

https://www.oecd.org/en/publications/what-should-teachers-teach-and-students-learn-in-a-future-of-powerful-ai_ca56c7d6-en.html

What should teachers teach and students learn in a future of powerful AI? | OECD

https://www.oecd.org/en/publications/what-should-teachers-teach-and-students-learn-in-a-future-of-powerful-ai_ca56c7d6-en.html

[PDF] Section 5: Teacher Competencies for Navigating Complexities - OECD

https://www.oecd.org/content/dam/oecd/en/about/projects/edu/education-2040/publications/section_5a.pdf

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

AI Skills for Teachers – A competency framework from UNESCO

https://jobmarketmonitor.com/2025/01/28/ai-skills-for-teachers-a-competency-framework-from-unesco/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

Use of ChatGPT in academia: Academic integrity hangs in the balance - ScienceDirect

https://www.sciencedirect.com/science/article/pii/S0160791X23001756

Use of ChatGPT in academia: Academic integrity hangs in the balance - ScienceDirect

https://www.sciencedirect.com/science/article/pii/S0160791X23001756

UAE launches 'Afaq' program to empower teachers with AI skills

https://gulfnews.com/uae/education/uae-launches-afaq-program-to-empower-teachers-with-ai-skills-1.500321642

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198

How to Teach AI in Schools: Lessons from Global Classrooms — Code School Finland

https://www.codeschool.fi/blog/what-we-have-learned-teaching-ai

How to Teach AI in Schools: Lessons from Global Classrooms — Code School Finland

https://www.codeschool.fi/blog/what-we-have-learned-teaching-ai

How AI is impacting trust among college students and teachers

https://www.fastcompany.com/91369428/ai-trust-college-students-teachers

Disclosing AI use can backfire, research shows

https://eller.arizona.edu/news/disclosing-ai-use-can-backfire-research-shows

AI’s future for students is in our hands | Brookings

https://www.brookings.edu/articles/ais-future-for-students-is-in-our-hands/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

Rising Use of AI in Schools Comes With Big Downsides for Students

https://www.edweek.org/technology/rising-use-of-ai-in-schools-comes-with-big-downsides-for-students/2025/10

https://eurodljournal.com/articles/140/files/6808dfe154065.pdf

https://eurodljournal.com/articles/140/files/6808dfe154065.pdf

New AI Literacy Framework to Equip Youth in an Age of AI – OECD Education and Skills Today

https://oecdedutoday.com/new-ai-literacy-framework-to-equip-youth-in-an-age-of-ai/

AI competency framework for teachers | UNESCO

https://www.unesco.org/en/articles/ai-competency-framework-teachers

Cognitive Apprenticeship

https://www.aft.org/ae/winter1991/collins_brown_holum

VisibleLearning: Subject Matter Knowledge

http://visablelearning.blogspot.com/p/subject-matter-knowledge.html

Cognitive Apprenticeship

https://www.aft.org/ae/winter1991/collins_brown_holum

The effect of ChatGPT on students’ learning performance, learning perception, and higher-order thinking: insights from a meta-analysis | Humanities and Social Sciences Communications

https://www.nature.com/articles/s41599-025-04787-y?error=cookies_not_supported&code=b49a5e16-4290-45cb-8105-006fb7f79bad

Survey: 86% of Students Already Use AI in Their Studies

https://campustechnology.com/articles/2024/08/28/survey-86-of-students-already-use-ai-in-their-studies.aspx

Finland's war on fake news starts in schools. AI could make that a lot ...

https://www.euronews.com/next/2025/08/19/finlands-war-on-fake-news-starts-in-schools-ai-could-make-that-a-lot-harder

Finnish 5th and 6th grade students' pre-instructional conceptions of ...

https://www.sciencedirect.com/science/article/pii/S2666920X22000509

Foundational Gen AI Literacy and Skills for All Students, Teachers | MOE

https://www.moe.gov.sg/news/parliamentary-replies/20250925-foundational-gen-ai-literacy-and-skills-for-all-students-teachers-and-educators

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Ohio State University is requiring every student to use AI in class to become 'AI fluent' | Fortune

https://fortune.com/2025/06/10/ohio-state-university-ai-fluency-requirement-all-students/

Use of ChatGPT in academia: Academic integrity hangs in the balance - ScienceDirect

https://www.sciencedirect.com/science/article/pii/S0160791X23001756

PwC earmarks $1B for generative AI, aligning with Microsoft/OpenAI ...

https://futureofconsulting.ai/industry-news/pwc-1b-ai/

Use of ChatGPT in academia: Academic integrity hangs in the balance - ScienceDirect

https://www.sciencedirect.com/science/article/pii/S0160791X23001756

ISTE Standards for Students

https://iste.org/standards/students

AI in schools and colleges: what you need to know – The Education Hub

https://educationhub.blog.gov.uk/2025/06/artificial-intelligence-in-schools-everything-you-need-to-know/

Department for Education Guidance on Generative AI - Smoothwall

https://smoothwall.com/resources/dfe-guidance-on-generative-ai-a-summary-for-slts-dsls

Exploring the Impact of Generative AI ChatGPT on Critical Thinking in Higher Education: Passive AI-Directed Use or Human–AI Supported Collaboration? | MDPI

https://www.mdpi.com/2227-7102/15/9/1198
All Sources

visablel....blogspot

brookings

phys

edworkingpapers

aft

scirp

gulfnews

educatio....blog.gov

mdpi

nature
jiecr

moe.gov
onlineeducation

epc

eurodljournal

unesco

oecdedutoday

fortune

euronews

reddit
jiecr

arts.ubc

sciencedirect
tandfonline

aiculturelab

researchgate

fastcompany

codeschool

rdworldonline
ojs.aaai

world-education-blog

linkedin

ai-in-education.co

twinscience

youtube

futureofconsulting

innovationleader

nearshoreamericas

pwc

forbes

guild

iste

iste.ascd

iste-ascd

oecd

jobmarketmonitor

eller.arizona

edweek

campustechnology

smoothwall